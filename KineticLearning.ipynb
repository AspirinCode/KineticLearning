{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Kinetics from Time Series Data\n",
    "\n",
    "This notebook is an implementation of Kinetic Learning created by Zak Costello."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Set Flags For Running Code\n",
    "Four data sets are supported in this code.  Two are simulated, and two are from experimental data. Change the set_num variable below to choose which data set to fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Choose the data set to run\n",
    "data_sets = [('simulated','small'),          #Small Canonical Pathway Model with Feedback Inhibition \n",
    "             ('simulated','limonene'),        #Large Limonene Model Developed Based on the Literature\n",
    "             ('experimental','isopentenol'),  #Experimental Isopentenol Pathway Time Series Data\n",
    "             ('experimental','limonene')]     #Experimental Limonene Pathway Time Series Data\n",
    "\n",
    "set_num = 2\n",
    "\n",
    "simulated_data_file_path = 'data/Fulld10000n0.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Machine Learning Model is determined by these flags.  The features to use can be automatic or manual and the machine learning model can be chosen. Additionally, the random seed can be set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_selection = 'Automatic'   #Manual or Automatic: Manual has hand selected features, Autmatic uses an algorithm\n",
    "machine_learning_model = 'tpot'   #neural,random_forest(RF)\n",
    "\n",
    "#Set Random Seed for deterministic Execution (Not yet Implemented)\n",
    "seed = None #If seed is set then output will be deterministic (And results can be cached for faster execution)\n",
    "\n",
    "#Time Consuming, will be cached for each unique case. (Only Relevant for experimental Limonene Pathway)\n",
    "run_kinetic_model = True #Allows for comparison between ml model and kinetic model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables to Set For Simulated Data Sets (How Many Strains and how many training sets needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Relevant if Simulated\n",
    "data_type,pathway = data_sets[set_num]\n",
    "if data_sets[set_num][0] == 'simulated':\n",
    "    strain_numbers = [2,10,100]\n",
    "    training_sets = 20 #Change to support a variable number here (I want to shoot for 10...)\n",
    "    \n",
    "    #Strains Needed Calculation\n",
    "    strains_needed = (max(strain_numbers) + 1)*training_sets + 1 + 2000\n",
    "    print(strains_needed)\n",
    "    \n",
    "    #Calculation of training_size values\n",
    "    train_sizes = []\n",
    "    for strain_number in strain_numbers:\n",
    "        train_size = (strain_number + 1) * training_sets / (strains_needed - 1)\n",
    "        train_sizes.append(train_size)\n",
    "    print(train_sizes)\n",
    "    \n",
    "    print([int((strains_needed-1)*size/training_sets)-1 for size in train_sizes])\n",
    "    #Strains Required\n",
    "\n",
    "    training_strains = strains_needed\n",
    "    test_strains = 1200\n",
    "\n",
    "elif data_type == 'experimental':\n",
    "    pass\n",
    "\n",
    "\n",
    "#Plotting Options (If set true more extensive plots are created for troubleshooting purposes)\n",
    "Plot_Data = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Import Modules & Setup\n",
    "Importing all required modules and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from plot import *\n",
    "from helper import *\n",
    "from IPython.display import display\n",
    "from sklearn.base import clone\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.integrate import quad\n",
    "from scipy.optimize import differential_evolution\n",
    "import math\n",
    "import sys\n",
    "sys.path.append('/usr/local/lib/python3.5/site-packages')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Load & Format Data into DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_type == 'simulated':\n",
    "    if pathway == 'limonene':\n",
    "        file_name = simulated_data_file_path\n",
    "        y0 = [1e-5]*10\n",
    "        timeKey = 'Time'\n",
    "        df = pd.read_csv(file_name)\n",
    "        df = df.pivot_table(df,index=['Strain','Time'],aggfunc=np.sum)\n",
    "\n",
    "        strains = df.index.get_level_values(0).unique()\n",
    "        sample = random.sample(list(strains),training_strains+test_strains)\n",
    "\n",
    "        #create test df\n",
    "        test_df = df.loc[(sample[0:test_strains],slice(None))]\n",
    "\n",
    "        #create the training df\n",
    "        df = df.loc[(sample[test_strains:],slice(None))]\n",
    "\n",
    "        features = ['AtoB','HMGR','HMGS','MK','PMK','PMD','Idi','GPPS','LS']\n",
    "        targets=['Acetyl-CoA','Acetoacetyl-CoA','HMG-CoA','Mev','MevP','MevPP','IPP','DMAPP','GPP','Limonene']\n",
    "        specific_features = {'Acetyl-CoA':      ['AtoB','Acetyl-CoA','Acetoacetyl-CoA','HMGR'],\n",
    "                             'Acetoacetyl-CoA': ['AtoB','Acetyl-CoA','Acetoacetyl-CoA','HMGR','HMG-CoA'],\n",
    "                             'HMG-CoA':         ['Acetyl-CoA','Acetoacetyl-CoA','HMGR','HMGS','HMG-CoA'],\n",
    "                             'Mev':             ['Acetyl-CoA','Acetoacetyl-CoA','HMGS','HMG-CoA','MK','Mev','GPP','MevP'],\n",
    "                             'MevP':            ['MK','Mev','GPP','MevP','PMK'],\n",
    "                             'MevPP':           ['PMK','MevP','PMD','MevPP','Mev'],\n",
    "                             'IPP':             ['PMD','MevPP','MevP','Mev','Idi','IPP','GPPS','DMAPP'],\n",
    "                             'DMAPP':           ['Idi','IPP','GPPS','DMAPP'],\n",
    "                             'GPP':             ['GPPS','IPP','DMAPP','GPP','LS'],\n",
    "                             'Limonene':        ['LS','GPP']}\n",
    "\n",
    "        #Create TSDF for training strains\n",
    "        tsdf = generateTSDataSet(df,features,targets)\n",
    "        \n",
    "        #Create Validation TSDF to test the models predictive power\n",
    "        test_tsdf = generateTSDataSet(test_df,features,targets)\n",
    "        display(test_tsdf)\n",
    "        \n",
    "    elif pathway=='small':\n",
    "        file_name = 'data/SmallKineticsd30000n0.csv'\n",
    "        y0 = [0.2]*3\n",
    "        timeKey = 'Time'\n",
    "        df = pd.read_csv(file_name)\n",
    "        df = df.pivot_table(df,index=['Strain','Time'],aggfunc=np.sum)\n",
    "\n",
    "        strains = df.index.get_level_values(0).unique()\n",
    "        sample = random.sample(list(strains),training_strains+test_strains)\n",
    "\n",
    "        #create test df\n",
    "        test_df = df.loc[(sample[0:test_strains],slice(None))]\n",
    "\n",
    "        #create the training df\n",
    "        df = df.loc[(sample[test_strains:],slice(None))]\n",
    "\n",
    "        features = ['e0','e1']\n",
    "        targets=['s0','s1','s2']\n",
    "        specific_features = {'s0': ['e0','e1','s0','s1','s2'],\n",
    "                             's1': ['e0','e1','s0','s1','s2'],\n",
    "                             's2': ['e0','e1','s0','s1','s2']}\n",
    "\n",
    "        tsdf = generateTSDataSet(df,features,targets)\n",
    "    \n",
    "elif data_type == 'experimental':\n",
    "    if pathway == 'limonene':\n",
    "        #Parameters that Can Be Set:\n",
    "        strains = ['L1','L2','L3']\n",
    "        training_strains = ['L1','L3']\n",
    "        test_strains = ['L2']\n",
    "\n",
    "        #Define Machine Learning Targets & Features To Use\n",
    "\n",
    "        targets = ['Acetyl-CoA (uM)','HMG-CoA (uM)','Intracellular Mevalonate (uM)','Mev-P (uM)','IPP/DMAPP (uM)','Limonene g/L']\n",
    "        features = ['OD600','ATP (uM)','AtoB','Limonene Synthase','HMGR','HMGS','MK','PMK','PMD','GPP (uM)',\n",
    "                    'NAD (uM)','NADP (uM)','Acetate g/L','Pyruvate g/L','citrate (uM)']\n",
    "\n",
    "        specific_features = {'Acetyl-CoA (uM)':['OD600','ATP (uM)','AtoB','HMGR','HMGS', 'Acetyl-CoA (uM)', 'HMG-CoA (uM)','Pyruvate g/L','citrate (uM)'],\n",
    "                             'HMG-CoA (uM)':['OD600','HMG-CoA (uM)','HMGR','MK','Mev-P (uM)','Intracellular Mevalonate (uM)','NAD (uM)','NADP (uM)'],\n",
    "                             'Intracellular Mevalonate (uM)':['OD600','ATP (uM)','Intracellular Mevalonate (uM)','MK','PMK','Mev-P (uM)'],\n",
    "                             'Mev-P (uM)':['OD600','ATP (uM)','PMD','Mev-P (uM)','Intracellular Mevalonate (uM)','IPP/DMAPP (uM)'],\n",
    "                             'IPP/DMAPP (uM)':['OD600','ATP (uM)','IPP/DMAPP (uM)','Limonene Synthase','Limonene g/L'],\n",
    "                             'Limonene g/L':['OD600','ATP (uM)','Limonene Synthase','Limonene g/L','GPP (uM)']}\n",
    "\n",
    "    elif pathway == 'isopentenol':\n",
    "        #Parameters that Can Be Set:\n",
    "        strains = ['I1','I2','I3']\n",
    "        training_strains = ['I1','I3']\n",
    "        test_strains = ['I2']\n",
    "        \n",
    "        targets = ['Acetyl-CoA (uM)','HMG-CoA (uM)','Intracellular Mevalonate (uM)','Mev-P (uM)','IPP/DMAPP (uM)','Isopentenol g/L']\n",
    "        features = ['OD600','ATP (uM)','AtoB','NudB','HMGR','HMGS','MK','PMK','PMD',\n",
    "                    'NAD (uM)','NADP (uM)','Acetate g/L','Pyruvate g/L','citrate (uM)']\n",
    "\n",
    "        specific_features = {'Acetyl-CoA (uM)':['OD600','ATP (uM)','AtoB','HMGR','HMGS', 'Acetyl-CoA (uM)', 'HMG-CoA (uM)','Pyruvate g/L','citrate (uM)'],\n",
    "                             'HMG-CoA (uM)':['OD600','HMG-CoA (uM)','HMGR','MK','Mev-P (uM)','Intracellular Mevalonate (uM)','NAD (uM)','NADP (uM)'],\n",
    "                             'Intracellular Mevalonate (uM)':['OD600','ATP (uM)','Intracellular Mevalonate (uM)','MK','PMK','Mev-P (uM)'],\n",
    "                             'Mev-P (uM)':['OD600','ATP (uM)','PMD','Mev-P (uM)','Intracellular Mevalonate (uM)','IPP/DMAPP (uM)'],\n",
    "                             'IPP/DMAPP (uM)':['OD600','ATP (uM)','IPP/DMAPP (uM)','NudB','Isopentenol g/L'],\n",
    "                             'Isopentenol g/L':['OD600','ATP (uM)','NudB','Isopentenol g/L','IPP/DMAPP (uM)']}\n",
    "\n",
    "    \n",
    "    #Processing Data Files and Creating a Shared Dataframe\n",
    "    metabolite_file_name = 'data/time_series_metabolomics.csv'\n",
    "    protein_file_name = 'data/time_series_proteomics.csv'\n",
    "\n",
    "    mdf = pd.read_csv(metabolite_file_name)\n",
    "    pdf = pd.read_csv(protein_file_name)\n",
    "\n",
    "    #Format protein tables\n",
    "    proteins = ['AtoB','HMGR','HMGS','MK','PMD','PMK','Idi','GPPS','Limonene Synthase','NudB']\n",
    "    proteins = [('ProteinArea',protein) for protein in proteins]\n",
    "    columns = ['Strain','Time','Protein','ProteinArea']\n",
    "\n",
    "    pdf = pdf.loc[pdf['Strain'].isin(strains)]\n",
    "    pdf = pdf.loc[~pdf['Hour'].isin(['72C'])]\n",
    "    pdf['Hour'] = pdf['Hour'].convert_objects(convert_numeric=True)\n",
    "\n",
    "    pdf.rename(columns={'Hour': 'Time'}, inplace=True)    \n",
    "    display(pdf)\n",
    "    print(columns)\n",
    "\n",
    "    pdf = pdf[columns].pivot_table(pdf[columns],index=['Strain','Time'],columns='Protein',aggfunc=np.sum)\n",
    "    pdf = pdf[proteins]\n",
    "    pdf.columns = pdf.columns.get_level_values(1)\n",
    "    \n",
    "    #Format metabolite tables\n",
    "    metabolites = ['Acetyl-CoA (uM)','Acetyl-CoA extracellular (uM)',\n",
    "                   'Acetoacetyl-coA (uM)','Acetoacetyl-CoA extracellular (uM)',\n",
    "                   'HMG-CoA (uM)','HMG-CoA extracellular (uM)',\n",
    "                   'Intracellular Mevalonate (uM)','MEVALONATE extracellular (uM)',\n",
    "                   'Mev-P (uM)','MEV-P extracellular (uM)',\n",
    "                   'IPP/DMAPP (uM)','IPP/DMAPP extracellular (uM)',\n",
    "                   'GPP (uM)','GPP extracellular (uM)',\n",
    "                   'Limonene g/L','ATP (uM)','OD600','NAD (uM)','NADP (uM)',\n",
    "                   'Acetate g/L','Pyruvate g/L','citrate (uM)','Isopentenol g/L']\n",
    "\n",
    "    mdf = mdf.loc[mdf['Strain'].isin(strains)]\n",
    "    \n",
    "    mdf.rename(columns={'Hour': 'Time'}, inplace=True)\n",
    "    mdf = mdf[['Strain','Time'] + metabolites].pivot_table(mdf[['Strain','Time'] + metabolites],index=['Strain','Time'],aggfunc=np.sum)\n",
    "    \n",
    "    #Extracellular Metabolites (mg/L) | Intracelluar Metabolites (mg/L) |Combined Metabolites (mg/L)\n",
    "\n",
    "    #Join metabolite and protein tables\n",
    "    df = pd.concat([pdf, mdf], axis=1)\n",
    "    test_df = df.loc[(test_strains,slice(None))]\n",
    "    display(test_df)\n",
    "    df = df.loc[(training_strains,slice(None))]\n",
    "    tsdf = generateTSDataSet(df,features,targets,n_points=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Construct Machine Learning Models\n",
    "First Do Feature Selection if Automatic features are selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Automatic Feature Selection using Recursive Feature Selection cross validated with a Random Forest Regressor\n",
    "if feature_selection == 'automatic':\n",
    "    estimator = RandomForestRegressor()\n",
    "    specific_features = {}\n",
    "    feature_list = [('feature',feature) for feature in features + targets]\n",
    "    for target in targets:\n",
    "        \n",
    "        #display(tsdf[feature_list].ix[:100])\n",
    "        if data_type == 'simulated':\n",
    "            X = tsdf[feature_list].ix[:2000].values.tolist()\n",
    "            y = tsdf[('target',target)].ix[:2000].values.tolist()        \n",
    "        else:\n",
    "            X = tsdf[feature_list].values.tolist()\n",
    "            y = tsdf[('target',target)].values.tolist()\n",
    "            \n",
    "        selector = RFECV(estimator,verbose=1)\n",
    "        selector.fit(X,y)\n",
    "        mask = selector.get_support()\n",
    "        specific_features[target] = [feature for i,feature in enumerate(features + targets) if mask[i]]\n",
    "        print(target,specific_features[target])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define all Possible Machine Learning Models that can be used in the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "#Create Models for Each Target Column\n",
    "modelDict = {}\n",
    "\n",
    "#Pipeline Regressors\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "supportVectorRegressor = Pipeline([('Scaler',StandardScaler()),\n",
    "                               ('SVR',SVR())])\n",
    "\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
    "polynomialRegressor = Pipeline([('Scaler',StandardScaler()),\n",
    "                                ('Polynomial Features',PolynomialFeatures(degree=3, include_bias=True, interaction_only=True)),\n",
    "                                ('Feature Reduction',RFECV(Ridge(),cv=None, scoring='r2')),\n",
    "                                ('Linear Regressor',BaggingRegressor(base_estimator=Ridge(),\n",
    "                                                                     n_estimators=100, max_samples=.8,\n",
    "                                                                     bootstrap=False,\n",
    "                                                                     bootstrap_features=False,\n",
    "                                                                     random_state=None))])\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "featureReducingRF = Pipeline([('Feature Reduction',SelectKBest(mutual_info_regression, k=4)),\n",
    "                              ('Random Forest Regressor',RandomForestRegressor())])\n",
    "\n",
    "\n",
    "model_str = machine_learning_model\n",
    "featureReduction = False\n",
    "if model_str == 'SVR':\n",
    "    mlmodel = supportVectorRegressor\n",
    "elif model_str == 'RF':\n",
    "    mlmodel = RandomForestRegressor(n_estimators=20)\n",
    "elif model_str == 'Poly':\n",
    "    mlmodel = polynomialRegressor\n",
    "elif model_str == 'Bagging':\n",
    "    mlmodel = BaggingRegressor(base_estimator=supportVectorRegressor)\n",
    "elif model_str == 'FeatRF':\n",
    "    mlmodel = featureReducingRF \n",
    "    featureReduction = True\n",
    "elif model_str == 'Log':\n",
    "    LogScale = FunctionTransformer(np.log1p)\n",
    "    mlmodel = Pipeline([('Log Transform',LogScale),('Scaler',StandardScaler()),\n",
    "                        ('Linear',LassoLarsIC())])\n",
    "elif model_str == 'neural':\n",
    "    neural_model = Pipeline([('Scaler',StandardScaler()),\n",
    "                               ('neural_net',MLPRegressor(hidden_layer_sizes=(5,5,5),\n",
    "                                                          learning_rate_init=0.1,\n",
    "                                                          learning_rate='adaptive',\n",
    "                                                          solver='sgd',\n",
    "                                                          activation='tanh',\n",
    "                                                          max_iter=1000))])\n",
    "    #mlmodel = MLPRegressor(hidden_layer_sizes=(10,10),learning_rate_init=0.1,solver='lbfgs')\n",
    "    mlmodel= neural_model\n",
    "    \n",
    "elif model_str == 'gaussian':\n",
    "    mlmodel = Pipeline([('Scaler',StandardScaler()),\n",
    "                        ('gassian',GaussianProcessRegressor(normalize_y=True))])\n",
    "elif model_str == 'tpot':\n",
    "    from tpot import TPOTRegressor\n",
    "    #mlmodel = TPOTRegressor(generations=130, population_size=100, verbosity=2,max_time_mins=20,cv=ShuffleSplit(),scoring='r2',n_jobs=1)\n",
    "    #mlmodel = TPOTRegressor(generations=130, population_size=80, verbosity=2,cv=ShuffleSplit(),n_jobs=1,max_time_mins=35)\n",
    "    mlmodel = TPOTRegressor(generations=12, population_size=30, verbosity=2,cv=ShuffleSplit(),n_jobs=1)#,max_time_mins=10)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Fitting & Evaluating Machine Learning Models\n",
    "\n",
    "For both simulated and real data we calculate the goodness of fit of the machine learning models. For Simulated Data we calculate the following Metrics which help us evaluate goodness of fit.\n",
    "\n",
    "Simulated Data Results:\n",
    "1. Learning Curves for Derivatives\n",
    "2. Error Distribution for Derivatives (Combined Vector Derivative Error RMSE)\n",
    "3. Goodness of Fit Plot For Simulated Curves\n",
    "4. RMSE for all simulated curves, Figure out the right one to report (Seperate and together -- Normalized / Percent / Regular)\n",
    "\n",
    "Experimental Data Results:\n",
    "1. Learning Curves for Derivatives\n",
    "2. Error Distribution for Derivatives, (Combined Vector)\n",
    "3. Comparison Plot between Real and Predicted Curves\n",
    "4. RMSE for Single Predicted Curve, (By Metabolite and Combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#1. Calculate Learning Curves for Derivatives\n",
    "\n",
    "#Pick number of strains so that the total number is equal to max tested strains...\n",
    "if data_type == 'simulated':\n",
    "    strains = tsdf.index.get_level_values(0).unique()\n",
    "    tsdf_max_strains = tsdf.loc[slice(strains[0],strains[max(strain_numbers)]),slice(None)]\n",
    "else:\n",
    "    tsdf_max_strains = tsdf\n",
    "\n",
    "average_training_score = 0\n",
    "n = 0\n",
    "for target_index in tsdf_max_strains.columns:\n",
    "    if target_index[0] == 'feature':\n",
    "        continue\n",
    "    n += 1\n",
    "    target = target_index[1]\n",
    "    print(target)\n",
    "\n",
    "\n",
    "    #feature_indecies = [('feature', feature) for feature in specific_features[target]]\n",
    "    X = tsdf_max_strains['feature'].values.tolist()\n",
    "    y = tsdf_max_strains[target_index].values.tolist()\n",
    "\n",
    "    #if fit_log_targets:\n",
    "    #    y = [math.log(val) for val in y]\n",
    "    \n",
    "    #print(X)\n",
    "    #print(y)\n",
    "    if model_str == 'tpot':\n",
    "        modelDict[target] = clone(mlmodel).fit(np.array(X),np.array(y)).fitted_pipeline_\n",
    "        try:\n",
    "            crossValPlot = plot_learning_curve(modelDict[target],target,X,y,cv=ShuffleSplit())\n",
    "        except:\n",
    "            pass\n",
    "    else:\n",
    "        modelDict[target] = clone(mlmodel)\n",
    "        crossValPlot = plot_learning_curve(modelDict[target],target,X,y,cv=ShuffleSplit())\n",
    "    \n",
    "    ax = plt.gca()\n",
    "    ax.set_ylim([-0.1, 1.1])\n",
    "    strip_target = ''.join([char for char in target if char != '/'])\n",
    "    print(strip_target)\n",
    "    crossValPlot.savefig('figures/' + strip_target + 'CrossValPlot.pdf',transparent=False)\n",
    "    plt.show()\n",
    "    \n",
    "    #Save out Cross Validation Plot\n",
    "    \n",
    "    modelDict[target] = modelDict[target].fit(X,y)\n",
    "    \n",
    "    score = modelDict[target].score(X,y)\n",
    "    print('Cross Validated Pearson R: {:f}'.format(score))\n",
    "    average_training_score +=  score\n",
    "    #print(modelDict[target_name].predict([reduced_features[2]]))\n",
    "    \n",
    "average_training_score /= n\n",
    "print('Average Training Score:',average_training_score,'n:',n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#2. Calculate Error Distribution and Plot + Report Moments\n",
    "ts_test_df = generateTSDataSet(test_df,features,targets)\n",
    "display(ts_test_df)\n",
    "errors = []\n",
    "for target in modelDict:\n",
    "    times,y_test = remove_NaN(ts_test_df.reset_index()['Time (h)'].values,ts_test_df[('target',target)].values)\n",
    "    \n",
    "    feature_list = [('feature',feature) for feature in tsdf['feature'].columns]\n",
    "    target_df = ts_test_df[feature_list]\n",
    "    target_df = target_df[ts_test_df.index.get_level_values('Time (h)').isin(times)]\n",
    "    \n",
    "    #Check to make sure there are no NaNs in each feature\n",
    "    for feature in target_df.columns:\n",
    "        if any([math.isnan(val) for val in target_df[feature].values]):\n",
    "            X,y = remove_NaN(target_df.reset_index()['Time'].values,target_df[feature].values)\n",
    "            fnc = interp1d(X,y)\n",
    "        index = 0\n",
    "        for time,val in zip(times,target_df[feature].values):\n",
    "            if math.isnan(val):\n",
    "                #print(feature,time,fnc(time))\n",
    "                target_df[feature].iloc[index] = fnc(time)\n",
    "            index += 1\n",
    "    #display(target_df)\n",
    "    \n",
    "    y_prediction = modelDict[target].predict(target_df.values)\n",
    "    \n",
    "    #print(y_prediction)\n",
    "    #print(y_test)\n",
    "    log_error = [math.log(max(y_p,0.0001)) - math.log(max(y_t,0.0001)) for y_p,y_t in zip(y_prediction,y_test)]\n",
    "    error = [y_p - y_t for y_p,y_t in zip(y_prediction,y_test)]\n",
    "    errors.append(error)\n",
    "    \n",
    "    mu = np.mean(error)\n",
    "    sigma = np.std(error)\n",
    "    print(target,'Mean Error:',mu,'Error Standard Deviation:',sigma)\n",
    "    \n",
    "    plt.figure(figsize=(13,4))\n",
    "    plt.subplot(121)\n",
    "    sns.distplot(error)\n",
    "    plt.title(target + ' Derivative '+ 'Error Residual Histagram')\n",
    "    plt.xlabel('Derivative Residual Error')\n",
    "    plt.ylabel('Probability Density')\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    plot_model_fit(target,y_prediction,y_test)\n",
    "    \n",
    "    strip_target = ''.join([char for char in target if char != '/'])\n",
    "    plt.savefig('figures/' + strip_target + 'ErrorResiduals.pdf')\n",
    "    plt.show()\n",
    "    \n",
    "    #modelDict[target].predict()\n",
    "\n",
    "#Compute Net Error Magnitude\n",
    "error_magnitude = [0]*len(errors[0])\n",
    "for error in errors:\n",
    "    error_magnitude = [em + e**2 for em,e in zip(error_magnitude,error)]\n",
    "error_magnitude = [math.sqrt(e) for e in error_magnitude]\n",
    "mu = np.mean(error_magnitude)\n",
    "sigma = np.std(error_magnitude)\n",
    "print('Total Derivative','Mean Error:',mu,'Error Standard Deviation:',sigma)\n",
    "    \n",
    "sns.distplot(error_magnitude)\n",
    "plt.title('Total Derivative Error Risidual Histagram')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If Experimental Limonene Results show that the Hand created \n",
    "# Kinetic Model Does not fit as well as the machine learning model\n",
    "\n",
    "def proteomicsData(t,k):\n",
    "    e = []\n",
    "    for i in range(int(len(k)/3)):\n",
    "        #Sorting the gains to ensure proteins only increase\n",
    "        gains = sorted(k[3*i:3*(i+1)],reverse=True)\n",
    "        e.append(leaky_hill_fcn(t,*gains)) \n",
    "    return e\n",
    "\n",
    "\n",
    "def kinetic_model(e1,e2,e3,e4,e5,e6,e7,e8,e9,\n",
    "                  s1,s2,s3,s4,s5,s6,s7,s8,s9,s10,\n",
    "                  k11,k12,k13,k21,k22,k23,k24,k31,k32,k33,k34,k35,\n",
    "                  k41,k42,k43,k44,k45,k51,k52,k61,k62,k63,k64,k65,\n",
    "                  k71,k72,k81,k82,k83,k84,k91,k92,Vin,ks3):\n",
    "    r1 = Vin - (k11*e1*s1)/(k12 + k13*s1) - k21*e2*s1*s2*ks3 / (k22*s2 + k23*s1 + k24*s1*s2)\n",
    "    r2 = (k11*e1*s1)/(k12 + k13*s1) - k21*e2*s1*s2*ks3 / (k22*s2 + k23*s1 + k24*s1*s2)\n",
    "    r3 = k21*e2*s1*s2*ks3 / (k22*s2 + k23*s1 + k24*s1*s2) - k31*e3*s3 / (k32*s1 + k33*s2 + k34*s3 + k35)\n",
    "    r4 = k31*e3*s3 / (k32*s1 + k33*s2 + k34*s3 + k35) - k41*e4*s4 / (k42*s9 + k43*s5 + k44*s4 + k45)\n",
    "    r5 = k41*e4*s4 / (k42*s9 + k43*s5 + k44*s4 + k45) - k51*e5*s5 / (k52 + s5)\n",
    "    r6 = k51*e5*s5 / (k52 + s5) - k61*e6*s6 / (k62*s5 + k63*s4 + k64*s6 + k65)\n",
    "    r7 = k61*e6*s6 / (k62*s5 + k63*s4 + k64*s6 + k65) - k71*e7*s7 / (k72 + s7) - k81*e8*s7*s8 / (k82 + k83*s7 + k84*s8 + s7*s8)\n",
    "    r8 = k71*e7*s7 / (k72 + s7) - k81*e8*s7*s8 / (k82 + k83*s7 + k84*s8 + s7*s8)\n",
    "    r9 = k81*e8*s7*s8 / (k82 + k83*s7 + k84*s8 + s7*s8) - k91*e9*s9 / (k92 + s9)\n",
    "    r10 = k91*e9*s9 / (k92 + s9)\n",
    "    return [r1,r2,r3,r4,r5,r6,r7,r8,r9,r10]\n",
    "    \n",
    "def solve_kinetic_ode(f,y0,times,k_fit):\n",
    "    sol = 1\n",
    "    return sol\n",
    "\n",
    "measured_substrates = ['Acetyl-CoA (uM)', 'HMG-CoA (uM)', 'Intracellular Mevalonate (uM)', 'Mev-P (uM)', 'IPP/DMAPP (uM)', 'Limonene g/L']\n",
    "measured_enzymes = ['AtoB', 'HMGS', 'HMGR', 'MK', 'PMK', 'PMD', 'Limonene Synthase']\n",
    "x_features = [('feature',val) for val in measured_enzymes+measured_substrates]\n",
    "y_targets = [('target',val) for val in measured_substrates]\n",
    "\n",
    "#print(x_features)\n",
    "#print(y_targets)\n",
    "\n",
    "if data_type == 'experimental' and  pathway == 'limonene':\n",
    "    [('feature', feature) for feature in specific_features[target]]\n",
    "    X = tsdf_max_strains[x_features].values.tolist()\n",
    "    y = tsdf_max_strains[y_targets].values.tolist()\n",
    "    \n",
    "    #Solve for Kinetic Coefficients from Training Set\n",
    "    def cost_fcn_gen(X,y):\n",
    "        def cost_fcn(free_params):\n",
    "            cost = 0\n",
    "            for x_val,y_vals in zip(X,y):\n",
    "                params = []\n",
    "                params.extend(x_val[0:6])       # AtoB to PMD Values\n",
    "                params.extend(free_params[0:2]) # Keep Constant GPPS and IDI levels as free parameters\n",
    "                params.extend(x_val[6:8])       # LS and Acetyl-CoA\n",
    "                params.append(free_params[2])   # AcetoAcetyl-CoA as a free Param\n",
    "                params.extend(x_val[8:11])      # HMG-CoA & Mev & MevP measured\n",
    "                params.append(free_params[3])   #MevPP \n",
    "                params.extend([x_val[11],x_val[11]]) #DMAPP & IDI Measured\n",
    "                params.extend([free_params[4],x_val[12]]) #GPP as a Free Parameter #Measured Limonene Synthase\n",
    "                params.extend(free_params[5:])  # Remaining Kinetic Free Parameters\n",
    "                \n",
    "                mp = kinetic_model(*params)\n",
    "                prediction = [mp[0],mp[2],mp[3],mp[4],mp[6]+mp[7],mp[9]]\n",
    "                cost += sum([(fx_val - y_val)**2  for fx_val,y_val in zip(prediction,y_vals)])\n",
    "            return cost\n",
    "        return cost_fcn\n",
    "\n",
    "    cost_fcn = cost_fcn_gen(X,y)\n",
    "    \n",
    "    #Call to check its working (num free params = 39)\n",
    "    print(cost_fcn([1,]*39))\n",
    "    \n",
    "    #Solve For Optimal Parameters\n",
    "    bounds = [(1*10**-12,10**9)]*39\n",
    "    sol = differential_evolution(cost_fcn,bounds,disp=True,maxiter=10000)\n",
    "    best_params = sol.x\n",
    "    print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import statistics\n",
    "\n",
    "def plot_species_curves(modelDict, title, df, targets, specific_features, ylim=None, cv=None,\n",
    "                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 3),training_sets=5):\n",
    "    \"\"\"\n",
    "    Generate a simple plot of the test and training learning curve. Returns Metrics for each predicted curve\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
    "        An object of that type which is cloned for each validation.\n",
    "\n",
    "    title : string\n",
    "        Title for the chart.\n",
    "\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Training vector, where n_samples is the number of samples and\n",
    "        n_features is the number of features.\n",
    "\n",
    "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
    "        Target relative to X for classification or regression;\n",
    "        None for unsupervised learning.\n",
    "\n",
    "    ylim : tuple, shape (ymin, ymax), optional\n",
    "        Defines minimum and maximum yvalues plotted.\n",
    "\n",
    "    cv : int, cross-validation generator or an iterable, optional\n",
    "        Determines the cross-validation splitting strategy.\n",
    "        Possible inputs for cv are:\n",
    "          - None, to use the default 3-fold cross-validation,\n",
    "          - integer, to specify the number of folds.\n",
    "          - An object to be used as a cross-validation generator.\n",
    "          - An iterable yielding train/test splits.\n",
    "\n",
    "        For integer/None inputs, if ``y`` is binary or multiclass,\n",
    "        :class:`StratifiedKFold` used. If the estimator is not a classifier\n",
    "        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n",
    "\n",
    "        Refer :ref:`User Guide <cross_validation>` for the various\n",
    "        cross-validators that can be used here.\n",
    "\n",
    "    n_jobs : integer, optional\n",
    "        Number of jobs to run in parallel (default 1).\n",
    "    \"\"\"\n",
    "    \n",
    "    #Set Random Seed For training\n",
    "    #seed = 103\n",
    "    #random.seed(seed)\n",
    "    \n",
    "    #Create figure / plots\n",
    "    fig = plt.figure(figsize=(12,16))\n",
    "    #fig.set_title(title)\n",
    "    #if ylim is not None:\n",
    "    #    plt.ylim(*ylim)\n",
    "    \n",
    "    #Create subplots for each target\n",
    "    ax = {}\n",
    "    for i,target in enumerate(targets):\n",
    "        ax[target] = plt.subplot(int(len(targets)/2), 2, i+1)\n",
    "    \n",
    "    #Get Randomized List of all Strains\n",
    "    strains = df.index.get_level_values(0).unique()\n",
    "    strains = list(strains.values)\n",
    "    #print(strains)\n",
    "    strains = random.sample(strains, len(strains))\n",
    "    \n",
    "    #Pick test strain\n",
    "    test_df = df.loc[(slice(strains[0],strains[0]),slice(None)),:]\n",
    "    strains = strains[1:]\n",
    "    \n",
    "    #Create Interpolation functions for each feature in the test strain\n",
    "    interpFun = {}\n",
    "    #display(test_df.reset_index())\n",
    "    for feature in df.columns:\n",
    "        X,y = remove_NaN(test_df.reset_index()['Time (h)'].tolist(),test_df[feature].tolist())\n",
    "        if isinstance(feature,tuple):\n",
    "            if feature[0] == 'feature':\n",
    "                feature = feature[1]\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        interpFun[feature] = interp1d(X,y)  \n",
    "\n",
    "    print([key for key in interpFun])\n",
    "    \n",
    "    train_sizes = [int(len(strains)*size/training_sets)-1 for size in train_sizes]\n",
    "    for i,size in enumerate(train_sizes):\n",
    "        if size < 2:\n",
    "            train_sizes[i] = 2\n",
    "            \n",
    "    #Create Fits for each training set\n",
    "    fits = {}\n",
    "    for training_set in range(training_sets):        \n",
    "        fits[training_set] = {}\n",
    "\n",
    "        #Generate training strain data for this training set\n",
    "        training_strains = strains[0:(train_sizes[-1] + 1)]\n",
    "        #print(training_strains)\n",
    "        strains = strains[train_sizes[-1]:]\n",
    "        endSamples = train_sizes\n",
    "        #print('Strains:',strains)\n",
    "        #print('End Samples',endSamples)\n",
    "        sample_sets = [df.loc[(training_strains[0:endSample],slice(None)),:] for endSample in endSamples]\n",
    "\n",
    "        #For each set size in the training set fit the model and store it\n",
    "        for j,sample_set in enumerate(sample_sets):\n",
    "            \n",
    "            #print('Sample Set:',sample_set.index.get_level_values(0).unique().values)\n",
    "            \n",
    "            # Train Model\n",
    "            print('Training Models for Training Set',training_set,'In Sample set',j)\n",
    "            for target in targets:\n",
    "                feature_indecies = [('feature', feature) for feature in df['feature'].columns]\n",
    "                X = sample_set[feature_indecies].values.tolist()\n",
    "\n",
    "                #print(feature_indecies)\n",
    "                #display(sample_set[feature_indecies])\n",
    "                target_index = ('target',target)\n",
    "                y = sample_set[target_index].values.tolist()\n",
    "                modelDict[target].fit(X,y)\n",
    "\n",
    "            print('Integrating ODEs!')\n",
    "            # Integrate Given Model Test Case\n",
    "            g = mlode(modelDict, test_df, targets, specific_features)\n",
    "            times = test_df.reset_index()['Time (h)'].tolist()\n",
    "\n",
    "            #Set Y0 initial condition\n",
    "            appended_targets = [('feature',target) for target in targets]\n",
    "            #display(test_df)\n",
    "            #display(test_df[appended_targets].iloc[0])\n",
    "            y0 = test_df[appended_targets].iloc[0].tolist()\n",
    "\n",
    "            #print('times:',times)\n",
    "            fit  = odeintz(g,y0,times)\n",
    "            fitT = list(map(list, zip(*fit)))\n",
    "            fits[training_set][train_sizes[j]] = fitT\n",
    "\n",
    "    \n",
    "    #Perform Statistics on Fits and generate plots\n",
    "    colors = ['b','g','k','y','m']\n",
    "    predictions = {}\n",
    "    lines =[]\n",
    "    labels = []\n",
    "    for k,target in enumerate(targets):\n",
    "        actual_data = [interpFun[target](t) for t in times]\n",
    "        predictions[target] = {'actual':actual_data}\n",
    "        predictions['Time'] = times\n",
    "        if k == 0:\n",
    "            lines.append(ax[target].plot(times,actual_data,'--', color='r')[0])\n",
    "            labels.append('Actual Dynamics')\n",
    "        else:\n",
    "            ax[target].plot(times,actual_data,'--', color='r')\n",
    "        ax[target].set_title(target)\n",
    "        \n",
    "        for j in range(len(sample_sets)):\n",
    "            upper = []\n",
    "            lower = []\n",
    "            aves = []\n",
    "            \n",
    "            predictions[target][train_sizes[j]] = []\n",
    "            for training_set in range(training_sets):\n",
    "                predictions[target][train_sizes[j]].append(fits[training_set][train_sizes[j]][k])\n",
    "            \n",
    "            for i,time in enumerate(times):\n",
    "                \n",
    "                values = []\n",
    "                for training_set in range(training_sets):\n",
    "                    #print(training_set,train_sizes[j],i)\n",
    "                    values.append(fits[training_set][train_sizes[j]][k][i])\n",
    "\n",
    "                #Compute Statistics of Values\n",
    "                #print(values)\n",
    "                ave = statistics.mean(values)\n",
    "                std = statistics.stdev(values)\n",
    "                aves += [ave,]\n",
    "                upper += [ave + std,]\n",
    "                lower += [ave - std,]\n",
    "                \n",
    "                #print(upper)\n",
    "                #print(times)\n",
    "                \n",
    "            #Compute upper and lower bounds for shading\n",
    "            ax[target].fill_between(times, lower,upper, alpha=0.1, color=colors[j])\n",
    "            if k == 0:\n",
    "                lines.append(ax[target].plot(times,aves,colors[j])[0])\n",
    "                labels.append(str(train_sizes[j]) + ' Strain Prediction')\n",
    "            else:\n",
    "                ax[target].plot(times,aves,colors[j])\n",
    "            print(colors[j],train_sizes[j])\n",
    "        plt.figlegend( lines, labels, loc = 'lower center', ncol=5, labelspacing=0. )       \n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#3. Plot Comparison between Actual and Predicted Results\n",
    "if data_type == 'simulated':\n",
    "    real_vs_predicted = plot_species_curves(modelDict, 'title', tsdf, targets, specific_features,train_sizes=train_sizes,training_sets=training_sets)\n",
    "    plt.savefig('figures/MainFigure.pdf')\n",
    "    \n",
    "    \n",
    "elif data_type == 'experimental':\n",
    "    #Fit each test strain by solving the differential equation\n",
    "\n",
    "    #Select Random Time Series\n",
    "    strains = test_df.index.get_level_values(0).unique()\n",
    "    random_strain = random.sample(tuple(strains), 1)\n",
    "    random_strain = list(random_strain)[0]\n",
    "    random_df = test_df.loc[(slice(random_strain,random_strain),slice(None)),:]\n",
    "\n",
    "    #Set Y0 initial condition\n",
    "    y0 = random_df[targets].iloc[0].tolist()\n",
    "    print('y0',y0)\n",
    "    \n",
    "    #Create Differential Equation to Solve\n",
    "    if featureReduction:\n",
    "        target_features = [header[1:]]*len(targets[0])\n",
    "        g = mlode(modelDict, random_df, targets, specific_features)\n",
    "    else:\n",
    "        g = mlode(modelDict, random_df, targets, specific_features,time_index='Time')\n",
    "\n",
    "    #Solve Differential Equation numerically using a runge-kutta 4,5 implementation\n",
    "    times = random_df.reset_index()['Time'].tolist()\n",
    "    fit = odeintz(g,y0,times)\n",
    "    fitT = list(map(list, zip(*fit)))\n",
    "    \n",
    "    \n",
    "    #Create Interpolation functions for each feature\n",
    "    interpFun = {}\n",
    "    for feature in random_df.columns:\n",
    "        X,y = remove_NaN(random_df.reset_index()['Time'].tolist(),random_df[feature].tolist())\n",
    "        interpFun[feature] = interp1d(X,y)\n",
    "\n",
    "        \n",
    "    if pathway == 'limonene':\n",
    "        #print(random_df.columns)\n",
    "        proteins = ['AtoB', 'HMGR', 'HMGS', 'MK', 'PMD', 'PMK','Limonene Synthase']\n",
    "        protein_fcns= []\n",
    "        for feature in random_df.columns:\n",
    "            X,y = remove_NaN(random_df.reset_index()['Time'].tolist(),random_df[feature].tolist())\n",
    "            protein_fcns.append(interp1d(X,y))\n",
    "        \n",
    "        #Generate Fit Kinetic Model ODE\n",
    "        def kinetic_ode(x,t,proteomic_fcns):\n",
    "            \n",
    "            #proteomics \n",
    "            proteomic_data = [proteomic_fcn(t) for proteomic_fcn in proteomic_fcns]\n",
    "\n",
    "            params = []\n",
    "            params.extend(proteomic_data[0:6]) # AtoB to PMD Values\n",
    "            params.extend(best_params[0:2])     # Keep Constant GPPS and IDI levels as free parameters\n",
    "            params.append(proteomic_data[6])   # \n",
    "            params.append(x[0])                #Acetyl-CoA\n",
    "            params.append(best_params[2])       # AcetoAcetyl-CoA as a free Param\n",
    "            params.extend(x[1:4])               # HMG-CoA & Mev & MevP measured\n",
    "            params.append(best_params[3])       # MevPP \n",
    "            params.extend([x[4],x[4]])          # DMAPP & IDI Measured\n",
    "            params.extend([best_params[4],x[5]]) # GPP as a Free Parameter #Measured Limonene Synthase\n",
    "            params.extend(best_params[5:])      # Remaining Kinetic Free Parameters\n",
    "            \n",
    "            \n",
    "            dxdt = kinetic_model(*params)\n",
    "            dxdt_combined = [dxdt[0],dxdt[2],dxdt[3],dxdt[4],dxdt[6]+dxdt[7],dxdt[9]]\n",
    "            return dxdt_combined\n",
    "        \n",
    "        for protein in proteins:\n",
    "            protein_fcns.append(interpFun[protein])\n",
    "        \n",
    "        #Solve Kinetic ODE at initial Conditions\n",
    "        kinetic_ode_p = lambda x,t : kinetic_ode(x,t,protein_fcns)\n",
    "        kinetic_fit = odeintz(kinetic_ode_p,y0,times)\n",
    "        kinetic_fit = list(map(list, zip(*kinetic_fit)))\n",
    "        \n",
    "    real_vs_predicted = {}\n",
    "    plt.figure(figsize=(12,8))\n",
    "    for i,target in enumerate(targets):\n",
    "        plt.subplot(2,3,i+1)\n",
    "        if data_type == 'experimental':\n",
    "            #Plot both High and Low Strain values\n",
    "            for strain in training_strains:\n",
    "                strainInterpFun = {}\n",
    "                strain_df = df.loc[(strain,slice(None))]\n",
    "                X,y = remove_NaN(strain_df.reset_index()['Time'].tolist(),strain_df[target].tolist())\n",
    "                strainInterpFun[target] = interp1d(X,y)\n",
    "                actual_data = [strainInterpFun[target](t) for t in times]\n",
    "                train_line, = plt.plot(times,actual_data,'r--')\n",
    "            \n",
    "            #Plot Kinetic Model Fit\n",
    "            if pathway =='limonene':\n",
    "                kinetic_pred = [max(kinetic_fit[i][j],0) for j,t in enumerate(times)]\n",
    "                kinetic_line, = plt.plot(times,kinetic_pred,color='k')\n",
    "\n",
    "        actual_data = [interpFun[target](t) for t in times]\n",
    "        pos_pred = [max(fitT[i][j],0) for j,t in enumerate(times)]\n",
    "        prediction_line, = plt.plot(times,pos_pred)\n",
    "        #print(actual_data)\n",
    "        test_line, = plt.plot(times,actual_data,'g--')\n",
    "        plt.ylabel(target)\n",
    "        plt.xlabel('Time [h]')\n",
    "        plt.xlim([0,72])\n",
    "        \n",
    "        if i == 5 and pathway == 'limonene':\n",
    "            plt.ylim([0,0.6])\n",
    "        \n",
    "        #Create variable for processing error residuals\n",
    "        if pathway == 'limonene':\n",
    "            real_vs_predicted[target] = [times,actual_data,pos_pred,kinetic_pred]\n",
    "        else:\n",
    "            real_vs_predicted[target] = [times,actual_data,pos_pred]\n",
    "    #Add Legend\n",
    "    if set_num==2:\n",
    "        product = 'Isopentenol'\n",
    "    elif set_num==3:\n",
    "        product = 'Limonene'\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.90)\n",
    "    plt.subplots_adjust(bottom=0.12)\n",
    "    plt.suptitle('Prediction of ' + product + ' Strain Dynamics', fontsize=18)\n",
    "    if pathway == 'limonene':\n",
    "        plt.figlegend( (train_line,test_line,prediction_line,kinetic_line), ('Training Set Data','Test Data','Machine Learning Model Prediction','Kinetic Model Prediction'), loc = 'lower center', ncol=5, labelspacing=0. )  \n",
    "    else:\n",
    "        plt.figlegend( (train_line,test_line,prediction_line), ('Training Set Data','Test Data','Machine Learning Model Prediction'), loc = 'lower center', ncol=5, labelspacing=0. ) \n",
    "    plt.savefig('data/' + product + '_prediction.eps', format='eps', dpi=600)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#4. Error Residuals of Time Series Data + Key Moments Report\n",
    "\n",
    "#Not Quite Right... Need to Integrate over the \n",
    "if data_type == 'experimental':\n",
    "    #print(real_vs_predicted)\n",
    "\n",
    "    rmse_percent = []\n",
    "    rmse_average = []\n",
    "    rmse_percent_k = []\n",
    "    rmse_average_k = []\n",
    "    for metabolite in real_vs_predicted:\n",
    "        if pathway == 'limonene':\n",
    "            times,real,predicted,kinetic = real_vs_predicted[metabolite]\n",
    "        else:\n",
    "            times,real,predicted = real_vs_predicted[metabolite]\n",
    "        real_fcn = interp1d(times,real)\n",
    "        pred_fcn = interp1d(times,predicted)\n",
    "\n",
    "        integrand = lambda t: (real_fcn(t) - pred_fcn(t))**2\n",
    "        rmse = math.sqrt(quad(integrand,min(times),max(times))[0])\n",
    "        rmse_average.append(rmse)\n",
    "        percent_integrand = lambda t: abs(real_fcn(t) - pred_fcn(t))/(real_fcn(t)*max(times))\n",
    "        #print(metabolite,[percent_integrand(t) for t in times],[real_fcn(t) for t in times],[pred_fcn(t) for t in times])\n",
    "        \n",
    "        rmsep = math.sqrt(quad(percent_integrand,min(times),max(times))[0])\n",
    "        rmse_percent.append(rmsep)\n",
    "        print('ML Fit:',metabolite,rmse,'RMSE percent:',rmsep*100)\n",
    "        \n",
    "        if pathway == 'limonene':\n",
    "            kinetic_fcn = interp1d(times,kinetic)\n",
    "            integrand = lambda t: (real_fcn(t) - kinetic_fcn(t))**2\n",
    "            rmsek = math.sqrt(quad(integrand,min(times),max(times))[0])\n",
    "            percent_integrand = lambda t: abs(real_fcn(t) - kinetic_fcn(t))/(real_fcn(t)*max(times))\n",
    "            rmsepk = math.sqrt(quad(percent_integrand,min(times),max(times))[0])\n",
    "            rmse_percent_k.append(rmsepk)\n",
    "            rmse_average_k.append(rmsek)\n",
    "            print('Kinetic Fit:',metabolite,rmsek,'RMSE percent:',rmsepk*100)\n",
    "    \n",
    "    print('')\n",
    "    print('Machine Learning Model Agrigate Error')\n",
    "    print('Average RMSE:',sum(rmse_average)/len(rmse_average))\n",
    "    print('Total Percent Error:',sum(rmse_percent)/len(rmse_percent)*100)\n",
    "\n",
    "    if pathway == 'limonene':\n",
    "        print('')\n",
    "        print('Kinetic Model Agrigate Error')\n",
    "        print('Average RMSE:',sum(rmse_average_k)/len(rmse_average_k))\n",
    "        print('Total Percent Error:',sum(rmse_percent_k)/len(rmse_percent_k)*100)\n",
    "        \n",
    "elif data_type == 'simulated':\n",
    "    times = real_vs_predicted['Time']\n",
    "    rmse_dict = {}\n",
    "    for metabolite in real_vs_predicted:\n",
    "        if metabolite not in ['Time',]:\n",
    "            actual = real_vs_predicted[metabolite]['actual']\n",
    "            real_fcn = interp1d(times,actual)\n",
    "            rmse_dict[metabolite] = {}\n",
    "            for test_size in real_vs_predicted[metabolite]:\n",
    "                print(metabolite,'Test Size:',test_size)\n",
    "                total_rmse = []\n",
    "                if test_size not in ['Time','actual']:\n",
    "                    #For Each Test Size Produce Values + Moments Reporting...\n",
    "                    rmse = []\n",
    "                    for test_strain in enumerate(real_vs_predicted[metabolite][test_size]):\n",
    "                        #print(test_strain)\n",
    "                        pred_fcn = interp1d(times,test_strain[1])\n",
    "                        integrand = lambda t: (real_fcn(t) - pred_fcn(t))**2\n",
    "                        rmse.append(math.sqrt(quad(integrand,min(times),max(times))[0]))\n",
    "                        \n",
    "                    #Report Moments for each Test Size\n",
    "                    rmse_dict[metabolite][test_size] = rmse\n",
    "                    print('Mean RMSE:',np.mean(rmse),'Standard Deviation RMSE:',np.std(rmse))\n",
    "                    print('RMSEs:',rmse)\n",
    "                    print('')\n",
    "                    \n",
    "\n",
    "    #Calculate Mean Total Error + Mean Standard Deviation for Composite Error\n",
    "    size_dict = {}\n",
    "    for metabolite in rmse_dict:\n",
    "        for test_size in rmse_dict[metabolite]:\n",
    "            size_dict[test_size] = [0]*len(rmse_dict[metabolite][test_size])\n",
    "            size_dict[test_size] = [s + r for s,r in zip(rmse_dict[metabolite][test_size],size_dict[test_size])]\n",
    "\n",
    "    #Print out Agrigate Statistics\n",
    "    for test_size in size_dict:\n",
    "        rmse = size_dict[test_size]\n",
    "        print('Test Size for Agrigate Statistics:',test_size)\n",
    "        print('Mean RMSE:',np.mean(rmse),'Standard Deviation RMSE:',np.std(rmse))\n",
    "        print('RMSEs:',rmse)\n",
    "        print('')\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if data_type == 'simulated':\n",
    "        \n",
    "    #Plot Agrigate Statistics in the form of a bar graph...\n",
    "    sizes = list(size_dict.keys())\n",
    "    means = [np.mean(size_dict[size]) for size in sizes]\n",
    "    stdevs = [np.std(size_dict[size]) for size in sizes]\n",
    "    y_pos = np.arange(len(sizes))\n",
    "\n",
    "    #Calculate Percent Errors              \n",
    "    #for size in sizes:\n",
    "    #    for training_set in training_sets:\n",
    "    #        percent_integrand = lambda t: abs(real_fcn(t) - kinetic_fcn(t))/(real_fcn(t)*max(times))\n",
    "        \n",
    "    plt.bar(y_pos, means, align='center',yerr=stdevs)\n",
    "    plt.xticks(y_pos, sizes)\n",
    "    plt.ylabel('RMSE')\n",
    "    plt.xlabel('Training Set Size')\n",
    "    plt.title('RMSE vs Training Set Size')\n",
    "    plt.savefig('figures/RMSEvsSize.pdf')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Perform Analysis to See if Learned Model Can predict the best & Worst producers from 100 strains\n",
    "\n",
    "def predict_production(modelDict,strain,tsdf):\n",
    "    #Use Model To figure out production using dynamics\n",
    "    #Set Y0 initial condition\n",
    "    #display(tsdf)\n",
    "    #y0_targets = pd.MultiIndex.from_tuples([('feature',target) for target in targets])\n",
    "    strain_df = tsdf.loc[tsdf.index.get_level_values(0) == strain]\n",
    "    y0 = strain_df.loc[:,tsdf.columns.isin(targets)].iloc[0]\n",
    "    y0 = y0.reindex(targets).values\n",
    "    \n",
    "    #Create Differential Equation to Solve\n",
    "    g = mlode(modelDict, strain_df, targets, specific_features, time_index='Time')\n",
    "\n",
    "    #Solve Differential Equation numerically using a runge-kutta 4,5 implementation\n",
    "    times = strain_df.reset_index()['Time'].tolist()\n",
    "    fit = odeintz(g,y0,times)\n",
    "    fitT = list(map(list, zip(*fit)))\n",
    "    \n",
    "    #print(fitT[-1])\n",
    "    production = fitT[-1][-1]\n",
    "    \n",
    "    #Return Producion at the final time point\n",
    "    return production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mlode(modelDict, df, targets, specific_features,time_index='Time (h)'):\n",
    "    #print(specific_features)\n",
    "    #print(targets)\n",
    "    \n",
    "    #Create Interpolation functions for each feature\n",
    "    ml_interpFun = {}\n",
    "    #display(df)\n",
    "    #df = df['feature']\n",
    "    for feature in df.columns:\n",
    "        #print(feature)\n",
    "        if feature not in targets:\n",
    "            #print(feature)\n",
    "            X,y = remove_NaN(df.reset_index()[time_index].tolist(),df[feature].tolist())\n",
    "            if isinstance(feature,tuple):\n",
    "                feature = feature[1]\n",
    "            ml_interpFun[feature] = interp1d(X,y)\n",
    "        \n",
    "    #print([key for key in ml_interpFun])\n",
    "    #Define the function to integrate\n",
    "    def f(x,t):\n",
    "        x_dot = []\n",
    "        #Generate Derivatives for Each Target\n",
    "        for target in targets:\n",
    "            x_pred = []\n",
    "            for feature in df.columns:\n",
    "                #If the Feature is dynamically changing, use the Dynamic Value\n",
    "                if feature in targets:\n",
    "                    x_pred= np.append(x_pred, x[targets.index(feature)])\n",
    "                \n",
    "                #Otherwise use a value parameterized by time\n",
    "                else:\n",
    "                    x_pred= np.append(x_pred, ml_interpFun[feature](t))\n",
    "            \n",
    "            #Append the Predicted Derivative to the output vector\n",
    "            #print(x_pred)\n",
    "            model_prediction = modelDict[target].predict([x_pred])\n",
    "            #print(x_dot,model_prediction)\n",
    "            x_dot = np.append(x_dot,model_prediction)\n",
    "                \n",
    "        return x_dot\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_trials = 10\n",
    "if data_type == 'simulated':\n",
    "    \n",
    "    strain_numbers = list(test_tsdf.index.get_level_values(0).unique())\n",
    "    strain_lists = random.sample(strain_numbers,100*num_trials)\n",
    "    strain_lists = [strain_lists[x:x+100] for x in range(0, len(strain_lists), 100)]\n",
    "    \n",
    "    actual_max_ranks = []\n",
    "    actual_min_ranks = []\n",
    "    actual_min_productions = []\n",
    "    actual_max_productions = []\n",
    "    prediction_errors = []\n",
    "    productions = []\n",
    "    actual_productions = []\n",
    "    for strain_list in strain_lists:\n",
    "        \n",
    "        #Sort Chosen Strains by production & Reindex by Rank\n",
    "        strain_list_index = pd.MultiIndex.from_tuples([(strain,69.0) for strain in strain_list])\n",
    "        actual_production_df = test_tsdf.loc[test_tsdf.index.isin(strain_list_index),('feature','Limonene')].to_frame()\n",
    "        actual_production_df.columns = ['Limonene']\n",
    "        actual_production_df = actual_production_df.sort_values('Limonene',axis=0).reset_index()\n",
    "        actual_production_df = actual_production_df[['Strain','Limonene']]\n",
    "        #display(actual_production_df)\n",
    "        \n",
    "    \n",
    "        #Use Model To Predict Production For all Strains\n",
    "        min_production = float('inf')\n",
    "        max_production = 0\n",
    "        display(actual_production_df)\n",
    "        for strain in strain_list:\n",
    "            production = predict_production(modelDict,strain,test_tsdf)\n",
    "            actual_production = actual_production_df.loc[actual_production_df['Strain']==strain,'Limonene'].values[0]\n",
    "            print(production,actual_production)\n",
    "            \n",
    "            #Select Best and Worst Strains\n",
    "            if production < min_production:\n",
    "                min_production = production\n",
    "                actual_min_production = actual_production\n",
    "                min_strain = strain\n",
    "                \n",
    "            if production > max_production:\n",
    "                max_production = production\n",
    "                actual_max_production = actual_production\n",
    "                max_strain = strain\n",
    "            \n",
    "            productions.append(actual_production)\n",
    "            actual_productions.append(production)\n",
    "            prediction_errors.append(production - actual_production)\n",
    "            \n",
    "            print('minimum producing strain ({}) produces {:.4f} g/L'.format(min_strain,min_production))\n",
    "            print('maximum producing strain ({}) produces {:.4f} g/L'.format(max_strain,max_production))\n",
    "            \n",
    "        #Calculate Strain Ranks\n",
    "        actual_max_ranks.append(actual_production_df.loc[actual_production_df['Strain']==max_strain].index)\n",
    "        actual_min_ranks.append(actual_production_df.loc[actual_production_df['Strain']==min_strain].index)\n",
    "        \n",
    "        #Calculate Predicted Highest Production / Actual Highest Production\n",
    "        actual_max_productions.append(actual_max_production)\n",
    "        actual_min_productions.append(actual_min_production)\n",
    "    \n",
    "    #Do Hectors Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Plot all findings from Production Prediction Experiment!\n",
    "if data_type == 'simulated':\n",
    "    #Plot Average Rank Predicted for Min / Max Strains (And Compute/ Present Statistics)\n",
    "    print(np.mean(actual_max_ranks)+1,np.std(actual_max_ranks))\n",
    "    print(np.mean(actual_min_ranks)+1,np.std(actual_min_ranks))\n",
    "    \n",
    "    fig=plt.figure()\n",
    "    ax = plt.gca()\n",
    "    plt.bar(np.arange(2),[100-np.mean(actual_max_ranks),np.mean(actual_min_ranks)+1],yerr=[np.std(actual_max_ranks),np.std(actual_min_ranks)],align='center')\n",
    "    ax.set_xticks([0,1])\n",
    "    ax.set_xticklabels(('Min Producing Strain Rank Error','Max Producing Strain Rank Error'))\n",
    "    plt.title('Average Predicted Rank (Best Rank is 1/Worst Rank is 100)')\n",
    "    plt.savefig('figures/rank_prediction.pdf')\n",
    "    plt.show()\n",
    "    \n",
    "    #Plot Predicted Max vs Actual Max for each Set\n",
    "    #plt.scatter(actual_max_productions,actual_min_productions)\n",
    "    #plt.show()\n",
    "    \n",
    "    #Plot Production Error histogram (And Compute/Present Statistics)\n",
    "    print('95% of all predicted strain productivities are within {:.2f} mg/L Limonene.'.format(np.std(prediction_errors)*2*1000))\n",
    "    sns.distplot(prediction_errors)\n",
    "    plt.title('Prediction Error of Limonene (g/L)')\n",
    "    plt.xlabel('Error in Predicting Final Limonene Production (g/L)')\n",
    "    plt.ylabel('Probability Density')\n",
    "    plt.savefig('figures/prediction_error_distribution.pdf')\n",
    "    plt.show()\n",
    "    \n",
    "    plot_model_fit('Limonene',productions,actual_productions)    \n",
    "    plt.savefig('figures/final_production_error_residuals.pdf')\n",
    "    plt.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Build Final Figure for Hector 2/10/100 (How Many Strains does it get Correct??)\n",
    "#Pick 100 sets of 3 10 times and see if the order matches\n",
    "\n",
    "sizes = [100,10,2]\n",
    "training_strains = list(tsdf.index.get_level_values(0).unique())\n",
    "test_strains = list(test_tsdf.index.get_level_values(0).unique())\n",
    "training_strains = np.reshape(random.sample(training_strains,10*sizes[0]),(10,-1))\n",
    "#print(training_strains)\n",
    "success_dict = {100:[],10:[],2:[]}\n",
    "for trial in range(10):\n",
    "    \n",
    "    #Generate 100 Sets of 3 (With Replacement)\n",
    "    test_sets = [random.choices(test_strains,k=3) for _ in range(100)]\n",
    "    \n",
    "    #Train Model on different number of strains\n",
    "    substrains = None\n",
    "    for size in sizes:\n",
    "        print(trial,size)\n",
    "        #Pick Strains To Train On\n",
    "        if substrains is None:\n",
    "            substrains = training_strains[trial]\n",
    "        else:\n",
    "            #sample a set of the substrains with cardinality of size\n",
    "            substrains = random.sample(set(substrains),size)\n",
    "        \n",
    "        #Generate features (X) and targets (y)\n",
    "        X = tsdf['feature'].loc[tsdf.index.get_level_values(0).isin(substrains)].values\n",
    "\n",
    "        #Train Models\n",
    "        for key in modelDict:\n",
    "            y = tsdf['target'].loc[tsdf.index.get_level_values(0).isin(substrains),tsdf['target'].columns == key].values.ravel()\n",
    "            modelDict[key].fit(X,y)\n",
    "        \n",
    "        print('predict production')\n",
    "        #For Each test set try to predict order (high / medium / low)\n",
    "        successes = 0\n",
    "        for i in range(100):\n",
    "            #print(test_sets[i])\n",
    "        \n",
    "            strain_list_index = pd.MultiIndex.from_tuples([(strain,69.0) for strain in test_sets[i]])\n",
    "            actual_production_df = test_tsdf.loc[test_tsdf.index.isin(strain_list_index),('feature','Limonene')].to_frame()\n",
    "            actual_production_df.columns = ['Limonene']\n",
    "            actual_production_df = actual_production_df.sort_values('Limonene',axis=0).reset_index()\n",
    "            actual_production_df = actual_production_df[['Strain','Limonene']]\n",
    "            sorted_strains = actual_production_df['Strain']\n",
    "            #display(actual_production_df)\n",
    "            \n",
    "            \n",
    "            productions = [ ]\n",
    "            #if production is always increasing... then label a success\n",
    "            for test_strain in sorted_strains:\n",
    "                productions.append(predict_production(modelDict,test_strain,test_df))\n",
    "                \n",
    "            if productions[0] < productions[1] and productions[1] < productions[2]:\n",
    "                print('Success!')\n",
    "                successes += 1\n",
    "            else:\n",
    "                print('Failure')\n",
    "            \n",
    "            print(productions)\n",
    "            print(i,successes/(i+1))\n",
    "            display(actual_production_df)\n",
    "\n",
    "            #print(productions,test_sets[i])\n",
    "            \n",
    "        success_dict[size].append(successes)          "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
