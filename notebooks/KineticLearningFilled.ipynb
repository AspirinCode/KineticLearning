{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Kinetics from Time Series Data\n",
    "\n",
    "This notebook is an implementation of Kinetic Learning created by Zak Costello."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO for Publication\n",
    "\n",
    "### Code Functionality:\n",
    "1. Develop a Data Set to simulate the method for testing.\n",
    "    * [X] Trouble Shoot Data Generation Function to figure out why Integration is failing...\n",
    "2. [X] Implement Kinetic Model Prediction + Plot Prediction on Experimental Prediction Plots\n",
    "3. [X] Add RMSE Metrics For Kinetic Model and report\n",
    "\n",
    "### Code Quality:\n",
    "1. [X] Rewrite and Simplify Code.\n",
    "2. [X] Add to Version control.\n",
    "\n",
    "### Paper Writing:\n",
    "\n",
    "1. [X] Normalize Axes On Big Simulated Data Plot\n",
    "\n",
    "3. Update Figures for Each Simulation to Mirror Most Up to Date Version\n",
    "    * [X] Improve Simulated Figures (Add Legend + Make Target more noticable)\n",
    "    * [X] Simulated / Simple\n",
    "    * [X] Simulated / Limonene\n",
    "    * [X] Experimental / Limonene\n",
    "    * [X] Experimental / Isopentenol\n",
    "6. [X] Incorporate Kinetic Model Comparisons into Existing Experimental data set plots (if they are even at all close)\n",
    "\n",
    "7. [X] Make All plots write out files and save all data generated!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ideas for Future:\n",
    "1. Implement Code in tensor flow (with a better machine learning model...)\n",
    "2. Expand number of pathway models -- show i can learn simulated data for many pathways\n",
    "3. Optimize titer using this method directly inferred from data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Set Flags For Running Code\n",
    "Four data sets are supported in this code.  Two are simulated, and two are from experimental data. Change the set_num variable below to choose which data set to fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Choose the data set to run\n",
    "data_sets = [('simulated','small'),          #Small Canonical Pathway Model with Feedback Inhibition \n",
    "             ('simulated','limonene'),        #Large Limonene Model Developed Based on the Literature\n",
    "             ('experimental','isopentenol'),  #Experimental Isopentenol Pathway Time Series Data\n",
    "             ('experimental','limonene')]     #Experimental Limonene Pathway Time Series Data\n",
    "\n",
    "set_num = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Machine Learning Model is determined by these flags.  The features to use can be automatic or manual and the machine learning model can be chosen. Additionally, the random seed can be set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_selection = 'Automatic'   #Manual or Automatic: Manual has hand selected features, Autmatic uses an algorithm\n",
    "machine_learning_model = 'tpot'   #neural,random_forest(RF)\n",
    "\n",
    "#Set Random Seed for deterministic Execution\n",
    "seed = None #If seed is set then output will be deterministic (And results can be cached for faster execution)\n",
    "\n",
    "#Time Consuming, will be cached for each unique case. (Only Relevant for experimental Limonene Pathway)\n",
    "run_kinetic_model = True #Allows for comparison between ml model and kinetic model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables to Set For Simulated Data Sets (How Many Strains and how many training sets needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Relevant if Simulated\n",
    "data_type,pathway = data_sets[set_num]\n",
    "if data_sets[set_num][0] == 'simulated':\n",
    "    strain_numbers = [2,10,100]\n",
    "    training_sets = 20 #Change to support a variable number here (I want to shoot for 10...)\n",
    "    \n",
    "    #Strains Needed Calculation\n",
    "    strains_needed = (max(strain_numbers) + 1)*training_sets + 1\n",
    "    print(strains_needed)\n",
    "    \n",
    "    #Calculation of training_size values\n",
    "    train_sizes = []\n",
    "    for strain_number in strain_numbers:\n",
    "        train_size = (strain_number + 1) * training_sets / (strains_needed - 1)\n",
    "        train_sizes.append(train_size)\n",
    "    print(train_sizes)\n",
    "    \n",
    "    print([int((strains_needed-1)*size/training_sets)-1 for size in train_sizes])\n",
    "    #Strains Required\n",
    "\n",
    "    training_strains = strains_needed\n",
    "    test_strains = 200\n",
    "\n",
    "elif data_type == 'experimental':\n",
    "    pass\n",
    "\n",
    "\n",
    "#Plotting Options (If set true more extensive plots are created for troubleshooting purposes)\n",
    "Plot_Data = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Import Modules & Setup\n",
    "Importing all required modules and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from plot import *\n",
    "from helper import *\n",
    "from IPython.display import display\n",
    "from sklearn.base import clone\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.integrate import quad\n",
    "from scipy.optimize import differential_evolution\n",
    "import math\n",
    "import sys\n",
    "sys.path.append('/usr/local/lib/python3.5/site-packages')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Load & Format Data into DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:109: FutureWarning: convert_objects is deprecated.  Use the data-type specific converters pd.to_datetime, pd.to_timedelta and pd.to_numeric.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sample Name</th>\n",
       "      <th>Time</th>\n",
       "      <th>Organism</th>\n",
       "      <th>Strain</th>\n",
       "      <th>Replicate</th>\n",
       "      <th>Protein</th>\n",
       "      <th>Protein name</th>\n",
       "      <th>Pathway</th>\n",
       "      <th>Peptide</th>\n",
       "      <th>PeptideArea</th>\n",
       "      <th>...</th>\n",
       "      <th>PeptideCorrectedArea Normalized to BSA</th>\n",
       "      <th>ProteinArea</th>\n",
       "      <th>ProteinArea Normalized to AmpR</th>\n",
       "      <th>ProteinArea Normalized to Cam</th>\n",
       "      <th>ProteinArea Normalized to BSA</th>\n",
       "      <th>PrecursorMz</th>\n",
       "      <th>PrecursorCharge</th>\n",
       "      <th>RetentionTime</th>\n",
       "      <th>Background</th>\n",
       "      <th>PeakRank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7550</th>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>Escherichia coli</td>\n",
       "      <td>L1</td>\n",
       "      <td>51</td>\n",
       "      <td>AtoB</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mevalonate Pathway</td>\n",
       "      <td>LGDGQVYDVILR</td>\n",
       "      <td>6177.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020394</td>\n",
       "      <td>4104.5</td>\n",
       "      <td>0.0229</td>\n",
       "      <td>0.0145</td>\n",
       "      <td>0.0136</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7551</th>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>Escherichia coli</td>\n",
       "      <td>L1</td>\n",
       "      <td>51</td>\n",
       "      <td>AtoB</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mevalonate Pathway</td>\n",
       "      <td>TFVFSQDEFPK</td>\n",
       "      <td>2032.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006709</td>\n",
       "      <td>4104.5</td>\n",
       "      <td>0.0229</td>\n",
       "      <td>0.0145</td>\n",
       "      <td>0.0136</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7552</th>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>L1</td>\n",
       "      <td>51</td>\n",
       "      <td>Bisabolene</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mevalonate Pathway</td>\n",
       "      <td>ASQLAFPGENILDEAK</td>\n",
       "      <td>2595.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008568</td>\n",
       "      <td>1946.5</td>\n",
       "      <td>0.0109</td>\n",
       "      <td>0.0069</td>\n",
       "      <td>0.0064</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7553</th>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>L1</td>\n",
       "      <td>51</td>\n",
       "      <td>Bisabolene</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mevalonate Pathway</td>\n",
       "      <td>YNVSPAIFDNFK</td>\n",
       "      <td>1298.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004286</td>\n",
       "      <td>1946.5</td>\n",
       "      <td>0.0109</td>\n",
       "      <td>0.0069</td>\n",
       "      <td>0.0064</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7554</th>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>Saccharomyces cerevisiae</td>\n",
       "      <td>L1</td>\n",
       "      <td>51</td>\n",
       "      <td>HMGR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mevalonate Pathway</td>\n",
       "      <td>SDVSALVELNIAK</td>\n",
       "      <td>23223.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.075007</td>\n",
       "      <td>30588.0</td>\n",
       "      <td>0.1709</td>\n",
       "      <td>0.1079</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>505</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7555</th>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>Saccharomyces cerevisiae</td>\n",
       "      <td>L1</td>\n",
       "      <td>51</td>\n",
       "      <td>HMGR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mevalonate Pathway</td>\n",
       "      <td>SVVAEATIPGDVVR</td>\n",
       "      <td>38458.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.126976</td>\n",
       "      <td>30588.0</td>\n",
       "      <td>0.1709</td>\n",
       "      <td>0.1079</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7556</th>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>Saccharomyces cerevisiae</td>\n",
       "      <td>L1</td>\n",
       "      <td>51</td>\n",
       "      <td>HMGS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mevalonate Pathway</td>\n",
       "      <td>GLVSDPAGSDALNVLK</td>\n",
       "      <td>5542.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018298</td>\n",
       "      <td>13860.0</td>\n",
       "      <td>0.0775</td>\n",
       "      <td>0.0489</td>\n",
       "      <td>0.0458</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7557</th>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>Saccharomyces cerevisiae</td>\n",
       "      <td>L1</td>\n",
       "      <td>51</td>\n",
       "      <td>HMGS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mevalonate Pathway</td>\n",
       "      <td>LEVGTETLIDK</td>\n",
       "      <td>22178.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.073224</td>\n",
       "      <td>13860.0</td>\n",
       "      <td>0.0775</td>\n",
       "      <td>0.0489</td>\n",
       "      <td>0.0458</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7558</th>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>Escherichia coli</td>\n",
       "      <td>L1</td>\n",
       "      <td>51</td>\n",
       "      <td>Idi</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mevalonate Pathway</td>\n",
       "      <td>LSAFTQLK</td>\n",
       "      <td>543703.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.793741</td>\n",
       "      <td>339361.5</td>\n",
       "      <td>1.8964</td>\n",
       "      <td>1.1967</td>\n",
       "      <td>1.1205</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>420</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7559</th>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>Escherichia coli</td>\n",
       "      <td>L1</td>\n",
       "      <td>51</td>\n",
       "      <td>Idi</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mevalonate Pathway</td>\n",
       "      <td>YELGVEITPPESIYPDFR</td>\n",
       "      <td>135523.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.447178</td>\n",
       "      <td>339361.5</td>\n",
       "      <td>1.8964</td>\n",
       "      <td>1.1967</td>\n",
       "      <td>1.1205</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>83</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7560</th>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>Escherichia coli</td>\n",
       "      <td>L1</td>\n",
       "      <td>51</td>\n",
       "      <td>IspA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mevalonate Pathway</td>\n",
       "      <td>HVPLDALER</td>\n",
       "      <td>22217.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.073353</td>\n",
       "      <td>15523.0</td>\n",
       "      <td>0.0867</td>\n",
       "      <td>0.0547</td>\n",
       "      <td>0.0513</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7561</th>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>Escherichia coli</td>\n",
       "      <td>L1</td>\n",
       "      <td>51</td>\n",
       "      <td>IspA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mevalonate Pathway</td>\n",
       "      <td>STYPALLGLEQAR</td>\n",
       "      <td>8944.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029150</td>\n",
       "      <td>15523.0</td>\n",
       "      <td>0.0867</td>\n",
       "      <td>0.0547</td>\n",
       "      <td>0.0513</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>115</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7562</th>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>L1</td>\n",
       "      <td>51</td>\n",
       "      <td>Limonene Synthase</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mevalonate Pathway</td>\n",
       "      <td>GVNVIPYLR</td>\n",
       "      <td>141541.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.467322</td>\n",
       "      <td>116540.0</td>\n",
       "      <td>0.6512</td>\n",
       "      <td>0.4110</td>\n",
       "      <td>0.3848</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7563</th>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>L1</td>\n",
       "      <td>51</td>\n",
       "      <td>Limonene Synthase</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mevalonate Pathway</td>\n",
       "      <td>LADDLGTSVEEVSR</td>\n",
       "      <td>91539.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.302232</td>\n",
       "      <td>116540.0</td>\n",
       "      <td>0.6512</td>\n",
       "      <td>0.4110</td>\n",
       "      <td>0.3848</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7564</th>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>Saccharomyces cerevisiae</td>\n",
       "      <td>L1</td>\n",
       "      <td>51</td>\n",
       "      <td>MK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mevalonate Pathway</td>\n",
       "      <td>QQIDDLLLPGNTNLPWTS</td>\n",
       "      <td>4083.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013481</td>\n",
       "      <td>9766.0</td>\n",
       "      <td>0.0546</td>\n",
       "      <td>0.0344</td>\n",
       "      <td>0.0322</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7565</th>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>Saccharomyces cerevisiae</td>\n",
       "      <td>L1</td>\n",
       "      <td>51</td>\n",
       "      <td>MK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mevalonate Pathway</td>\n",
       "      <td>SLVFQLFENK</td>\n",
       "      <td>15449.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051008</td>\n",
       "      <td>9766.0</td>\n",
       "      <td>0.0546</td>\n",
       "      <td>0.0344</td>\n",
       "      <td>0.0322</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7566</th>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>Escherichia coli</td>\n",
       "      <td>L1</td>\n",
       "      <td>51</td>\n",
       "      <td>NudB</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mevalonate Pathway</td>\n",
       "      <td>VLMLQR</td>\n",
       "      <td>9657.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031884</td>\n",
       "      <td>5413.5</td>\n",
       "      <td>0.0303</td>\n",
       "      <td>0.0191</td>\n",
       "      <td>0.0179</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7567</th>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>Escherichia coli</td>\n",
       "      <td>L1</td>\n",
       "      <td>51</td>\n",
       "      <td>NudB</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mevalonate Pathway</td>\n",
       "      <td>WLDAPAAAALTK</td>\n",
       "      <td>1170.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003863</td>\n",
       "      <td>5413.5</td>\n",
       "      <td>0.0303</td>\n",
       "      <td>0.0191</td>\n",
       "      <td>0.0179</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7568</th>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>Saccharomyces cerevisiae</td>\n",
       "      <td>L1</td>\n",
       "      <td>51</td>\n",
       "      <td>PMD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mevalonate Pathway</td>\n",
       "      <td>DASLPTLSQWK</td>\n",
       "      <td>174932.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.573253</td>\n",
       "      <td>174631.5</td>\n",
       "      <td>0.9759</td>\n",
       "      <td>0.6158</td>\n",
       "      <td>0.5766</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1307</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7569</th>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>Saccharomyces cerevisiae</td>\n",
       "      <td>L1</td>\n",
       "      <td>51</td>\n",
       "      <td>PMD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mevalonate Pathway</td>\n",
       "      <td>LYQLPQSTSEISR</td>\n",
       "      <td>175638.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.579899</td>\n",
       "      <td>174631.5</td>\n",
       "      <td>0.9759</td>\n",
       "      <td>0.6158</td>\n",
       "      <td>0.5766</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7570</th>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>Saccharomyces cerevisiae</td>\n",
       "      <td>L1</td>\n",
       "      <td>51</td>\n",
       "      <td>PMK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mevalonate Pathway</td>\n",
       "      <td>VQWLDVTQADWGVR</td>\n",
       "      <td>25253.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.082037</td>\n",
       "      <td>39064.5</td>\n",
       "      <td>0.2183</td>\n",
       "      <td>0.1378</td>\n",
       "      <td>0.1290</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>406</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7571</th>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>Saccharomyces cerevisiae</td>\n",
       "      <td>L1</td>\n",
       "      <td>51</td>\n",
       "      <td>PMK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mevalonate Pathway</td>\n",
       "      <td>YEAFVVGLSAR</td>\n",
       "      <td>57505.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.175920</td>\n",
       "      <td>39064.5</td>\n",
       "      <td>0.2183</td>\n",
       "      <td>0.1378</td>\n",
       "      <td>0.1290</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4223</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7572</th>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>Staphylococcus aureus</td>\n",
       "      <td>L1</td>\n",
       "      <td>51</td>\n",
       "      <td>HMGR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mevalonate Pathway</td>\n",
       "      <td>IADEAYPSIK</td>\n",
       "      <td>5441.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017964</td>\n",
       "      <td>3644.0</td>\n",
       "      <td>0.0204</td>\n",
       "      <td>0.0129</td>\n",
       "      <td>0.0120</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7573</th>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>Staphylococcus aureus</td>\n",
       "      <td>L1</td>\n",
       "      <td>51</td>\n",
       "      <td>HMGR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mevalonate Pathway</td>\n",
       "      <td>LIGTIEVPMTLAIVGGGTK</td>\n",
       "      <td>1847.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006098</td>\n",
       "      <td>3644.0</td>\n",
       "      <td>0.0204</td>\n",
       "      <td>0.0129</td>\n",
       "      <td>0.0120</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7574</th>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>Staphylococcus aureus</td>\n",
       "      <td>L1</td>\n",
       "      <td>51</td>\n",
       "      <td>HMGS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mevalonate Pathway</td>\n",
       "      <td>SGYEDAVDYNR</td>\n",
       "      <td>98.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000324</td>\n",
       "      <td>759.0</td>\n",
       "      <td>0.0042</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0025</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7575</th>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>Staphylococcus aureus</td>\n",
       "      <td>L1</td>\n",
       "      <td>51</td>\n",
       "      <td>HMGS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mevalonate Pathway</td>\n",
       "      <td>VLVIATDTAR</td>\n",
       "      <td>1420.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004688</td>\n",
       "      <td>759.0</td>\n",
       "      <td>0.0042</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0025</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7576</th>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>Escherichia coli</td>\n",
       "      <td>L1</td>\n",
       "      <td>51</td>\n",
       "      <td>6PGD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pentose Phosphate Pathway</td>\n",
       "      <td>EAYELVAPILTK</td>\n",
       "      <td>397692.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.313048</td>\n",
       "      <td>323324.0</td>\n",
       "      <td>1.8068</td>\n",
       "      <td>1.1402</td>\n",
       "      <td>1.0675</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7577</th>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>Escherichia coli</td>\n",
       "      <td>L1</td>\n",
       "      <td>51</td>\n",
       "      <td>6PGD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pentose Phosphate Pathway</td>\n",
       "      <td>NLALNIESR</td>\n",
       "      <td>250263.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.821971</td>\n",
       "      <td>323324.0</td>\n",
       "      <td>1.8068</td>\n",
       "      <td>1.1402</td>\n",
       "      <td>1.0675</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1307</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7578</th>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>Escherichia coli</td>\n",
       "      <td>L1</td>\n",
       "      <td>51</td>\n",
       "      <td>FRDA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mixed Acid</td>\n",
       "      <td>LGSNSLAELVVFGR</td>\n",
       "      <td>39034.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.127844</td>\n",
       "      <td>38721.0</td>\n",
       "      <td>0.2164</td>\n",
       "      <td>0.1365</td>\n",
       "      <td>0.1278</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>313</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7579</th>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>Escherichia coli</td>\n",
       "      <td>L1</td>\n",
       "      <td>51</td>\n",
       "      <td>CAPP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mixed Acid</td>\n",
       "      <td>ADLWLAEYYDQR</td>\n",
       "      <td>23956.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.077457</td>\n",
       "      <td>40910.0</td>\n",
       "      <td>0.2286</td>\n",
       "      <td>0.1443</td>\n",
       "      <td>0.1351</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>496</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11899</th>\n",
       "      <td>79</td>\n",
       "      <td>72</td>\n",
       "      <td>Escherichia coli</td>\n",
       "      <td>L3</td>\n",
       "      <td>79</td>\n",
       "      <td>ACON1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tricarboxylic Acid Cycle</td>\n",
       "      <td>VVIAESFER</td>\n",
       "      <td>53943.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.171001</td>\n",
       "      <td>54780.0</td>\n",
       "      <td>0.3883</td>\n",
       "      <td>0.1493</td>\n",
       "      <td>0.1737</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11900</th>\n",
       "      <td>79</td>\n",
       "      <td>72</td>\n",
       "      <td>Escherichia coli</td>\n",
       "      <td>L3</td>\n",
       "      <td>79</td>\n",
       "      <td>MAO1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Misc</td>\n",
       "      <td>AWIQYQGFK</td>\n",
       "      <td>20819.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.065997</td>\n",
       "      <td>15042.0</td>\n",
       "      <td>0.1066</td>\n",
       "      <td>0.0410</td>\n",
       "      <td>0.0477</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11901</th>\n",
       "      <td>79</td>\n",
       "      <td>72</td>\n",
       "      <td>Escherichia coli</td>\n",
       "      <td>L3</td>\n",
       "      <td>79</td>\n",
       "      <td>MAO1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Misc</td>\n",
       "      <td>TSAEALQQAIDDNFWQAEYR</td>\n",
       "      <td>9265.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029370</td>\n",
       "      <td>15042.0</td>\n",
       "      <td>0.1066</td>\n",
       "      <td>0.0410</td>\n",
       "      <td>0.0477</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11902</th>\n",
       "      <td>79</td>\n",
       "      <td>72</td>\n",
       "      <td>Escherichia coli</td>\n",
       "      <td>L3</td>\n",
       "      <td>79</td>\n",
       "      <td>TKT1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pentose Phosphate Pathway</td>\n",
       "      <td>ALSMDAVQK</td>\n",
       "      <td>52695.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.167044</td>\n",
       "      <td>55110.0</td>\n",
       "      <td>0.3906</td>\n",
       "      <td>0.1502</td>\n",
       "      <td>0.1747</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11903</th>\n",
       "      <td>79</td>\n",
       "      <td>72</td>\n",
       "      <td>Escherichia coli</td>\n",
       "      <td>L3</td>\n",
       "      <td>79</td>\n",
       "      <td>TKT1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pentose Phosphate Pathway</td>\n",
       "      <td>TEEQLANIAR</td>\n",
       "      <td>57525.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.182356</td>\n",
       "      <td>55110.0</td>\n",
       "      <td>0.3906</td>\n",
       "      <td>0.1502</td>\n",
       "      <td>0.1747</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11904</th>\n",
       "      <td>79</td>\n",
       "      <td>72</td>\n",
       "      <td>Escherichia coli</td>\n",
       "      <td>L3</td>\n",
       "      <td>79</td>\n",
       "      <td>ACSA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Acetate Cycle</td>\n",
       "      <td>LVITSDEGVR</td>\n",
       "      <td>2967.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009405</td>\n",
       "      <td>2967.0</td>\n",
       "      <td>0.0210</td>\n",
       "      <td>0.0081</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11905</th>\n",
       "      <td>79</td>\n",
       "      <td>72</td>\n",
       "      <td>Escherichia coli</td>\n",
       "      <td>L3</td>\n",
       "      <td>79</td>\n",
       "      <td>TKT2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pentose Phosphate Pathway</td>\n",
       "      <td>EAILEAQSVK</td>\n",
       "      <td>51651.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.163399</td>\n",
       "      <td>45982.5</td>\n",
       "      <td>0.3259</td>\n",
       "      <td>0.1254</td>\n",
       "      <td>0.1458</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>106</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11906</th>\n",
       "      <td>79</td>\n",
       "      <td>72</td>\n",
       "      <td>Escherichia coli</td>\n",
       "      <td>L3</td>\n",
       "      <td>79</td>\n",
       "      <td>TKT2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pentose Phosphate Pathway</td>\n",
       "      <td>TVIGFGSPNK</td>\n",
       "      <td>40420.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.128132</td>\n",
       "      <td>45982.5</td>\n",
       "      <td>0.3259</td>\n",
       "      <td>0.1254</td>\n",
       "      <td>0.1458</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11907</th>\n",
       "      <td>79</td>\n",
       "      <td>72</td>\n",
       "      <td>Escherichia coli</td>\n",
       "      <td>L3</td>\n",
       "      <td>79</td>\n",
       "      <td>MQO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tricarboxylic Acid Cycle</td>\n",
       "      <td>FVFIGAGGAALK</td>\n",
       "      <td>8780.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027833</td>\n",
       "      <td>589221.5</td>\n",
       "      <td>4.1762</td>\n",
       "      <td>1.6064</td>\n",
       "      <td>1.8678</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11908</th>\n",
       "      <td>79</td>\n",
       "      <td>72</td>\n",
       "      <td>Escherichia coli</td>\n",
       "      <td>L3</td>\n",
       "      <td>79</td>\n",
       "      <td>MQO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tricarboxylic Acid Cycle</td>\n",
       "      <td>LLQESGIPEAK</td>\n",
       "      <td>1169663.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.707860</td>\n",
       "      <td>589221.5</td>\n",
       "      <td>4.1762</td>\n",
       "      <td>1.6064</td>\n",
       "      <td>1.8678</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11909</th>\n",
       "      <td>79</td>\n",
       "      <td>72</td>\n",
       "      <td>Escherichia coli</td>\n",
       "      <td>L3</td>\n",
       "      <td>79</td>\n",
       "      <td>ACON2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tricarboxylic Acid Cycle</td>\n",
       "      <td>IPLIIGR</td>\n",
       "      <td>371250.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.175169</td>\n",
       "      <td>270563.0</td>\n",
       "      <td>1.9176</td>\n",
       "      <td>0.7376</td>\n",
       "      <td>0.8577</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>537</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11910</th>\n",
       "      <td>79</td>\n",
       "      <td>72</td>\n",
       "      <td>Escherichia coli</td>\n",
       "      <td>L3</td>\n",
       "      <td>79</td>\n",
       "      <td>ACON2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tricarboxylic Acid Cycle</td>\n",
       "      <td>TDVLIDEVR</td>\n",
       "      <td>171123.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.540213</td>\n",
       "      <td>270563.0</td>\n",
       "      <td>1.9176</td>\n",
       "      <td>0.7376</td>\n",
       "      <td>0.8577</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>710</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11911</th>\n",
       "      <td>79</td>\n",
       "      <td>72</td>\n",
       "      <td>Escherichia coli</td>\n",
       "      <td>L3</td>\n",
       "      <td>79</td>\n",
       "      <td>RPIB</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pentose Phosphate Pathway</td>\n",
       "      <td>VVGLELAK</td>\n",
       "      <td>2532.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008027</td>\n",
       "      <td>2532.0</td>\n",
       "      <td>0.0179</td>\n",
       "      <td>0.0069</td>\n",
       "      <td>0.0080</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11912</th>\n",
       "      <td>79</td>\n",
       "      <td>72</td>\n",
       "      <td>Escherichia coli</td>\n",
       "      <td>L3</td>\n",
       "      <td>79</td>\n",
       "      <td>GPMI</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Glycolysis</td>\n",
       "      <td>AFFANPVLTGAVDK</td>\n",
       "      <td>53845.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.170690</td>\n",
       "      <td>67421.5</td>\n",
       "      <td>0.4779</td>\n",
       "      <td>0.1838</td>\n",
       "      <td>0.2137</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11913</th>\n",
       "      <td>79</td>\n",
       "      <td>72</td>\n",
       "      <td>Escherichia coli</td>\n",
       "      <td>L3</td>\n",
       "      <td>79</td>\n",
       "      <td>GPMI</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Glycolysis</td>\n",
       "      <td>ILINSPK</td>\n",
       "      <td>81018.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.256766</td>\n",
       "      <td>67421.5</td>\n",
       "      <td>0.4779</td>\n",
       "      <td>0.1838</td>\n",
       "      <td>0.2137</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11914</th>\n",
       "      <td>79</td>\n",
       "      <td>72</td>\n",
       "      <td>Escherichia coli</td>\n",
       "      <td>L3</td>\n",
       "      <td>79</td>\n",
       "      <td>MDH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tricarboxylic Acid Cycle</td>\n",
       "      <td>FGLSLVR</td>\n",
       "      <td>402482.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.272115</td>\n",
       "      <td>424140.5</td>\n",
       "      <td>3.0061</td>\n",
       "      <td>1.1563</td>\n",
       "      <td>1.3445</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1187</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11915</th>\n",
       "      <td>79</td>\n",
       "      <td>72</td>\n",
       "      <td>Escherichia coli</td>\n",
       "      <td>L3</td>\n",
       "      <td>79</td>\n",
       "      <td>MDH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tricarboxylic Acid Cycle</td>\n",
       "      <td>LFGVTTLDIIR</td>\n",
       "      <td>448621.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.416956</td>\n",
       "      <td>424140.5</td>\n",
       "      <td>3.0061</td>\n",
       "      <td>1.1563</td>\n",
       "      <td>1.3445</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1635</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11916</th>\n",
       "      <td>79</td>\n",
       "      <td>72</td>\n",
       "      <td>Escherichia coli</td>\n",
       "      <td>L3</td>\n",
       "      <td>79</td>\n",
       "      <td>GPMA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Glycolysis</td>\n",
       "      <td>ELPLTESLALTIDR</td>\n",
       "      <td>167483.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.526436</td>\n",
       "      <td>138530.0</td>\n",
       "      <td>0.9818</td>\n",
       "      <td>0.3777</td>\n",
       "      <td>0.4391</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1416</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11917</th>\n",
       "      <td>79</td>\n",
       "      <td>72</td>\n",
       "      <td>Escherichia coli</td>\n",
       "      <td>L3</td>\n",
       "      <td>79</td>\n",
       "      <td>GPMA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Glycolysis</td>\n",
       "      <td>FTGWYDVDLSEK</td>\n",
       "      <td>110993.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.351851</td>\n",
       "      <td>138530.0</td>\n",
       "      <td>0.9818</td>\n",
       "      <td>0.3777</td>\n",
       "      <td>0.4391</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11918</th>\n",
       "      <td>79</td>\n",
       "      <td>72</td>\n",
       "      <td>Escherichia coli</td>\n",
       "      <td>L3</td>\n",
       "      <td>79</td>\n",
       "      <td>DHSC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tricarboxylic Acid Cycle</td>\n",
       "      <td>ISFVITVVLSLLAGVLVW</td>\n",
       "      <td>1943.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006159</td>\n",
       "      <td>1943.0</td>\n",
       "      <td>0.0138</td>\n",
       "      <td>0.0053</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11919</th>\n",
       "      <td>79</td>\n",
       "      <td>72</td>\n",
       "      <td>Escherichia coli</td>\n",
       "      <td>L3</td>\n",
       "      <td>79</td>\n",
       "      <td>MAO2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Misc</td>\n",
       "      <td>GVIYQGR</td>\n",
       "      <td>21619.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.067965</td>\n",
       "      <td>20296.0</td>\n",
       "      <td>0.1438</td>\n",
       "      <td>0.0553</td>\n",
       "      <td>0.0643</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>179</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11920</th>\n",
       "      <td>79</td>\n",
       "      <td>72</td>\n",
       "      <td>Escherichia coli</td>\n",
       "      <td>L3</td>\n",
       "      <td>79</td>\n",
       "      <td>MAO2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Misc</td>\n",
       "      <td>NVFGYR</td>\n",
       "      <td>19152.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060712</td>\n",
       "      <td>20296.0</td>\n",
       "      <td>0.1438</td>\n",
       "      <td>0.0553</td>\n",
       "      <td>0.0643</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11921</th>\n",
       "      <td>79</td>\n",
       "      <td>72</td>\n",
       "      <td>Escherichia coli</td>\n",
       "      <td>L3</td>\n",
       "      <td>79</td>\n",
       "      <td>EUTD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Acetate Cycle</td>\n",
       "      <td>TPPDALEK</td>\n",
       "      <td>6022.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019090</td>\n",
       "      <td>6022.0</td>\n",
       "      <td>0.0427</td>\n",
       "      <td>0.0164</td>\n",
       "      <td>0.0191</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11922</th>\n",
       "      <td>79</td>\n",
       "      <td>72</td>\n",
       "      <td>NaN</td>\n",
       "      <td>L3</td>\n",
       "      <td>79</td>\n",
       "      <td>GPPS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mevalonate Pathway</td>\n",
       "      <td>AAPLLGLADYVAFR</td>\n",
       "      <td>1818481.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.750858</td>\n",
       "      <td>1430948.0</td>\n",
       "      <td>10.1419</td>\n",
       "      <td>3.9012</td>\n",
       "      <td>4.5361</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4344</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11923</th>\n",
       "      <td>79</td>\n",
       "      <td>72</td>\n",
       "      <td>NaN</td>\n",
       "      <td>L3</td>\n",
       "      <td>79</td>\n",
       "      <td>GPPS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mevalonate Pathway</td>\n",
       "      <td>EFSDELLNR</td>\n",
       "      <td>1048135.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.321421</td>\n",
       "      <td>1430948.0</td>\n",
       "      <td>10.1419</td>\n",
       "      <td>3.9012</td>\n",
       "      <td>4.5361</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>376</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11924</th>\n",
       "      <td>79</td>\n",
       "      <td>72</td>\n",
       "      <td>NaN</td>\n",
       "      <td>L3</td>\n",
       "      <td>79</td>\n",
       "      <td>AMPR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Antibiotic Marker</td>\n",
       "      <td>SALPAGWFIADK</td>\n",
       "      <td>171979.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.544081</td>\n",
       "      <td>141092.0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.3847</td>\n",
       "      <td>0.4473</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>346</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11925</th>\n",
       "      <td>79</td>\n",
       "      <td>72</td>\n",
       "      <td>NaN</td>\n",
       "      <td>L3</td>\n",
       "      <td>79</td>\n",
       "      <td>AMPR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Antibiotic Marker</td>\n",
       "      <td>VGYIELDLNSGK</td>\n",
       "      <td>113026.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.350449</td>\n",
       "      <td>141092.0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.3847</td>\n",
       "      <td>0.4473</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2475</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11926</th>\n",
       "      <td>79</td>\n",
       "      <td>72</td>\n",
       "      <td>NaN</td>\n",
       "      <td>L3</td>\n",
       "      <td>79</td>\n",
       "      <td>Cam</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Antibiotic Marker</td>\n",
       "      <td>FYPAFIHILAR</td>\n",
       "      <td>426382.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.351641</td>\n",
       "      <td>366798.0</td>\n",
       "      <td>2.5997</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.1628</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11927</th>\n",
       "      <td>79</td>\n",
       "      <td>72</td>\n",
       "      <td>NaN</td>\n",
       "      <td>L3</td>\n",
       "      <td>79</td>\n",
       "      <td>Cam</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Antibiotic Marker</td>\n",
       "      <td>ITGYTTVDISQWHR</td>\n",
       "      <td>308652.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.973876</td>\n",
       "      <td>366798.0</td>\n",
       "      <td>2.5997</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.1628</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1438</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11928</th>\n",
       "      <td>79</td>\n",
       "      <td>72</td>\n",
       "      <td>NaN</td>\n",
       "      <td>L3</td>\n",
       "      <td>79</td>\n",
       "      <td>BSA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Normalization Std.</td>\n",
       "      <td>HLVDEPQNLIK</td>\n",
       "      <td>317537.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>315455.0</td>\n",
       "      <td>2.2358</td>\n",
       "      <td>0.8600</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2082</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4077 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Sample Name  Time                  Organism Strain  Replicate  \\\n",
       "7550            51     0          Escherichia coli     L1         51   \n",
       "7551            51     0          Escherichia coli     L1         51   \n",
       "7552            51     0                       NaN     L1         51   \n",
       "7553            51     0                       NaN     L1         51   \n",
       "7554            51     0  Saccharomyces cerevisiae     L1         51   \n",
       "7555            51     0  Saccharomyces cerevisiae     L1         51   \n",
       "7556            51     0  Saccharomyces cerevisiae     L1         51   \n",
       "7557            51     0  Saccharomyces cerevisiae     L1         51   \n",
       "7558            51     0          Escherichia coli     L1         51   \n",
       "7559            51     0          Escherichia coli     L1         51   \n",
       "7560            51     0          Escherichia coli     L1         51   \n",
       "7561            51     0          Escherichia coli     L1         51   \n",
       "7562            51     0                       NaN     L1         51   \n",
       "7563            51     0                       NaN     L1         51   \n",
       "7564            51     0  Saccharomyces cerevisiae     L1         51   \n",
       "7565            51     0  Saccharomyces cerevisiae     L1         51   \n",
       "7566            51     0          Escherichia coli     L1         51   \n",
       "7567            51     0          Escherichia coli     L1         51   \n",
       "7568            51     0  Saccharomyces cerevisiae     L1         51   \n",
       "7569            51     0  Saccharomyces cerevisiae     L1         51   \n",
       "7570            51     0  Saccharomyces cerevisiae     L1         51   \n",
       "7571            51     0  Saccharomyces cerevisiae     L1         51   \n",
       "7572            51     0     Staphylococcus aureus     L1         51   \n",
       "7573            51     0     Staphylococcus aureus     L1         51   \n",
       "7574            51     0     Staphylococcus aureus     L1         51   \n",
       "7575            51     0     Staphylococcus aureus     L1         51   \n",
       "7576            51     0          Escherichia coli     L1         51   \n",
       "7577            51     0          Escherichia coli     L1         51   \n",
       "7578            51     0          Escherichia coli     L1         51   \n",
       "7579            51     0          Escherichia coli     L1         51   \n",
       "...            ...   ...                       ...    ...        ...   \n",
       "11899           79    72          Escherichia coli     L3         79   \n",
       "11900           79    72          Escherichia coli     L3         79   \n",
       "11901           79    72          Escherichia coli     L3         79   \n",
       "11902           79    72          Escherichia coli     L3         79   \n",
       "11903           79    72          Escherichia coli     L3         79   \n",
       "11904           79    72          Escherichia coli     L3         79   \n",
       "11905           79    72          Escherichia coli     L3         79   \n",
       "11906           79    72          Escherichia coli     L3         79   \n",
       "11907           79    72          Escherichia coli     L3         79   \n",
       "11908           79    72          Escherichia coli     L3         79   \n",
       "11909           79    72          Escherichia coli     L3         79   \n",
       "11910           79    72          Escherichia coli     L3         79   \n",
       "11911           79    72          Escherichia coli     L3         79   \n",
       "11912           79    72          Escherichia coli     L3         79   \n",
       "11913           79    72          Escherichia coli     L3         79   \n",
       "11914           79    72          Escherichia coli     L3         79   \n",
       "11915           79    72          Escherichia coli     L3         79   \n",
       "11916           79    72          Escherichia coli     L3         79   \n",
       "11917           79    72          Escherichia coli     L3         79   \n",
       "11918           79    72          Escherichia coli     L3         79   \n",
       "11919           79    72          Escherichia coli     L3         79   \n",
       "11920           79    72          Escherichia coli     L3         79   \n",
       "11921           79    72          Escherichia coli     L3         79   \n",
       "11922           79    72                       NaN     L3         79   \n",
       "11923           79    72                       NaN     L3         79   \n",
       "11924           79    72                       NaN     L3         79   \n",
       "11925           79    72                       NaN     L3         79   \n",
       "11926           79    72                       NaN     L3         79   \n",
       "11927           79    72                       NaN     L3         79   \n",
       "11928           79    72                       NaN     L3         79   \n",
       "\n",
       "                 Protein  Protein name                    Pathway  \\\n",
       "7550                AtoB           NaN         Mevalonate Pathway   \n",
       "7551                AtoB           NaN         Mevalonate Pathway   \n",
       "7552          Bisabolene           NaN         Mevalonate Pathway   \n",
       "7553          Bisabolene           NaN         Mevalonate Pathway   \n",
       "7554                HMGR           NaN         Mevalonate Pathway   \n",
       "7555                HMGR           NaN         Mevalonate Pathway   \n",
       "7556                HMGS           NaN         Mevalonate Pathway   \n",
       "7557                HMGS           NaN         Mevalonate Pathway   \n",
       "7558                 Idi           NaN         Mevalonate Pathway   \n",
       "7559                 Idi           NaN         Mevalonate Pathway   \n",
       "7560                IspA           NaN         Mevalonate Pathway   \n",
       "7561                IspA           NaN         Mevalonate Pathway   \n",
       "7562   Limonene Synthase           NaN         Mevalonate Pathway   \n",
       "7563   Limonene Synthase           NaN         Mevalonate Pathway   \n",
       "7564                  MK           NaN         Mevalonate Pathway   \n",
       "7565                  MK           NaN         Mevalonate Pathway   \n",
       "7566                NudB           NaN         Mevalonate Pathway   \n",
       "7567                NudB           NaN         Mevalonate Pathway   \n",
       "7568                 PMD           NaN         Mevalonate Pathway   \n",
       "7569                 PMD           NaN         Mevalonate Pathway   \n",
       "7570                 PMK           NaN         Mevalonate Pathway   \n",
       "7571                 PMK           NaN         Mevalonate Pathway   \n",
       "7572                HMGR           NaN         Mevalonate Pathway   \n",
       "7573                HMGR           NaN         Mevalonate Pathway   \n",
       "7574                HMGS           NaN         Mevalonate Pathway   \n",
       "7575                HMGS           NaN         Mevalonate Pathway   \n",
       "7576                6PGD           NaN  Pentose Phosphate Pathway   \n",
       "7577                6PGD           NaN  Pentose Phosphate Pathway   \n",
       "7578                FRDA           NaN                 Mixed Acid   \n",
       "7579                CAPP           NaN                 Mixed Acid   \n",
       "...                  ...           ...                        ...   \n",
       "11899              ACON1           NaN   Tricarboxylic Acid Cycle   \n",
       "11900               MAO1           NaN                       Misc   \n",
       "11901               MAO1           NaN                       Misc   \n",
       "11902               TKT1           NaN  Pentose Phosphate Pathway   \n",
       "11903               TKT1           NaN  Pentose Phosphate Pathway   \n",
       "11904               ACSA           NaN              Acetate Cycle   \n",
       "11905               TKT2           NaN  Pentose Phosphate Pathway   \n",
       "11906               TKT2           NaN  Pentose Phosphate Pathway   \n",
       "11907                MQO           NaN   Tricarboxylic Acid Cycle   \n",
       "11908                MQO           NaN   Tricarboxylic Acid Cycle   \n",
       "11909              ACON2           NaN   Tricarboxylic Acid Cycle   \n",
       "11910              ACON2           NaN   Tricarboxylic Acid Cycle   \n",
       "11911               RPIB           NaN  Pentose Phosphate Pathway   \n",
       "11912               GPMI           NaN                 Glycolysis   \n",
       "11913               GPMI           NaN                 Glycolysis   \n",
       "11914                MDH           NaN   Tricarboxylic Acid Cycle   \n",
       "11915                MDH           NaN   Tricarboxylic Acid Cycle   \n",
       "11916               GPMA           NaN                 Glycolysis   \n",
       "11917               GPMA           NaN                 Glycolysis   \n",
       "11918               DHSC           NaN   Tricarboxylic Acid Cycle   \n",
       "11919               MAO2           NaN                       Misc   \n",
       "11920               MAO2           NaN                       Misc   \n",
       "11921               EUTD           NaN              Acetate Cycle   \n",
       "11922               GPPS           NaN         Mevalonate Pathway   \n",
       "11923               GPPS           NaN         Mevalonate Pathway   \n",
       "11924               AMPR           NaN          Antibiotic Marker   \n",
       "11925               AMPR           NaN          Antibiotic Marker   \n",
       "11926                Cam           NaN          Antibiotic Marker   \n",
       "11927                Cam           NaN          Antibiotic Marker   \n",
       "11928                BSA           NaN         Normalization Std.   \n",
       "\n",
       "                    Peptide  PeptideArea    ...     \\\n",
       "7550           LGDGQVYDVILR       6177.0    ...      \n",
       "7551            TFVFSQDEFPK       2032.0    ...      \n",
       "7552       ASQLAFPGENILDEAK       2595.0    ...      \n",
       "7553           YNVSPAIFDNFK       1298.0    ...      \n",
       "7554          SDVSALVELNIAK      23223.0    ...      \n",
       "7555         SVVAEATIPGDVVR      38458.0    ...      \n",
       "7556       GLVSDPAGSDALNVLK       5542.0    ...      \n",
       "7557            LEVGTETLIDK      22178.0    ...      \n",
       "7558               LSAFTQLK     543703.0    ...      \n",
       "7559     YELGVEITPPESIYPDFR     135523.0    ...      \n",
       "7560              HVPLDALER      22217.0    ...      \n",
       "7561          STYPALLGLEQAR       8944.0    ...      \n",
       "7562              GVNVIPYLR     141541.0    ...      \n",
       "7563         LADDLGTSVEEVSR      91539.0    ...      \n",
       "7564     QQIDDLLLPGNTNLPWTS       4083.0    ...      \n",
       "7565             SLVFQLFENK      15449.0    ...      \n",
       "7566                 VLMLQR       9657.0    ...      \n",
       "7567           WLDAPAAAALTK       1170.0    ...      \n",
       "7568            DASLPTLSQWK     174932.0    ...      \n",
       "7569          LYQLPQSTSEISR     175638.0    ...      \n",
       "7570         VQWLDVTQADWGVR      25253.0    ...      \n",
       "7571            YEAFVVGLSAR      57505.0    ...      \n",
       "7572             IADEAYPSIK       5441.0    ...      \n",
       "7573    LIGTIEVPMTLAIVGGGTK       1847.0    ...      \n",
       "7574            SGYEDAVDYNR         98.0    ...      \n",
       "7575             VLVIATDTAR       1420.0    ...      \n",
       "7576           EAYELVAPILTK     397692.0    ...      \n",
       "7577              NLALNIESR     250263.0    ...      \n",
       "7578         LGSNSLAELVVFGR      39034.0    ...      \n",
       "7579           ADLWLAEYYDQR      23956.0    ...      \n",
       "...                     ...          ...    ...      \n",
       "11899             VVIAESFER      53943.0    ...      \n",
       "11900             AWIQYQGFK      20819.0    ...      \n",
       "11901  TSAEALQQAIDDNFWQAEYR       9265.0    ...      \n",
       "11902             ALSMDAVQK      52695.0    ...      \n",
       "11903            TEEQLANIAR      57525.0    ...      \n",
       "11904            LVITSDEGVR       2967.0    ...      \n",
       "11905            EAILEAQSVK      51651.0    ...      \n",
       "11906            TVIGFGSPNK      40420.0    ...      \n",
       "11907          FVFIGAGGAALK       8780.0    ...      \n",
       "11908           LLQESGIPEAK    1169663.0    ...      \n",
       "11909               IPLIIGR     371250.0    ...      \n",
       "11910             TDVLIDEVR     171123.0    ...      \n",
       "11911              VVGLELAK       2532.0    ...      \n",
       "11912        AFFANPVLTGAVDK      53845.0    ...      \n",
       "11913               ILINSPK      81018.0    ...      \n",
       "11914               FGLSLVR     402482.0    ...      \n",
       "11915           LFGVTTLDIIR     448621.0    ...      \n",
       "11916        ELPLTESLALTIDR     167483.0    ...      \n",
       "11917          FTGWYDVDLSEK     110993.0    ...      \n",
       "11918    ISFVITVVLSLLAGVLVW       1943.0    ...      \n",
       "11919               GVIYQGR      21619.0    ...      \n",
       "11920                NVFGYR      19152.0    ...      \n",
       "11921              TPPDALEK       6022.0    ...      \n",
       "11922        AAPLLGLADYVAFR    1818481.0    ...      \n",
       "11923             EFSDELLNR    1048135.0    ...      \n",
       "11924          SALPAGWFIADK     171979.0    ...      \n",
       "11925          VGYIELDLNSGK     113026.0    ...      \n",
       "11926           FYPAFIHILAR     426382.0    ...      \n",
       "11927        ITGYTTVDISQWHR     308652.0    ...      \n",
       "11928           HLVDEPQNLIK     317537.0    ...      \n",
       "\n",
       "       PeptideCorrectedArea Normalized to BSA  ProteinArea  \\\n",
       "7550                                 0.020394       4104.5   \n",
       "7551                                 0.006709       4104.5   \n",
       "7552                                 0.008568       1946.5   \n",
       "7553                                 0.004286       1946.5   \n",
       "7554                                 0.075007      30588.0   \n",
       "7555                                 0.126976      30588.0   \n",
       "7556                                 0.018298      13860.0   \n",
       "7557                                 0.073224      13860.0   \n",
       "7558                                 1.793741     339361.5   \n",
       "7559                                 0.447178     339361.5   \n",
       "7560                                 0.073353      15523.0   \n",
       "7561                                 0.029150      15523.0   \n",
       "7562                                 0.467322     116540.0   \n",
       "7563                                 0.302232     116540.0   \n",
       "7564                                 0.013481       9766.0   \n",
       "7565                                 0.051008       9766.0   \n",
       "7566                                 0.031884       5413.5   \n",
       "7567                                 0.003863       5413.5   \n",
       "7568                                 0.573253     174631.5   \n",
       "7569                                 0.579899     174631.5   \n",
       "7570                                 0.082037      39064.5   \n",
       "7571                                 0.175920      39064.5   \n",
       "7572                                 0.017964       3644.0   \n",
       "7573                                 0.006098       3644.0   \n",
       "7574                                 0.000324        759.0   \n",
       "7575                                 0.004688        759.0   \n",
       "7576                                 1.313048     323324.0   \n",
       "7577                                 0.821971     323324.0   \n",
       "7578                                 0.127844      38721.0   \n",
       "7579                                 0.077457      40910.0   \n",
       "...                                       ...          ...   \n",
       "11899                                0.171001      54780.0   \n",
       "11900                                0.065997      15042.0   \n",
       "11901                                0.029370      15042.0   \n",
       "11902                                0.167044      55110.0   \n",
       "11903                                0.182356      55110.0   \n",
       "11904                                0.009405       2967.0   \n",
       "11905                                0.163399      45982.5   \n",
       "11906                                0.128132      45982.5   \n",
       "11907                                0.027833     589221.5   \n",
       "11908                                3.707860     589221.5   \n",
       "11909                                1.175169     270563.0   \n",
       "11910                                0.540213     270563.0   \n",
       "11911                                0.008027       2532.0   \n",
       "11912                                0.170690      67421.5   \n",
       "11913                                0.256766      67421.5   \n",
       "11914                                1.272115     424140.5   \n",
       "11915                                1.416956     424140.5   \n",
       "11916                                0.526436     138530.0   \n",
       "11917                                0.351851     138530.0   \n",
       "11918                                0.006159       1943.0   \n",
       "11919                                0.067965      20296.0   \n",
       "11920                                0.060712      20296.0   \n",
       "11921                                0.019090       6022.0   \n",
       "11922                                5.750858    1430948.0   \n",
       "11923                                3.321421    1430948.0   \n",
       "11924                                0.544081     141092.0   \n",
       "11925                                0.350449     141092.0   \n",
       "11926                                1.351641     366798.0   \n",
       "11927                                0.973876     366798.0   \n",
       "11928                                1.000000     315455.0   \n",
       "\n",
       "       ProteinArea Normalized to AmpR  ProteinArea Normalized to Cam  \\\n",
       "7550                           0.0229                         0.0145   \n",
       "7551                           0.0229                         0.0145   \n",
       "7552                           0.0109                         0.0069   \n",
       "7553                           0.0109                         0.0069   \n",
       "7554                           0.1709                         0.1079   \n",
       "7555                           0.1709                         0.1079   \n",
       "7556                           0.0775                         0.0489   \n",
       "7557                           0.0775                         0.0489   \n",
       "7558                           1.8964                         1.1967   \n",
       "7559                           1.8964                         1.1967   \n",
       "7560                           0.0867                         0.0547   \n",
       "7561                           0.0867                         0.0547   \n",
       "7562                           0.6512                         0.4110   \n",
       "7563                           0.6512                         0.4110   \n",
       "7564                           0.0546                         0.0344   \n",
       "7565                           0.0546                         0.0344   \n",
       "7566                           0.0303                         0.0191   \n",
       "7567                           0.0303                         0.0191   \n",
       "7568                           0.9759                         0.6158   \n",
       "7569                           0.9759                         0.6158   \n",
       "7570                           0.2183                         0.1378   \n",
       "7571                           0.2183                         0.1378   \n",
       "7572                           0.0204                         0.0129   \n",
       "7573                           0.0204                         0.0129   \n",
       "7574                           0.0042                         0.0027   \n",
       "7575                           0.0042                         0.0027   \n",
       "7576                           1.8068                         1.1402   \n",
       "7577                           1.8068                         1.1402   \n",
       "7578                           0.2164                         0.1365   \n",
       "7579                           0.2286                         0.1443   \n",
       "...                               ...                            ...   \n",
       "11899                          0.3883                         0.1493   \n",
       "11900                          0.1066                         0.0410   \n",
       "11901                          0.1066                         0.0410   \n",
       "11902                          0.3906                         0.1502   \n",
       "11903                          0.3906                         0.1502   \n",
       "11904                          0.0210                         0.0081   \n",
       "11905                          0.3259                         0.1254   \n",
       "11906                          0.3259                         0.1254   \n",
       "11907                          4.1762                         1.6064   \n",
       "11908                          4.1762                         1.6064   \n",
       "11909                          1.9176                         0.7376   \n",
       "11910                          1.9176                         0.7376   \n",
       "11911                          0.0179                         0.0069   \n",
       "11912                          0.4779                         0.1838   \n",
       "11913                          0.4779                         0.1838   \n",
       "11914                          3.0061                         1.1563   \n",
       "11915                          3.0061                         1.1563   \n",
       "11916                          0.9818                         0.3777   \n",
       "11917                          0.9818                         0.3777   \n",
       "11918                          0.0138                         0.0053   \n",
       "11919                          0.1438                         0.0553   \n",
       "11920                          0.1438                         0.0553   \n",
       "11921                          0.0427                         0.0164   \n",
       "11922                         10.1419                         3.9012   \n",
       "11923                         10.1419                         3.9012   \n",
       "11924                          1.0000                         0.3847   \n",
       "11925                          1.0000                         0.3847   \n",
       "11926                          2.5997                         1.0000   \n",
       "11927                          2.5997                         1.0000   \n",
       "11928                          2.2358                         0.8600   \n",
       "\n",
       "       ProteinArea Normalized to BSA  PrecursorMz  PrecursorCharge  \\\n",
       "7550                          0.0136            0                0   \n",
       "7551                          0.0136            0                0   \n",
       "7552                          0.0064            0                0   \n",
       "7553                          0.0064            0                0   \n",
       "7554                          0.1010            0                0   \n",
       "7555                          0.1010            0                0   \n",
       "7556                          0.0458            0                0   \n",
       "7557                          0.0458            0                0   \n",
       "7558                          1.1205            0                0   \n",
       "7559                          1.1205            0                0   \n",
       "7560                          0.0513            0                0   \n",
       "7561                          0.0513            0                0   \n",
       "7562                          0.3848            0                0   \n",
       "7563                          0.3848            0                0   \n",
       "7564                          0.0322            0                0   \n",
       "7565                          0.0322            0                0   \n",
       "7566                          0.0179            0                0   \n",
       "7567                          0.0179            0                0   \n",
       "7568                          0.5766            0                0   \n",
       "7569                          0.5766            0                0   \n",
       "7570                          0.1290            0                0   \n",
       "7571                          0.1290            0                0   \n",
       "7572                          0.0120            0                0   \n",
       "7573                          0.0120            0                0   \n",
       "7574                          0.0025            0                0   \n",
       "7575                          0.0025            0                0   \n",
       "7576                          1.0675            0                0   \n",
       "7577                          1.0675            0                0   \n",
       "7578                          0.1278            0                0   \n",
       "7579                          0.1351            0                0   \n",
       "...                              ...          ...              ...   \n",
       "11899                         0.1737            0                0   \n",
       "11900                         0.0477            0                0   \n",
       "11901                         0.0477            0                0   \n",
       "11902                         0.1747            0                0   \n",
       "11903                         0.1747            0                0   \n",
       "11904                         0.0094            0                0   \n",
       "11905                         0.1458            0                0   \n",
       "11906                         0.1458            0                0   \n",
       "11907                         1.8678            0                0   \n",
       "11908                         1.8678            0                0   \n",
       "11909                         0.8577            0                0   \n",
       "11910                         0.8577            0                0   \n",
       "11911                         0.0080            0                0   \n",
       "11912                         0.2137            0                0   \n",
       "11913                         0.2137            0                0   \n",
       "11914                         1.3445            0                0   \n",
       "11915                         1.3445            0                0   \n",
       "11916                         0.4391            0                0   \n",
       "11917                         0.4391            0                0   \n",
       "11918                         0.0062            0                0   \n",
       "11919                         0.0643            0                0   \n",
       "11920                         0.0643            0                0   \n",
       "11921                         0.0191            0                0   \n",
       "11922                         4.5361            0                0   \n",
       "11923                         4.5361            0                0   \n",
       "11924                         0.4473            0                0   \n",
       "11925                         0.4473            0                0   \n",
       "11926                         1.1628            0                0   \n",
       "11927                         1.1628            0                0   \n",
       "11928                         1.0000            0                0   \n",
       "\n",
       "       RetentionTime  Background  PeakRank  \n",
       "7550               0           0         0  \n",
       "7551               0           0         0  \n",
       "7552               0           0         0  \n",
       "7553               0           0         0  \n",
       "7554               0         505         0  \n",
       "7555               0           0         0  \n",
       "7556               0           0         0  \n",
       "7557               0           0         0  \n",
       "7558               0         420         0  \n",
       "7559               0          83         0  \n",
       "7560               0           0         0  \n",
       "7561               0         115         0  \n",
       "7562               0           0         0  \n",
       "7563               0           0         0  \n",
       "7564               0           0         0  \n",
       "7565               0           0         0  \n",
       "7566               0           0         0  \n",
       "7567               0           0         0  \n",
       "7568               0        1307         0  \n",
       "7569               0           0         0  \n",
       "7570               0         406         0  \n",
       "7571               0        4223         0  \n",
       "7572               0           0         0  \n",
       "7573               0           0         0  \n",
       "7574               0           0         0  \n",
       "7575               0           0         0  \n",
       "7576               0           0         0  \n",
       "7577               0        1307         0  \n",
       "7578               0         313         0  \n",
       "7579               0         496         0  \n",
       "...              ...         ...       ...  \n",
       "11899              0           0         0  \n",
       "11900              0           0         0  \n",
       "11901              0           0         0  \n",
       "11902              0           0         0  \n",
       "11903              0           0         0  \n",
       "11904              0           0         0  \n",
       "11905              0         106         0  \n",
       "11906              0           0         0  \n",
       "11907              0           0         0  \n",
       "11908              0           0         0  \n",
       "11909              0         537         0  \n",
       "11910              0         710         0  \n",
       "11911              0           0         0  \n",
       "11912              0           0         0  \n",
       "11913              0          20         0  \n",
       "11914              0        1187         0  \n",
       "11915              0        1635         0  \n",
       "11916              0        1416         0  \n",
       "11917              0           0         0  \n",
       "11918              0           0         0  \n",
       "11919              0         179         0  \n",
       "11920              0           0         0  \n",
       "11921              0           0         0  \n",
       "11922              0        4344         0  \n",
       "11923              0         376         0  \n",
       "11924              0         346         0  \n",
       "11925              0        2475         0  \n",
       "11926              0           0         0  \n",
       "11927              0        1438         0  \n",
       "11928              0        2082         0  \n",
       "\n",
       "[4077 rows x 23 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Strain', 'Time', 'Protein', 'ProteinArea']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>AtoB</th>\n",
       "      <th>HMGR</th>\n",
       "      <th>HMGS</th>\n",
       "      <th>MK</th>\n",
       "      <th>PMD</th>\n",
       "      <th>PMK</th>\n",
       "      <th>Idi</th>\n",
       "      <th>GPPS</th>\n",
       "      <th>Limonene Synthase</th>\n",
       "      <th>NudB</th>\n",
       "      <th>...</th>\n",
       "      <th>Intracellular Mevalonate (uM)</th>\n",
       "      <th>Limonene g/L</th>\n",
       "      <th>MEV-P extracellular (uM)</th>\n",
       "      <th>MEVALONATE extracellular (uM)</th>\n",
       "      <th>Mev-P (uM)</th>\n",
       "      <th>NAD (uM)</th>\n",
       "      <th>NADP (uM)</th>\n",
       "      <th>OD600</th>\n",
       "      <th>Pyruvate g/L</th>\n",
       "      <th>citrate (uM)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Strain</th>\n",
       "      <th>Time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"14\" valign=\"top\">L2</th>\n",
       "      <th>0</th>\n",
       "      <td>1268050.0</td>\n",
       "      <td>975303.0</td>\n",
       "      <td>1814537.0</td>\n",
       "      <td>66644.0</td>\n",
       "      <td>222086.0</td>\n",
       "      <td>38871.0</td>\n",
       "      <td>992709.0</td>\n",
       "      <td>238334.0</td>\n",
       "      <td>1016183.0</td>\n",
       "      <td>13853.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.892763</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.126264</td>\n",
       "      <td>0.046703</td>\n",
       "      <td>0.542593</td>\n",
       "      <td>0.081442</td>\n",
       "      <td>0.713</td>\n",
       "      <td>0.039607</td>\n",
       "      <td>0.021561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.019869</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11694977.0</td>\n",
       "      <td>5552278.0</td>\n",
       "      <td>12874533.0</td>\n",
       "      <td>856518.0</td>\n",
       "      <td>1120042.0</td>\n",
       "      <td>780656.0</td>\n",
       "      <td>1562460.0</td>\n",
       "      <td>11734492.0</td>\n",
       "      <td>15152786.0</td>\n",
       "      <td>35169.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.651481</td>\n",
       "      <td>0.002552</td>\n",
       "      <td>0.081159</td>\n",
       "      <td>123.751846</td>\n",
       "      <td>0.083406</td>\n",
       "      <td>1.585056</td>\n",
       "      <td>0.494897</td>\n",
       "      <td>2.213</td>\n",
       "      <td>0.049653</td>\n",
       "      <td>0.023301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.081884</td>\n",
       "      <td>149.519213</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001345</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>20400000.0</td>\n",
       "      <td>8593167.0</td>\n",
       "      <td>23209717.0</td>\n",
       "      <td>1595060.0</td>\n",
       "      <td>2920532.0</td>\n",
       "      <td>1230925.0</td>\n",
       "      <td>2730791.0</td>\n",
       "      <td>18687520.0</td>\n",
       "      <td>25800000.0</td>\n",
       "      <td>58942.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.287004</td>\n",
       "      <td>0.007850</td>\n",
       "      <td>0.097335</td>\n",
       "      <td>185.592712</td>\n",
       "      <td>0.102936</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.196</td>\n",
       "      <td>0.001417</td>\n",
       "      <td>0.040126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.086296</td>\n",
       "      <td>214.908581</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>20400000.0</td>\n",
       "      <td>7944106.0</td>\n",
       "      <td>22010089.0</td>\n",
       "      <td>1656186.0</td>\n",
       "      <td>3014555.0</td>\n",
       "      <td>1747295.0</td>\n",
       "      <td>2283673.0</td>\n",
       "      <td>15128321.0</td>\n",
       "      <td>21800000.0</td>\n",
       "      <td>51299.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.087343</td>\n",
       "      <td>0.015688</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.129306</td>\n",
       "      <td>1.779064</td>\n",
       "      <td>0.787821</td>\n",
       "      <td>3.617</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.145918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.239964</td>\n",
       "      <td>262.301596</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>17725856.0</td>\n",
       "      <td>6821902.0</td>\n",
       "      <td>18686861.0</td>\n",
       "      <td>1654815.0</td>\n",
       "      <td>3312130.0</td>\n",
       "      <td>1897376.0</td>\n",
       "      <td>2084517.0</td>\n",
       "      <td>13543911.0</td>\n",
       "      <td>20800000.0</td>\n",
       "      <td>41562.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.357319</td>\n",
       "      <td>0.029025</td>\n",
       "      <td>0.336950</td>\n",
       "      <td>278.958738</td>\n",
       "      <td>0.171746</td>\n",
       "      <td>1.763556</td>\n",
       "      <td>0.652359</td>\n",
       "      <td>3.767</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.062459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.552074</td>\n",
       "      <td>294.473796</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>20000000.0</td>\n",
       "      <td>6527289.0</td>\n",
       "      <td>18827893.0</td>\n",
       "      <td>1704014.0</td>\n",
       "      <td>3419861.0</td>\n",
       "      <td>2212389.0</td>\n",
       "      <td>2511286.0</td>\n",
       "      <td>15171025.0</td>\n",
       "      <td>19586312.0</td>\n",
       "      <td>53747.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.594335</td>\n",
       "      <td>0.033795</td>\n",
       "      <td>0.949274</td>\n",
       "      <td>334.485578</td>\n",
       "      <td>0.264717</td>\n",
       "      <td>1.524906</td>\n",
       "      <td>0.389644</td>\n",
       "      <td>3.849</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>18090900.0</td>\n",
       "      <td>5140768.0</td>\n",
       "      <td>15830407.0</td>\n",
       "      <td>1552782.0</td>\n",
       "      <td>3320835.0</td>\n",
       "      <td>2094512.0</td>\n",
       "      <td>2200888.0</td>\n",
       "      <td>14993057.0</td>\n",
       "      <td>19665648.0</td>\n",
       "      <td>52512.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.166683</td>\n",
       "      <td>0.045728</td>\n",
       "      <td>2.317261</td>\n",
       "      <td>366.109605</td>\n",
       "      <td>0.492476</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.630</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>17057570.0</td>\n",
       "      <td>4649280.0</td>\n",
       "      <td>13606759.0</td>\n",
       "      <td>1494543.0</td>\n",
       "      <td>3229677.0</td>\n",
       "      <td>2350300.0</td>\n",
       "      <td>1973428.0</td>\n",
       "      <td>17127800.0</td>\n",
       "      <td>20800000.0</td>\n",
       "      <td>47867.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.371739</td>\n",
       "      <td>0.056036</td>\n",
       "      <td>4.563582</td>\n",
       "      <td>372.366100</td>\n",
       "      <td>0.389230</td>\n",
       "      <td>0.957042</td>\n",
       "      <td>0.107615</td>\n",
       "      <td>3.630</td>\n",
       "      <td>0.005109</td>\n",
       "      <td>0.326586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>15361209.0</td>\n",
       "      <td>4145041.0</td>\n",
       "      <td>10316009.0</td>\n",
       "      <td>1288346.0</td>\n",
       "      <td>2835588.0</td>\n",
       "      <td>2454097.0</td>\n",
       "      <td>1670806.0</td>\n",
       "      <td>18437344.0</td>\n",
       "      <td>21400000.0</td>\n",
       "      <td>54072.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.462721</td>\n",
       "      <td>0.055729</td>\n",
       "      <td>7.200263</td>\n",
       "      <td>384.243095</td>\n",
       "      <td>0.535650</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.810</td>\n",
       "      <td>0.116730</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   AtoB       HMGR        HMGS         MK        PMD  \\\n",
       "Strain Time                                                            \n",
       "L2     0      1268050.0   975303.0   1814537.0    66644.0   222086.0   \n",
       "       2            NaN        NaN         NaN        NaN        NaN   \n",
       "       4     11694977.0  5552278.0  12874533.0   856518.0  1120042.0   \n",
       "       6            NaN        NaN         NaN        NaN        NaN   \n",
       "       8     20400000.0  8593167.0  23209717.0  1595060.0  2920532.0   \n",
       "       10           NaN        NaN         NaN        NaN        NaN   \n",
       "       12    20400000.0  7944106.0  22010089.0  1656186.0  3014555.0   \n",
       "       16           NaN        NaN         NaN        NaN        NaN   \n",
       "       18    17725856.0  6821902.0  18686861.0  1654815.0  3312130.0   \n",
       "       20           NaN        NaN         NaN        NaN        NaN   \n",
       "       24    20000000.0  6527289.0  18827893.0  1704014.0  3419861.0   \n",
       "       36    18090900.0  5140768.0  15830407.0  1552782.0  3320835.0   \n",
       "       48    17057570.0  4649280.0  13606759.0  1494543.0  3229677.0   \n",
       "       72    15361209.0  4145041.0  10316009.0  1288346.0  2835588.0   \n",
       "\n",
       "                   PMK        Idi        GPPS  Limonene Synthase     NudB  \\\n",
       "Strain Time                                                                 \n",
       "L2     0       38871.0   992709.0    238334.0          1016183.0  13853.0   \n",
       "       2           NaN        NaN         NaN                NaN      NaN   \n",
       "       4      780656.0  1562460.0  11734492.0         15152786.0  35169.0   \n",
       "       6           NaN        NaN         NaN                NaN      NaN   \n",
       "       8     1230925.0  2730791.0  18687520.0         25800000.0  58942.0   \n",
       "       10          NaN        NaN         NaN                NaN      NaN   \n",
       "       12    1747295.0  2283673.0  15128321.0         21800000.0  51299.0   \n",
       "       16          NaN        NaN         NaN                NaN      NaN   \n",
       "       18    1897376.0  2084517.0  13543911.0         20800000.0  41562.0   \n",
       "       20          NaN        NaN         NaN                NaN      NaN   \n",
       "       24    2212389.0  2511286.0  15171025.0         19586312.0  53747.0   \n",
       "       36    2094512.0  2200888.0  14993057.0         19665648.0  52512.0   \n",
       "       48    2350300.0  1973428.0  17127800.0         20800000.0  47867.0   \n",
       "       72    2454097.0  1670806.0  18437344.0         21400000.0  54072.0   \n",
       "\n",
       "                 ...       Intracellular Mevalonate (uM)  Limonene g/L  \\\n",
       "Strain Time      ...                                                     \n",
       "L2     0         ...                            0.892763      0.000000   \n",
       "       2         ...                                 NaN           NaN   \n",
       "       4         ...                            2.651481      0.002552   \n",
       "       6         ...                                 NaN           NaN   \n",
       "       8         ...                            3.287004      0.007850   \n",
       "       10        ...                                 NaN           NaN   \n",
       "       12        ...                            4.087343      0.015688   \n",
       "       16        ...                                 NaN           NaN   \n",
       "       18        ...                            3.357319      0.029025   \n",
       "       20        ...                                 NaN           NaN   \n",
       "       24        ...                            4.594335      0.033795   \n",
       "       36        ...                            5.166683      0.045728   \n",
       "       48        ...                            4.371739      0.056036   \n",
       "       72        ...                            5.462721      0.055729   \n",
       "\n",
       "             MEV-P extracellular (uM)  MEVALONATE extracellular (uM)  \\\n",
       "Strain Time                                                            \n",
       "L2     0                     0.000000                       0.126264   \n",
       "       2                          NaN                            NaN   \n",
       "       4                     0.081159                     123.751846   \n",
       "       6                     0.081884                     149.519213   \n",
       "       8                     0.097335                     185.592712   \n",
       "       10                    0.086296                     214.908581   \n",
       "       12                         NaN                            NaN   \n",
       "       16                    0.239964                     262.301596   \n",
       "       18                    0.336950                     278.958738   \n",
       "       20                    0.552074                     294.473796   \n",
       "       24                    0.949274                     334.485578   \n",
       "       36                    2.317261                     366.109605   \n",
       "       48                    4.563582                     372.366100   \n",
       "       72                    7.200263                     384.243095   \n",
       "\n",
       "             Mev-P (uM)  NAD (uM)  NADP (uM)  OD600  Pyruvate g/L  \\\n",
       "Strain Time                                                         \n",
       "L2     0       0.046703  0.542593   0.081442  0.713      0.039607   \n",
       "       2            NaN       NaN        NaN    NaN      0.019869   \n",
       "       4       0.083406  1.585056   0.494897  2.213      0.049653   \n",
       "       6            NaN       NaN        NaN    NaN      0.001345   \n",
       "       8       0.102936       NaN        NaN  3.196      0.001417   \n",
       "       10           NaN       NaN        NaN    NaN      0.000144   \n",
       "       12      0.129306  1.779064   0.787821  3.617      0.000044   \n",
       "       16           NaN       NaN        NaN    NaN      0.000000   \n",
       "       18      0.171746  1.763556   0.652359  3.767      0.000000   \n",
       "       20           NaN       NaN        NaN    NaN      0.000000   \n",
       "       24      0.264717  1.524906   0.389644  3.849      0.000000   \n",
       "       36      0.492476       NaN        NaN  3.630      0.000000   \n",
       "       48      0.389230  0.957042   0.107615  3.630      0.005109   \n",
       "       72      0.535650       NaN        NaN  3.810      0.116730   \n",
       "\n",
       "             citrate (uM)  \n",
       "Strain Time                \n",
       "L2     0         0.021561  \n",
       "       2              NaN  \n",
       "       4         0.023301  \n",
       "       6              NaN  \n",
       "       8         0.040126  \n",
       "       10             NaN  \n",
       "       12        0.145918  \n",
       "       16             NaN  \n",
       "       18        0.062459  \n",
       "       20             NaN  \n",
       "       24        0.111774  \n",
       "       36             NaN  \n",
       "       48        0.326586  \n",
       "       72             NaN  \n",
       "\n",
       "[14 rows x 32 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Time Series in Data Set:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/scipy/linalg/basic.py:1018: RuntimeWarning: internal gelsd driver lwork query error, required iwork dimension not returned. This is likely the result of LAPACK bug 0038, fixed in LAPACK 3.2.2 (released July 21, 2010). Falling back to 'gelss' driver.\n",
      "  warnings.warn(mesg, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "if data_type == 'simulated':\n",
    "    if pathway == 'limonene':\n",
    "        file_name = 'data/Fulld10000n0.csv'\n",
    "        y0 = [1e-5]*10\n",
    "        timeKey = 'Time'\n",
    "        df = pd.read_csv(file_name)\n",
    "        df = df.pivot_table(df,index=['Strain','Time'],aggfunc=np.sum)\n",
    "\n",
    "        strains = df.index.get_level_values(0).unique()\n",
    "        sample = random.sample(list(strains),training_strains+test_strains)\n",
    "\n",
    "        #create test df\n",
    "        test_df = df.loc[(sample[0:test_strains],slice(None))]\n",
    "\n",
    "        #create the training df\n",
    "        df = df.loc[(sample[test_strains:],slice(None))]\n",
    "\n",
    "        features = ['AtoB','HMGR','HMGS','MK','PMK','PMD','Idi','GPPS','LS']\n",
    "        targets=['Acetyl-CoA','Acetoacetyl-CoA','HMG-CoA','Mev','MevP','MevPP','IPP','DMAPP','GPP','Limonene']\n",
    "        specific_features = {'Acetyl-CoA':      ['AtoB','Acetyl-CoA','Acetoacetyl-CoA','HMGR'],\n",
    "                             'Acetoacetyl-CoA': ['AtoB','Acetyl-CoA','Acetoacetyl-CoA','HMGR','HMG-CoA'],\n",
    "                             'HMG-CoA':         ['Acetyl-CoA','Acetoacetyl-CoA','HMGR','HMGS','HMG-CoA'],\n",
    "                             'Mev':             ['Acetyl-CoA','Acetoacetyl-CoA','HMGS','HMG-CoA','MK','Mev','GPP','MevP'],\n",
    "                             'MevP':            ['MK','Mev','GPP','MevP','PMK'],\n",
    "                             'MevPP':           ['PMK','MevP','PMD','MevPP','Mev'],\n",
    "                             'IPP':             ['PMD','MevPP','MevP','Mev','Idi','IPP','GPPS','DMAPP'],\n",
    "                             'DMAPP':           ['Idi','IPP','GPPS','DMAPP'],\n",
    "                             'GPP':             ['GPPS','IPP','DMAPP','GPP','LS'],\n",
    "                             'Limonene':        ['LS','GPP']}\n",
    "\n",
    "        tsdf = generateTSDataSet(df,features,targets)\n",
    "\n",
    "    elif pathway=='small':\n",
    "        file_name = 'data/SmallKineticsd30000n0.csv'\n",
    "        y0 = [0.2]*3\n",
    "        timeKey = 'Time'\n",
    "        df = pd.read_csv(file_name)\n",
    "        df = df.pivot_table(df,index=['Strain','Time'],aggfunc=np.sum)\n",
    "\n",
    "        strains = df.index.get_level_values(0).unique()\n",
    "        sample = random.sample(list(strains),training_strains+test_strains)\n",
    "\n",
    "        #create test df\n",
    "        test_df = df.loc[(sample[0:test_strains],slice(None))]\n",
    "\n",
    "        #create the training df\n",
    "        df = df.loc[(sample[test_strains:],slice(None))]\n",
    "\n",
    "        features = ['e0','e1']\n",
    "        targets=['s0','s1','s2']\n",
    "        specific_features = {'s0': ['e0','e1','s0','s1','s2'],\n",
    "                             's1': ['e0','e1','s0','s1','s2'],\n",
    "                             's2': ['e0','e1','s0','s1','s2']}\n",
    "\n",
    "        tsdf = generateTSDataSet(df,features,targets)        \n",
    "    \n",
    "elif data_type == 'experimental':\n",
    "    if pathway == 'limonene':\n",
    "        #Parameters that Can Be Set:\n",
    "        strains = ['L1','L2','L3']\n",
    "        training_strains = ['L1','L3']\n",
    "        test_strains = ['L2']\n",
    "\n",
    "        #Define Machine Learning Targets & Features To Use\n",
    "\n",
    "        targets = ['Acetyl-CoA (uM)','HMG-CoA (uM)','Intracellular Mevalonate (uM)','Mev-P (uM)','IPP/DMAPP (uM)','Limonene g/L']\n",
    "        features = ['OD600','ATP (uM)','AtoB','Limonene Synthase','HMGR','HMGS','MK','PMK','PMD','GPP (uM)',\n",
    "                    'NAD (uM)','NADP (uM)','Acetate g/L','Pyruvate g/L','citrate (uM)']\n",
    "\n",
    "        specific_features = {'Acetyl-CoA (uM)':['OD600','ATP (uM)','AtoB','HMGR','HMGS', 'Acetyl-CoA (uM)', 'HMG-CoA (uM)','Pyruvate g/L','citrate (uM)'],\n",
    "                             'HMG-CoA (uM)':['OD600','HMG-CoA (uM)','HMGR','MK','Mev-P (uM)','Intracellular Mevalonate (uM)','NAD (uM)','NADP (uM)'],\n",
    "                             'Intracellular Mevalonate (uM)':['OD600','ATP (uM)','Intracellular Mevalonate (uM)','MK','PMK','Mev-P (uM)'],\n",
    "                             'Mev-P (uM)':['OD600','ATP (uM)','PMD','Mev-P (uM)','Intracellular Mevalonate (uM)','IPP/DMAPP (uM)'],\n",
    "                             'IPP/DMAPP (uM)':['OD600','ATP (uM)','IPP/DMAPP (uM)','Limonene Synthase','Limonene g/L'],\n",
    "                             'Limonene g/L':['OD600','ATP (uM)','Limonene Synthase','Limonene g/L','GPP (uM)']}\n",
    "\n",
    "    elif pathway == 'isopentenol':\n",
    "        #Parameters that Can Be Set:\n",
    "        strains = ['I1','I2','I3']\n",
    "        training_strains = ['I1','I3']\n",
    "        test_strains = ['I2']\n",
    "        \n",
    "        targets = ['Acetyl-CoA (uM)','HMG-CoA (uM)','Intracellular Mevalonate (uM)','Mev-P (uM)','IPP/DMAPP (uM)','Isopentenol g/L']\n",
    "        features = ['OD600','ATP (uM)','AtoB','NudB','HMGR','HMGS','MK','PMK','PMD',\n",
    "                    'NAD (uM)','NADP (uM)','Acetate g/L','Pyruvate g/L','citrate (uM)']\n",
    "\n",
    "        specific_features = {'Acetyl-CoA (uM)':['OD600','ATP (uM)','AtoB','HMGR','HMGS', 'Acetyl-CoA (uM)', 'HMG-CoA (uM)','Pyruvate g/L','citrate (uM)'],\n",
    "                             'HMG-CoA (uM)':['OD600','HMG-CoA (uM)','HMGR','MK','Mev-P (uM)','Intracellular Mevalonate (uM)','NAD (uM)','NADP (uM)'],\n",
    "                             'Intracellular Mevalonate (uM)':['OD600','ATP (uM)','Intracellular Mevalonate (uM)','MK','PMK','Mev-P (uM)'],\n",
    "                             'Mev-P (uM)':['OD600','ATP (uM)','PMD','Mev-P (uM)','Intracellular Mevalonate (uM)','IPP/DMAPP (uM)'],\n",
    "                             'IPP/DMAPP (uM)':['OD600','ATP (uM)','IPP/DMAPP (uM)','NudB','Isopentenol g/L'],\n",
    "                             'Isopentenol g/L':['OD600','ATP (uM)','NudB','Isopentenol g/L','IPP/DMAPP (uM)']}\n",
    "\n",
    "    \n",
    "    #Processing Data Files and Creating a Shared Dataframe\n",
    "    metabolite_file_name = 'data/time_series_metabolomics.csv'\n",
    "    protein_file_name = 'data/time_series_proteomics.csv'\n",
    "\n",
    "    mdf = pd.read_csv(metabolite_file_name)\n",
    "    pdf = pd.read_csv(protein_file_name)\n",
    "\n",
    "    #Format protein tables\n",
    "    proteins = ['AtoB','HMGR','HMGS','MK','PMD','PMK','Idi','GPPS','Limonene Synthase','NudB']\n",
    "    proteins = [('ProteinArea',protein) for protein in proteins]\n",
    "    columns = ['Strain','Time','Protein','ProteinArea']\n",
    "\n",
    "    pdf = pdf.loc[pdf['Strain'].isin(strains)]\n",
    "    pdf = pdf.loc[~pdf['Hour'].isin(['72C'])]\n",
    "    pdf['Hour'] = pdf['Hour'].convert_objects(convert_numeric=True)\n",
    "\n",
    "    pdf.rename(columns={'Hour': 'Time'}, inplace=True)    \n",
    "    display(pdf)\n",
    "    print(columns)\n",
    "\n",
    "    pdf = pdf[columns].pivot_table(pdf[columns],index=['Strain','Time'],columns='Protein',aggfunc=np.sum)\n",
    "    pdf = pdf[proteins]\n",
    "    pdf.columns = pdf.columns.get_level_values(1)\n",
    "    \n",
    "    #Format metabolite tables\n",
    "    metabolites = ['Acetyl-CoA (uM)','Acetyl-CoA extracellular (uM)',\n",
    "                   'Acetoacetyl-coA (uM)','Acetoacetyl-CoA extracellular (uM)',\n",
    "                   'HMG-CoA (uM)','HMG-CoA extracellular (uM)',\n",
    "                   'Intracellular Mevalonate (uM)','MEVALONATE extracellular (uM)',\n",
    "                   'Mev-P (uM)','MEV-P extracellular (uM)',\n",
    "                   'IPP/DMAPP (uM)','IPP/DMAPP extracellular (uM)',\n",
    "                   'GPP (uM)','GPP extracellular (uM)',\n",
    "                   'Limonene g/L','ATP (uM)','OD600','NAD (uM)','NADP (uM)',\n",
    "                   'Acetate g/L','Pyruvate g/L','citrate (uM)','Isopentenol g/L']\n",
    "\n",
    "    mdf = mdf.loc[mdf['Strain'].isin(strains)]\n",
    "    \n",
    "    mdf.rename(columns={'Hour': 'Time'}, inplace=True)\n",
    "    mdf = mdf[['Strain','Time'] + metabolites].pivot_table(mdf[['Strain','Time'] + metabolites],index=['Strain','Time'],aggfunc=np.sum)\n",
    "    \n",
    "    #Extracellular Metabolites (mg/L) | Intracelluar Metabolites (mg/L) |Combined Metabolites (mg/L)\n",
    "\n",
    "    #Join metabolite and protein tables\n",
    "    df = pd.concat([pdf, mdf], axis=1)\n",
    "    test_df = df.loc[(test_strains,slice(None))]\n",
    "    display(test_df)\n",
    "    df = df.loc[(training_strains,slice(None))]\n",
    "    tsdf = generateTSDataSet(df,features,targets)#,n_points=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>AtoB</th>\n",
       "      <th>HMGR</th>\n",
       "      <th>HMGS</th>\n",
       "      <th>MK</th>\n",
       "      <th>PMD</th>\n",
       "      <th>PMK</th>\n",
       "      <th>Idi</th>\n",
       "      <th>GPPS</th>\n",
       "      <th>Limonene Synthase</th>\n",
       "      <th>NudB</th>\n",
       "      <th>...</th>\n",
       "      <th>Intracellular Mevalonate (uM)</th>\n",
       "      <th>Limonene g/L</th>\n",
       "      <th>MEV-P extracellular (uM)</th>\n",
       "      <th>MEVALONATE extracellular (uM)</th>\n",
       "      <th>Mev-P (uM)</th>\n",
       "      <th>NAD (uM)</th>\n",
       "      <th>NADP (uM)</th>\n",
       "      <th>OD600</th>\n",
       "      <th>Pyruvate g/L</th>\n",
       "      <th>citrate (uM)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Strain</th>\n",
       "      <th>Time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"14\" valign=\"top\">L1</th>\n",
       "      <th>0</th>\n",
       "      <td>8209.0</td>\n",
       "      <td>68464.0</td>\n",
       "      <td>29238.0</td>\n",
       "      <td>19532.0</td>\n",
       "      <td>349263.0</td>\n",
       "      <td>78129.0</td>\n",
       "      <td>678723.0</td>\n",
       "      <td>7934.0</td>\n",
       "      <td>233080.0</td>\n",
       "      <td>10827.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003715</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>23.398599</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.641765</td>\n",
       "      <td>0.076970</td>\n",
       "      <td>1.325506</td>\n",
       "      <td>0.114881</td>\n",
       "      <td>0.042705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.104989</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1484471.0</td>\n",
       "      <td>223652.0</td>\n",
       "      <td>492929.0</td>\n",
       "      <td>50037.0</td>\n",
       "      <td>557136.0</td>\n",
       "      <td>32024.0</td>\n",
       "      <td>882715.0</td>\n",
       "      <td>1279279.0</td>\n",
       "      <td>14184519.0</td>\n",
       "      <td>48234.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.161369</td>\n",
       "      <td>0.002417</td>\n",
       "      <td>0.260874</td>\n",
       "      <td>3.833160</td>\n",
       "      <td>0.516691</td>\n",
       "      <td>2.462201</td>\n",
       "      <td>0.188178</td>\n",
       "      <td>2.967000</td>\n",
       "      <td>0.644858</td>\n",
       "      <td>0.129697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.336856</td>\n",
       "      <td>6.056747</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.694666</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1614585.0</td>\n",
       "      <td>204481.0</td>\n",
       "      <td>483006.0</td>\n",
       "      <td>51045.0</td>\n",
       "      <td>656509.0</td>\n",
       "      <td>19615.0</td>\n",
       "      <td>995526.0</td>\n",
       "      <td>1242422.0</td>\n",
       "      <td>14914904.0</td>\n",
       "      <td>43116.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.170785</td>\n",
       "      <td>0.007934</td>\n",
       "      <td>0.590045</td>\n",
       "      <td>8.055140</td>\n",
       "      <td>0.756921</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.361000</td>\n",
       "      <td>0.583925</td>\n",
       "      <td>0.071615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.996996</td>\n",
       "      <td>8.858314</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.286490</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1560691.0</td>\n",
       "      <td>181504.0</td>\n",
       "      <td>474976.0</td>\n",
       "      <td>53706.0</td>\n",
       "      <td>692486.0</td>\n",
       "      <td>19545.0</td>\n",
       "      <td>934538.0</td>\n",
       "      <td>1205546.0</td>\n",
       "      <td>13691535.0</td>\n",
       "      <td>40161.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.168154</td>\n",
       "      <td>0.014846</td>\n",
       "      <td>0.635044</td>\n",
       "      <td>75.074896</td>\n",
       "      <td>0.804748</td>\n",
       "      <td>2.895685</td>\n",
       "      <td>0.236927</td>\n",
       "      <td>3.285000</td>\n",
       "      <td>0.145497</td>\n",
       "      <td>0.064193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.516719</td>\n",
       "      <td>9.814299</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.022264</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1937084.0</td>\n",
       "      <td>218177.0</td>\n",
       "      <td>590173.0</td>\n",
       "      <td>66428.0</td>\n",
       "      <td>842557.0</td>\n",
       "      <td>18964.0</td>\n",
       "      <td>1233368.0</td>\n",
       "      <td>1609621.0</td>\n",
       "      <td>16143199.0</td>\n",
       "      <td>59525.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.142823</td>\n",
       "      <td>0.018471</td>\n",
       "      <td>1.547755</td>\n",
       "      <td>9.747147</td>\n",
       "      <td>2.333130</td>\n",
       "      <td>2.241478</td>\n",
       "      <td>0.212471</td>\n",
       "      <td>3.449000</td>\n",
       "      <td>0.006632</td>\n",
       "      <td>0.033405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.360717</td>\n",
       "      <td>9.663816</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.007130</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1477381.0</td>\n",
       "      <td>177012.0</td>\n",
       "      <td>483653.0</td>\n",
       "      <td>53693.0</td>\n",
       "      <td>668706.0</td>\n",
       "      <td>16031.0</td>\n",
       "      <td>885615.0</td>\n",
       "      <td>1279718.0</td>\n",
       "      <td>13275815.0</td>\n",
       "      <td>35031.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.180405</td>\n",
       "      <td>0.017365</td>\n",
       "      <td>3.174658</td>\n",
       "      <td>9.870779</td>\n",
       "      <td>2.284825</td>\n",
       "      <td>1.841208</td>\n",
       "      <td>0.227695</td>\n",
       "      <td>3.426000</td>\n",
       "      <td>0.009000</td>\n",
       "      <td>0.056512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1772569.0</td>\n",
       "      <td>184872.0</td>\n",
       "      <td>501223.0</td>\n",
       "      <td>55784.0</td>\n",
       "      <td>797741.0</td>\n",
       "      <td>23131.0</td>\n",
       "      <td>1025149.0</td>\n",
       "      <td>1270380.0</td>\n",
       "      <td>15309986.0</td>\n",
       "      <td>49631.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.089452</td>\n",
       "      <td>0.017365</td>\n",
       "      <td>3.663278</td>\n",
       "      <td>7.153915</td>\n",
       "      <td>0.743805</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.584000</td>\n",
       "      <td>0.013104</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1653221.0</td>\n",
       "      <td>175501.0</td>\n",
       "      <td>467626.0</td>\n",
       "      <td>54921.0</td>\n",
       "      <td>719763.0</td>\n",
       "      <td>16436.0</td>\n",
       "      <td>899851.0</td>\n",
       "      <td>1133490.0</td>\n",
       "      <td>13183372.0</td>\n",
       "      <td>40397.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.068482</td>\n",
       "      <td>0.017303</td>\n",
       "      <td>3.044194</td>\n",
       "      <td>5.062424</td>\n",
       "      <td>0.485417</td>\n",
       "      <td>0.965385</td>\n",
       "      <td>0.211330</td>\n",
       "      <td>3.584000</td>\n",
       "      <td>0.013924</td>\n",
       "      <td>0.041272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>1302122.0</td>\n",
       "      <td>143055.0</td>\n",
       "      <td>451549.0</td>\n",
       "      <td>51173.0</td>\n",
       "      <td>710718.0</td>\n",
       "      <td>18571.0</td>\n",
       "      <td>875530.0</td>\n",
       "      <td>936997.0</td>\n",
       "      <td>12169879.0</td>\n",
       "      <td>29451.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.069763</td>\n",
       "      <td>0.016883</td>\n",
       "      <td>2.061041</td>\n",
       "      <td>3.671399</td>\n",
       "      <td>0.179995</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.870000</td>\n",
       "      <td>0.015813</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"14\" valign=\"top\">L3</th>\n",
       "      <th>0</th>\n",
       "      <td>50439.0</td>\n",
       "      <td>122581.0</td>\n",
       "      <td>76314.0</td>\n",
       "      <td>27218.0</td>\n",
       "      <td>92311.0</td>\n",
       "      <td>211927.0</td>\n",
       "      <td>325994.0</td>\n",
       "      <td>297412.0</td>\n",
       "      <td>319950.0</td>\n",
       "      <td>4126.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036670</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078128</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018219</td>\n",
       "      <td>0.628496</td>\n",
       "      <td>0.056779</td>\n",
       "      <td>1.377974</td>\n",
       "      <td>0.119581</td>\n",
       "      <td>0.030871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.004638</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1243475.0</td>\n",
       "      <td>357869.0</td>\n",
       "      <td>792739.0</td>\n",
       "      <td>834097.0</td>\n",
       "      <td>1889751.0</td>\n",
       "      <td>588087.0</td>\n",
       "      <td>2778685.0</td>\n",
       "      <td>4043845.0</td>\n",
       "      <td>11680415.0</td>\n",
       "      <td>31528.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.700392</td>\n",
       "      <td>0.022377</td>\n",
       "      <td>0.186663</td>\n",
       "      <td>20.248786</td>\n",
       "      <td>2.769003</td>\n",
       "      <td>2.352336</td>\n",
       "      <td>0.169020</td>\n",
       "      <td>2.893000</td>\n",
       "      <td>0.036815</td>\n",
       "      <td>0.053163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.222984</td>\n",
       "      <td>31.900264</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.008440</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1637580.0</td>\n",
       "      <td>441403.0</td>\n",
       "      <td>1025023.0</td>\n",
       "      <td>1111356.0</td>\n",
       "      <td>2881791.0</td>\n",
       "      <td>750621.0</td>\n",
       "      <td>3357679.0</td>\n",
       "      <td>3753584.0</td>\n",
       "      <td>13418025.0</td>\n",
       "      <td>9948.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.848760</td>\n",
       "      <td>0.052849</td>\n",
       "      <td>0.272959</td>\n",
       "      <td>49.630448</td>\n",
       "      <td>2.790022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.499000</td>\n",
       "      <td>0.011203</td>\n",
       "      <td>0.040150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.484502</td>\n",
       "      <td>65.861372</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005433</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1751133.0</td>\n",
       "      <td>497937.0</td>\n",
       "      <td>1041544.0</td>\n",
       "      <td>1430725.0</td>\n",
       "      <td>3343904.0</td>\n",
       "      <td>959681.0</td>\n",
       "      <td>3922590.0</td>\n",
       "      <td>3156798.0</td>\n",
       "      <td>14887581.0</td>\n",
       "      <td>47360.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.945150</td>\n",
       "      <td>0.085682</td>\n",
       "      <td>1.752860</td>\n",
       "      <td>118.672340</td>\n",
       "      <td>1.490390</td>\n",
       "      <td>2.268526</td>\n",
       "      <td>0.219631</td>\n",
       "      <td>3.701000</td>\n",
       "      <td>0.004620</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.638599</td>\n",
       "      <td>96.470107</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003214</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1588690.0</td>\n",
       "      <td>435496.0</td>\n",
       "      <td>966447.0</td>\n",
       "      <td>1579868.0</td>\n",
       "      <td>3835400.0</td>\n",
       "      <td>1043792.0</td>\n",
       "      <td>3962489.0</td>\n",
       "      <td>2710574.0</td>\n",
       "      <td>15278968.0</td>\n",
       "      <td>44980.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.991804</td>\n",
       "      <td>0.186370</td>\n",
       "      <td>0.773046</td>\n",
       "      <td>100.771021</td>\n",
       "      <td>2.896497</td>\n",
       "      <td>2.808285</td>\n",
       "      <td>0.303869</td>\n",
       "      <td>3.727000</td>\n",
       "      <td>0.002582</td>\n",
       "      <td>0.070424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.901646</td>\n",
       "      <td>111.317533</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002405</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1714324.0</td>\n",
       "      <td>455960.0</td>\n",
       "      <td>1032127.0</td>\n",
       "      <td>1872053.0</td>\n",
       "      <td>4266870.0</td>\n",
       "      <td>1167795.0</td>\n",
       "      <td>4163940.0</td>\n",
       "      <td>2697381.0</td>\n",
       "      <td>15807578.0</td>\n",
       "      <td>35657.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.498061</td>\n",
       "      <td>0.235486</td>\n",
       "      <td>1.333476</td>\n",
       "      <td>134.232193</td>\n",
       "      <td>1.327356</td>\n",
       "      <td>2.487478</td>\n",
       "      <td>0.222971</td>\n",
       "      <td>3.849000</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.119009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1651317.0</td>\n",
       "      <td>457905.0</td>\n",
       "      <td>1079484.0</td>\n",
       "      <td>2057900.0</td>\n",
       "      <td>4686725.0</td>\n",
       "      <td>1309697.0</td>\n",
       "      <td>4052425.0</td>\n",
       "      <td>2891240.0</td>\n",
       "      <td>16342677.0</td>\n",
       "      <td>42943.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.931822</td>\n",
       "      <td>0.370229</td>\n",
       "      <td>2.039983</td>\n",
       "      <td>134.175128</td>\n",
       "      <td>2.192837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.530000</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1783767.0</td>\n",
       "      <td>461828.0</td>\n",
       "      <td>1086167.0</td>\n",
       "      <td>2168842.0</td>\n",
       "      <td>4928542.0</td>\n",
       "      <td>1408951.0</td>\n",
       "      <td>4271657.0</td>\n",
       "      <td>3108706.0</td>\n",
       "      <td>16580819.0</td>\n",
       "      <td>43636.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.213345</td>\n",
       "      <td>0.410789</td>\n",
       "      <td>3.815001</td>\n",
       "      <td>122.777736</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.721404</td>\n",
       "      <td>0.282933</td>\n",
       "      <td>3.530000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.059404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>1651169.0</td>\n",
       "      <td>411109.0</td>\n",
       "      <td>981081.0</td>\n",
       "      <td>1919306.0</td>\n",
       "      <td>4782684.0</td>\n",
       "      <td>1234621.0</td>\n",
       "      <td>3748650.0</td>\n",
       "      <td>2861896.0</td>\n",
       "      <td>15602310.0</td>\n",
       "      <td>34378.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.369915</td>\n",
       "      <td>0.503867</td>\n",
       "      <td>13.694100</td>\n",
       "      <td>109.380163</td>\n",
       "      <td>9.433809</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.630000</td>\n",
       "      <td>0.000418</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  AtoB      HMGR       HMGS         MK        PMD        PMK  \\\n",
       "Strain Time                                                                    \n",
       "L1     0        8209.0   68464.0    29238.0    19532.0   349263.0    78129.0   \n",
       "       2           NaN       NaN        NaN        NaN        NaN        NaN   \n",
       "       4     1484471.0  223652.0   492929.0    50037.0   557136.0    32024.0   \n",
       "       6           NaN       NaN        NaN        NaN        NaN        NaN   \n",
       "       8     1614585.0  204481.0   483006.0    51045.0   656509.0    19615.0   \n",
       "       10          NaN       NaN        NaN        NaN        NaN        NaN   \n",
       "       12    1560691.0  181504.0   474976.0    53706.0   692486.0    19545.0   \n",
       "       16          NaN       NaN        NaN        NaN        NaN        NaN   \n",
       "       18    1937084.0  218177.0   590173.0    66428.0   842557.0    18964.0   \n",
       "       20          NaN       NaN        NaN        NaN        NaN        NaN   \n",
       "       24    1477381.0  177012.0   483653.0    53693.0   668706.0    16031.0   \n",
       "       36    1772569.0  184872.0   501223.0    55784.0   797741.0    23131.0   \n",
       "       48    1653221.0  175501.0   467626.0    54921.0   719763.0    16436.0   \n",
       "       72    1302122.0  143055.0   451549.0    51173.0   710718.0    18571.0   \n",
       "L3     0       50439.0  122581.0    76314.0    27218.0    92311.0   211927.0   \n",
       "       2           NaN       NaN        NaN        NaN        NaN        NaN   \n",
       "       4     1243475.0  357869.0   792739.0   834097.0  1889751.0   588087.0   \n",
       "       6           NaN       NaN        NaN        NaN        NaN        NaN   \n",
       "       8     1637580.0  441403.0  1025023.0  1111356.0  2881791.0   750621.0   \n",
       "       10          NaN       NaN        NaN        NaN        NaN        NaN   \n",
       "       12    1751133.0  497937.0  1041544.0  1430725.0  3343904.0   959681.0   \n",
       "       16          NaN       NaN        NaN        NaN        NaN        NaN   \n",
       "       18    1588690.0  435496.0   966447.0  1579868.0  3835400.0  1043792.0   \n",
       "       20          NaN       NaN        NaN        NaN        NaN        NaN   \n",
       "       24    1714324.0  455960.0  1032127.0  1872053.0  4266870.0  1167795.0   \n",
       "       36    1651317.0  457905.0  1079484.0  2057900.0  4686725.0  1309697.0   \n",
       "       48    1783767.0  461828.0  1086167.0  2168842.0  4928542.0  1408951.0   \n",
       "       72    1651169.0  411109.0   981081.0  1919306.0  4782684.0  1234621.0   \n",
       "\n",
       "                   Idi       GPPS  Limonene Synthase     NudB      ...       \\\n",
       "Strain Time                                                        ...        \n",
       "L1     0      678723.0     7934.0           233080.0  10827.0      ...        \n",
       "       2           NaN        NaN                NaN      NaN      ...        \n",
       "       4      882715.0  1279279.0         14184519.0  48234.0      ...        \n",
       "       6           NaN        NaN                NaN      NaN      ...        \n",
       "       8      995526.0  1242422.0         14914904.0  43116.0      ...        \n",
       "       10          NaN        NaN                NaN      NaN      ...        \n",
       "       12     934538.0  1205546.0         13691535.0  40161.0      ...        \n",
       "       16          NaN        NaN                NaN      NaN      ...        \n",
       "       18    1233368.0  1609621.0         16143199.0  59525.0      ...        \n",
       "       20          NaN        NaN                NaN      NaN      ...        \n",
       "       24     885615.0  1279718.0         13275815.0  35031.0      ...        \n",
       "       36    1025149.0  1270380.0         15309986.0  49631.0      ...        \n",
       "       48     899851.0  1133490.0         13183372.0  40397.0      ...        \n",
       "       72     875530.0   936997.0         12169879.0  29451.0      ...        \n",
       "L3     0      325994.0   297412.0           319950.0   4126.0      ...        \n",
       "       2           NaN        NaN                NaN      NaN      ...        \n",
       "       4     2778685.0  4043845.0         11680415.0  31528.0      ...        \n",
       "       6           NaN        NaN                NaN      NaN      ...        \n",
       "       8     3357679.0  3753584.0         13418025.0   9948.0      ...        \n",
       "       10          NaN        NaN                NaN      NaN      ...        \n",
       "       12    3922590.0  3156798.0         14887581.0  47360.0      ...        \n",
       "       16          NaN        NaN                NaN      NaN      ...        \n",
       "       18    3962489.0  2710574.0         15278968.0  44980.0      ...        \n",
       "       20          NaN        NaN                NaN      NaN      ...        \n",
       "       24    4163940.0  2697381.0         15807578.0  35657.0      ...        \n",
       "       36    4052425.0  2891240.0         16342677.0  42943.0      ...        \n",
       "       48    4271657.0  3108706.0         16580819.0  43636.0      ...        \n",
       "       72    3748650.0  2861896.0         15602310.0  34378.0      ...        \n",
       "\n",
       "             Intracellular Mevalonate (uM)  Limonene g/L  \\\n",
       "Strain Time                                                \n",
       "L1     0                          0.003715      0.000000   \n",
       "       2                               NaN           NaN   \n",
       "       4                          0.161369      0.002417   \n",
       "       6                               NaN           NaN   \n",
       "       8                          0.170785      0.007934   \n",
       "       10                              NaN           NaN   \n",
       "       12                         0.168154      0.014846   \n",
       "       16                              NaN           NaN   \n",
       "       18                         0.142823      0.018471   \n",
       "       20                              NaN           NaN   \n",
       "       24                         0.180405      0.017365   \n",
       "       36                         0.089452      0.017365   \n",
       "       48                         0.068482      0.017303   \n",
       "       72                         0.069763      0.016883   \n",
       "L3     0                          0.036670      0.000000   \n",
       "       2                               NaN           NaN   \n",
       "       4                          0.700392      0.022377   \n",
       "       6                               NaN           NaN   \n",
       "       8                          0.848760      0.052849   \n",
       "       10                              NaN           NaN   \n",
       "       12                         0.945150      0.085682   \n",
       "       16                              NaN           NaN   \n",
       "       18                         0.991804      0.186370   \n",
       "       20                              NaN           NaN   \n",
       "       24                         1.498061      0.235486   \n",
       "       36                         0.931822      0.370229   \n",
       "       48                         1.213345      0.410789   \n",
       "       72                         1.369915      0.503867   \n",
       "\n",
       "             MEV-P extracellular (uM)  MEVALONATE extracellular (uM)  \\\n",
       "Strain Time                                                            \n",
       "L1     0                     0.000000                      23.398599   \n",
       "       2                          NaN                            NaN   \n",
       "       4                     0.260874                       3.833160   \n",
       "       6                     0.336856                       6.056747   \n",
       "       8                     0.590045                       8.055140   \n",
       "       10                    0.996996                       8.858314   \n",
       "       12                    0.635044                      75.074896   \n",
       "       16                    1.516719                       9.814299   \n",
       "       18                    1.547755                       9.747147   \n",
       "       20                    2.360717                       9.663816   \n",
       "       24                    3.174658                       9.870779   \n",
       "       36                    3.663278                       7.153915   \n",
       "       48                    3.044194                       5.062424   \n",
       "       72                    2.061041                       3.671399   \n",
       "L3     0                     0.078128                       0.000000   \n",
       "       2                          NaN                            NaN   \n",
       "       4                     0.186663                      20.248786   \n",
       "       6                     0.222984                      31.900264   \n",
       "       8                     0.272959                      49.630448   \n",
       "       10                    0.484502                      65.861372   \n",
       "       12                    1.752860                     118.672340   \n",
       "       16                    0.638599                      96.470107   \n",
       "       18                    0.773046                     100.771021   \n",
       "       20                    0.901646                     111.317533   \n",
       "       24                    1.333476                     134.232193   \n",
       "       36                    2.039983                     134.175128   \n",
       "       48                    3.815001                     122.777736   \n",
       "       72                   13.694100                     109.380163   \n",
       "\n",
       "             Mev-P (uM)  NAD (uM)  NADP (uM)     OD600  Pyruvate g/L  \\\n",
       "Strain Time                                                            \n",
       "L1     0       0.000000  0.641765   0.076970  1.325506      0.114881   \n",
       "       2            NaN       NaN        NaN       NaN      0.104989   \n",
       "       4       0.516691  2.462201   0.188178  2.967000      0.644858   \n",
       "       6            NaN       NaN        NaN       NaN      0.694666   \n",
       "       8       0.756921       NaN        NaN  3.361000      0.583925   \n",
       "       10           NaN       NaN        NaN       NaN      0.286490   \n",
       "       12      0.804748  2.895685   0.236927  3.285000      0.145497   \n",
       "       16           NaN       NaN        NaN       NaN      0.022264   \n",
       "       18      2.333130  2.241478   0.212471  3.449000      0.006632   \n",
       "       20           NaN       NaN        NaN       NaN      0.007130   \n",
       "       24      2.284825  1.841208   0.227695  3.426000      0.009000   \n",
       "       36      0.743805       NaN        NaN  3.584000      0.013104   \n",
       "       48      0.485417  0.965385   0.211330  3.584000      0.013924   \n",
       "       72      0.179995       NaN        NaN  3.870000      0.015813   \n",
       "L3     0       0.018219  0.628496   0.056779  1.377974      0.119581   \n",
       "       2            NaN       NaN        NaN       NaN      0.004638   \n",
       "       4       2.769003  2.352336   0.169020  2.893000      0.036815   \n",
       "       6            NaN       NaN        NaN       NaN      0.008440   \n",
       "       8       2.790022       NaN        NaN  3.499000      0.011203   \n",
       "       10           NaN       NaN        NaN       NaN      0.005433   \n",
       "       12      1.490390  2.268526   0.219631  3.701000      0.004620   \n",
       "       16           NaN       NaN        NaN       NaN      0.003214   \n",
       "       18      2.896497  2.808285   0.303869  3.727000      0.002582   \n",
       "       20           NaN       NaN        NaN       NaN      0.002405   \n",
       "       24      1.327356  2.487478   0.222971  3.849000      0.000046   \n",
       "       36      2.192837       NaN        NaN  3.530000      0.000073   \n",
       "       48           NaN  1.721404   0.282933  3.530000      0.000000   \n",
       "       72      9.433809       NaN        NaN  3.630000      0.000418   \n",
       "\n",
       "             citrate (uM)  \n",
       "Strain Time                \n",
       "L1     0         0.042705  \n",
       "       2              NaN  \n",
       "       4         0.129697  \n",
       "       6              NaN  \n",
       "       8         0.071615  \n",
       "       10             NaN  \n",
       "       12        0.064193  \n",
       "       16             NaN  \n",
       "       18        0.033405  \n",
       "       20             NaN  \n",
       "       24        0.056512  \n",
       "       36             NaN  \n",
       "       48        0.041272  \n",
       "       72             NaN  \n",
       "L3     0         0.030871  \n",
       "       2              NaN  \n",
       "       4         0.053163  \n",
       "       6              NaN  \n",
       "       8         0.040150  \n",
       "       10             NaN  \n",
       "       12             NaN  \n",
       "       16             NaN  \n",
       "       18        0.070424  \n",
       "       20             NaN  \n",
       "       24        0.119009  \n",
       "       36             NaN  \n",
       "       48        0.059404  \n",
       "       72             NaN  \n",
       "\n",
       "[28 rows x 32 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Construct Machine Learning Models\n",
    "First Do Feature Selection if Automatic features are selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Automatic Feature Selection using Recursive Feature Selection cross validated with a Random Forest Regressor\n",
    "if feature_selection == 'automatic':\n",
    "    estimator = RandomForestRegressor()\n",
    "    specific_features = {}\n",
    "    feature_list = [('feature',feature) for feature in features + targets]\n",
    "    for target in targets:\n",
    "        \n",
    "        #display(tsdf[feature_list].ix[:100])\n",
    "        if data_type == 'simulated':\n",
    "            X = tsdf[feature_list].ix[:2000].values.tolist()\n",
    "            y = tsdf[('target',target)].ix[:2000].values.tolist()        \n",
    "        else:\n",
    "            X = tsdf[feature_list].values.tolist()\n",
    "            y = tsdf[('target',target)].values.tolist()\n",
    "            \n",
    "        selector = RFECV(estimator,verbose=1)\n",
    "        selector.fit(X,y)\n",
    "        mask = selector.get_support()\n",
    "        specific_features[target] = [feature for i,feature in enumerate(features + targets) if mask[i]]\n",
    "        print(target,specific_features[target])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define all Possible Machine Learning Models that can be used in the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: xgboost.XGBRegressor is not available and will not be used by TPOT.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "#Create Models for Each Target Column\n",
    "modelDict = {}\n",
    "\n",
    "#Pipeline Regressors\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "supportVectorRegressor = Pipeline([('Scaler',StandardScaler()),\n",
    "                               ('SVR',SVR())])\n",
    "\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
    "polynomialRegressor = Pipeline([('Scaler',StandardScaler()),\n",
    "                                ('Polynomial Features',PolynomialFeatures(degree=3, include_bias=True, interaction_only=True)),\n",
    "                                ('Feature Reduction',RFECV(Ridge(),cv=None, scoring='r2')),\n",
    "                                ('Linear Regressor',BaggingRegressor(base_estimator=Ridge(),\n",
    "                                                                     n_estimators=100, max_samples=.8,\n",
    "                                                                     bootstrap=False,\n",
    "                                                                     bootstrap_features=False,\n",
    "                                                                     random_state=None))])\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "featureReducingRF = Pipeline([('Feature Reduction',SelectKBest(mutual_info_regression, k=4)),\n",
    "                              ('Random Forest Regressor',RandomForestRegressor())])\n",
    "\n",
    "\n",
    "model_str = machine_learning_model\n",
    "featureReduction = False\n",
    "if model_str == 'SVR':\n",
    "    mlmodel = supportVectorRegressor\n",
    "elif model_str == 'RF':\n",
    "    mlmodel = RandomForestRegressor(n_estimators=20)\n",
    "elif model_str == 'Poly':\n",
    "    mlmodel = polynomialRegressor\n",
    "elif model_str == 'Bagging':\n",
    "    mlmodel = BaggingRegressor(base_estimator=supportVectorRegressor)\n",
    "elif model_str == 'FeatRF':\n",
    "    mlmodel = featureReducingRF \n",
    "    featureReduction = True\n",
    "elif model_str == 'Log':\n",
    "    LogScale = FunctionTransformer(np.log1p)\n",
    "    mlmodel = Pipeline([('Log Transform',LogScale),('Scaler',StandardScaler()),\n",
    "                        ('Linear',LassoLarsIC())])\n",
    "elif model_str == 'neural':\n",
    "    neural_model = Pipeline([('Scaler',StandardScaler()),\n",
    "                               ('neural_net',MLPRegressor(hidden_layer_sizes=(5,5,5),\n",
    "                                                          learning_rate_init=0.1,\n",
    "                                                          learning_rate='adaptive',\n",
    "                                                          solver='sgd',\n",
    "                                                          activation='tanh',\n",
    "                                                          max_iter=1000))])\n",
    "    #mlmodel = MLPRegressor(hidden_layer_sizes=(10,10),learning_rate_init=0.1,solver='lbfgs')\n",
    "    mlmodel= neural_model\n",
    "    \n",
    "elif model_str == 'gaussian':\n",
    "    mlmodel = Pipeline([('Scaler',StandardScaler()),\n",
    "                        ('gassian',GaussianProcessRegressor(normalize_y=True))])\n",
    "elif model_str == 'tpot':\n",
    "    from tpot import TPOTRegressor\n",
    "    #mlmodel = TPOTRegressor(generations=130, population_size=100, verbosity=2,max_time_mins=20,cv=ShuffleSplit(),scoring='r2',n_jobs=1)\n",
    "    mlmodel = TPOTRegressor(generations=130, population_size=130, verbosity=2,n_jobs=1)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Fitting & Evaluating Machine Learning Models\n",
    "\n",
    "For both simulated and real data we calculate the goodness of fit of the machine learning models. For Simulated Data we calculate the following Metrics which help us evaluate goodness of fit.\n",
    "\n",
    "Simulated Data Results:\n",
    "1. Learning Curves for Derivatives\n",
    "2. Error Distribution for Derivatives (Combined Vector Derivative Error RMSE)\n",
    "3. Goodness of Fit Plot For Simulated Curves\n",
    "4. RMSE for all simulated curves, Figure out the right one to report (Seperate and together -- Normalized / Percent / Regular)\n",
    "\n",
    "Experimental Data Results:\n",
    "1. Learning Curves for Derivatives\n",
    "2. Error Distribution for Derivatives, (Combined Vector)\n",
    "3. Comparison Plot between Real and Predicted Curves\n",
    "4. RMSE for Single Predicted Curve, (By Metabolite and Combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acetyl-CoA (uM)\n",
      "Warning: xgboost.XGBRegressor is not available and will not be used by TPOT.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:   1%|▏         | 236/17030 [00:30<39:50,  7.03pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1 - Current best internal CV score: 0.0005805269488547657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:   2%|▏         | 348/17030 [01:01<1:03:14,  4.40pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 2 - Current best internal CV score: 0.0005805269488547657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:   3%|▎         | 460/17030 [01:29<34:35,  7.98pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 3 - Current best internal CV score: 0.0005805269488547657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:   3%|▎         | 571/17030 [01:58<39:23,  6.96pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 4 - Current best internal CV score: 0.0005805269488547657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:   4%|▍         | 678/17030 [02:26<40:07,  6.79pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 5 - Current best internal CV score: 0.0005804365816296449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:   5%|▍         | 782/17030 [02:46<46:13,  5.86pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 6 - Current best internal CV score: 0.0005804365816296449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:   5%|▌         | 891/17030 [03:06<32:05,  8.38pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 7 - Current best internal CV score: 0.0005804365815354135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:   6%|▌         | 985/17030 [03:22<40:39,  6.58pipeline/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 8 - Current best internal CV score: 0.0005804365815354135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:   6%|▋         | 1084/17030 [03:42<46:53,  5.67pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 9 - Current best internal CV score: 0.000575202606807153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:   7%|▋         | 1167/17030 [04:00<58:27,  4.52pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 10 - Current best internal CV score: 0.000575202606807153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:   7%|▋         | 1262/17030 [04:22<56:39,  4.64pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 11 - Current best internal CV score: 0.000575202606807153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:   8%|▊         | 1352/17030 [04:45<1:02:34,  4.18pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 12 - Current best internal CV score: 0.000575202606807153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:   8%|▊         | 1446/17030 [05:09<58:11,  4.46pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 13 - Current best internal CV score: 0.000575202606807153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:   9%|▉         | 1547/17030 [05:30<28:57,  8.91pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 14 - Current best internal CV score: 0.000575202606807153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  10%|▉         | 1642/17030 [05:52<1:15:07,  3.41pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 15 - Current best internal CV score: 0.0005652086763916694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  10%|█         | 1733/17030 [06:08<33:44,  7.56pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 16 - Current best internal CV score: 0.0005652086763916694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  11%|█         | 1824/17030 [06:23<25:09, 10.07pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 17 - Current best internal CV score: 0.0005518829193856993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  11%|█▏        | 1918/17030 [06:44<31:58,  7.88pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 18 - Current best internal CV score: 0.0005518829151204493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  12%|█▏        | 2008/17030 [06:59<32:02,  7.82pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 19 - Current best internal CV score: 0.0005497538661305613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  12%|█▏        | 2097/17030 [07:13<31:25,  7.92pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 20 - Current best internal CV score: 0.0005497538661305613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  13%|█▎        | 2190/17030 [07:27<32:06,  7.70pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 21 - Current best internal CV score: 0.0005467952936887398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  13%|█▎        | 2277/17030 [07:40<36:41,  6.70pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 22 - Current best internal CV score: 0.0005467952936887398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  14%|█▍        | 2377/17030 [07:52<27:48,  8.78pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 23 - Current best internal CV score: 0.0005467952936887398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  14%|█▍        | 2465/17030 [08:05<34:19,  7.07pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 24 - Current best internal CV score: 0.0005467952936887398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  15%|█▌        | 2558/17030 [08:21<41:57,  5.75pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 25 - Current best internal CV score: 0.0005467952936887398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  16%|█▌        | 2645/17030 [08:35<26:30,  9.04pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 26 - Current best internal CV score: 0.0005467952936887398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  16%|█▌        | 2737/17030 [08:47<27:07,  8.78pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 27 - Current best internal CV score: 0.0005467952936887398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  17%|█▋        | 2823/17030 [09:01<39:10,  6.04pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 28 - Current best internal CV score: 0.0005467952936887398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  17%|█▋        | 2910/17030 [09:16<38:34,  6.10pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 29 - Current best internal CV score: 0.0005467952936887398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  18%|█▊        | 2994/17030 [09:32<56:58,  4.11pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 30 - Current best internal CV score: 0.0005452484689404785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  18%|█▊        | 3080/17030 [09:46<39:41,  5.86pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 31 - Current best internal CV score: 0.0005452484689404785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  19%|█▊        | 3173/17030 [09:59<30:36,  7.55pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 32 - Current best internal CV score: 0.0005452484689404785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  19%|█▉        | 3274/17030 [10:14<35:12,  6.51pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 33 - Current best internal CV score: 0.0005452484689404785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  20%|█▉        | 3369/17030 [10:30<28:21,  8.03pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 34 - Current best internal CV score: 0.0005452484660195791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  20%|██        | 3463/17030 [10:44<41:29,  5.45pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 35 - Current best internal CV score: 0.0005452484660195791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  21%|██        | 3561/17030 [10:59<35:00,  6.41pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 36 - Current best internal CV score: 0.0005452484660195791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  21%|██▏       | 3655/17030 [11:12<30:51,  7.22pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 37 - Current best internal CV score: 0.0005452484660195791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  22%|██▏       | 3753/17030 [11:27<25:22,  8.72pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 38 - Current best internal CV score: 0.0005452484660195791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  23%|██▎       | 3847/17030 [11:42<33:21,  6.59pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 39 - Current best internal CV score: 0.0005452484660195791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  23%|██▎       | 3956/17030 [12:02<40:42,  5.35pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 40 - Current best internal CV score: 0.0005442008925573761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  24%|██▍       | 4055/17030 [12:16<26:47,  8.07pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 41 - Current best internal CV score: 0.0005439087757524384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  24%|██▍       | 4148/17030 [12:29<35:46,  6.00pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 42 - Current best internal CV score: 0.0005439087757524384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  25%|██▍       | 4249/17030 [12:43<33:08,  6.43pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 43 - Current best internal CV score: 0.0005439087757524384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  25%|██▌       | 4337/17030 [12:57<26:39,  7.94pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 44 - Current best internal CV score: 0.0005439087757524384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  26%|██▌       | 4428/17030 [13:09<32:11,  6.52pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 45 - Current best internal CV score: 0.0005439087757524384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  27%|██▋       | 4525/17030 [13:21<30:44,  6.78pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 46 - Current best internal CV score: 0.0005439087757524384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  27%|██▋       | 4633/17030 [13:39<25:45,  8.02pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 47 - Current best internal CV score: 0.0005439087757524384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  28%|██▊       | 4736/17030 [13:53<30:18,  6.76pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 48 - Current best internal CV score: 0.0005439087757524384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  28%|██▊       | 4850/17030 [14:10<33:13,  6.11pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 49 - Current best internal CV score: 0.0005439087438371621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  29%|██▉       | 4957/17030 [14:25<25:49,  7.79pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 50 - Current best internal CV score: 0.0005439087438371621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  30%|██▉       | 5059/17030 [14:38<24:13,  8.24pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 51 - Current best internal CV score: 0.0005439087435879367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  30%|███       | 5169/17030 [14:56<28:59,  6.82pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 52 - Current best internal CV score: 0.0005439087435879367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  31%|███       | 5281/17030 [15:12<21:28,  9.12pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 53 - Current best internal CV score: 0.0005439087413763829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  32%|███▏      | 5389/17030 [15:25<29:39,  6.54pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 54 - Current best internal CV score: 0.0005439087385543602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  32%|███▏      | 5498/17030 [15:43<27:04,  7.10pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 55 - Current best internal CV score: 0.0005439087371400991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  33%|███▎      | 5606/17030 [16:01<22:19,  8.53pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 56 - Current best internal CV score: 0.0005439086085878709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  34%|███▎      | 5708/17030 [16:13<21:51,  8.63pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 57 - Current best internal CV score: 0.0005439086085878709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  34%|███▍      | 5809/17030 [16:28<26:31,  7.05pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 58 - Current best internal CV score: 0.0005439086085878709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  35%|███▍      | 5911/17030 [16:43<29:25,  6.30pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 59 - Current best internal CV score: 0.0005439086085878709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  35%|███▌      | 6021/17030 [16:59<17:03, 10.75pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 60 - Current best internal CV score: 0.0005439085844295242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  36%|███▌      | 6119/17030 [17:14<21:49,  8.33pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 61 - Current best internal CV score: 0.0005439085844295242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  37%|███▋      | 6228/17030 [17:29<30:22,  5.93pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 62 - Current best internal CV score: 0.0005439085844295242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  37%|███▋      | 6335/17030 [17:43<25:26,  7.00pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 63 - Current best internal CV score: 0.0005439085844295242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  38%|███▊      | 6444/17030 [18:00<21:06,  8.36pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 64 - Current best internal CV score: 0.0005439085844295242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  38%|███▊      | 6549/17030 [18:13<19:19,  9.04pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 65 - Current best internal CV score: 0.0005439083774926555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  39%|███▉      | 6645/17030 [18:27<27:40,  6.25pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 66 - Current best internal CV score: 0.0005439083774926555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  40%|███▉      | 6746/17030 [18:44<32:56,  5.20pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 67 - Current best internal CV score: 0.0005439083774926555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  40%|████      | 6856/17030 [19:03<21:00,  8.07pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 68 - Current best internal CV score: 0.0005439083774926555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  41%|████      | 6956/17030 [19:19<20:04,  8.37pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 69 - Current best internal CV score: 0.0005439083774407326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  41%|████▏     | 7063/17030 [19:37<31:39,  5.25pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 70 - Current best internal CV score: 0.0005439083774407326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  42%|████▏     | 7172/17030 [19:55<26:29,  6.20pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 71 - Current best internal CV score: 0.0005439083774407326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  43%|████▎     | 7270/17030 [20:14<29:47,  5.46pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 72 - Current best internal CV score: 0.0005439083774407326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  43%|████▎     | 7366/17030 [20:28<33:42,  4.78pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 73 - Current best internal CV score: 0.0005439083774268864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  44%|████▍     | 7464/17030 [20:41<34:44,  4.59pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 74 - Current best internal CV score: 0.0005439083774268864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  44%|████▍     | 7564/17030 [20:57<27:51,  5.66pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 75 - Current best internal CV score: 0.0005439083774268864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  45%|████▍     | 7661/17030 [21:11<25:00,  6.25pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 76 - Current best internal CV score: 0.0005439083774268864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  46%|████▌     | 7761/17030 [21:29<27:13,  5.67pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 77 - Current best internal CV score: 0.0005439083774268864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  46%|████▌     | 7860/17030 [21:44<22:25,  6.82pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 78 - Current best internal CV score: 0.0005439083117192453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  47%|████▋     | 7954/17030 [21:59<26:29,  5.71pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 79 - Current best internal CV score: 0.0005439083117192453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  47%|████▋     | 8053/17030 [22:13<23:35,  6.34pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 80 - Current best internal CV score: 0.0005439083117192453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  48%|████▊     | 8151/17030 [22:27<22:09,  6.68pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 81 - Current best internal CV score: 0.0005439083117192453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  48%|████▊     | 8250/17030 [22:41<19:30,  7.50pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 82 - Current best internal CV score: 0.0005439083117192453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  49%|████▉     | 8350/17030 [22:55<17:44,  8.15pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 83 - Current best internal CV score: 0.0005439083117192453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  50%|████▉     | 8446/17030 [23:09<22:50,  6.26pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 84 - Current best internal CV score: 0.0005439083117192453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  50%|█████     | 8542/17030 [23:25<19:58,  7.08pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 85 - Current best internal CV score: 0.0005439083117192453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  51%|█████     | 8632/17030 [23:41<20:41,  6.77pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 86 - Current best internal CV score: 0.0005439083117192453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  51%|█████▏    | 8728/17030 [23:55<18:37,  7.43pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 87 - Current best internal CV score: 0.0005439083117192453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  52%|█████▏    | 8821/17030 [24:10<26:52,  5.09pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 88 - Current best internal CV score: 0.0005439066843483726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  52%|█████▏    | 8922/17030 [24:28<22:44,  5.94pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 89 - Current best internal CV score: 0.0005439066843483726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  53%|█████▎    | 9005/17030 [24:42<25:59,  5.15pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 90 - Current best internal CV score: 0.0005439066843483726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  53%|█████▎    | 9108/17030 [24:58<18:14,  7.24pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 91 - Current best internal CV score: 0.0005439066843483726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  54%|█████▍    | 9207/17030 [25:14<22:15,  5.86pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 92 - Current best internal CV score: 0.0005439066843483726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  55%|█████▍    | 9306/17030 [25:29<19:13,  6.70pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 93 - Current best internal CV score: 0.0005439066843483726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  55%|█████▌    | 9387/17030 [25:39<17:14,  7.39pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 94 - Current best internal CV score: 0.0005439066843483726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  56%|█████▌    | 9476/17030 [25:53<18:25,  6.83pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 95 - Current best internal CV score: 0.0005439066843483726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  56%|█████▌    | 9564/17030 [26:04<19:17,  6.45pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 96 - Current best internal CV score: 0.0005439066843483726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  57%|█████▋    | 9643/17030 [26:22<29:06,  4.23pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 97 - Current best internal CV score: 0.0005439066843483726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  57%|█████▋    | 9736/17030 [26:35<15:32,  7.82pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 98 - Current best internal CV score: 0.0005439066843483726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  58%|█████▊    | 9827/17030 [26:46<16:25,  7.31pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 99 - Current best internal CV score: 0.0005439066818100566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  58%|█████▊    | 9928/17030 [27:04<18:32,  6.38pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 100 - Current best internal CV score: 0.0005439058203319322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  59%|█████▉    | 10006/17030 [27:16<19:16,  6.07pipeline/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 101 - Current best internal CV score: 0.0005439058203319322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  59%|█████▉    | 10098/17030 [27:33<16:56,  6.82pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 102 - Current best internal CV score: 0.0005439058203319322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  60%|█████▉    | 10179/17030 [27:46<14:53,  7.66pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 103 - Current best internal CV score: 0.0005439058203319322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  60%|██████    | 10260/17030 [27:58<18:26,  6.12pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 104 - Current best internal CV score: 0.0005439058203319322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  61%|██████    | 10355/17030 [28:10<12:19,  9.03pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 105 - Current best internal CV score: 0.0005439058203319322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  61%|██████▏   | 10449/17030 [28:26<16:22,  6.70pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 106 - Current best internal CV score: 0.0005439058203319322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  62%|██████▏   | 10536/17030 [28:39<16:50,  6.43pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 107 - Current best internal CV score: 0.0005439058203319322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  62%|██████▏   | 10638/17030 [28:56<18:30,  5.76pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 108 - Current best internal CV score: 0.0005439058203319322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  63%|██████▎   | 10728/17030 [29:12<14:29,  7.25pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 109 - Current best internal CV score: 0.0005439058203319322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  63%|██████▎   | 10814/17030 [29:25<15:52,  6.53pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 110 - Current best internal CV score: 0.0005439058203319322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  64%|██████▍   | 10909/17030 [29:39<13:58,  7.30pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 111 - Current best internal CV score: 0.0005439058203319322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  65%|██████▍   | 10998/17030 [29:56<16:04,  6.25pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 112 - Current best internal CV score: 0.0005439058203319322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  65%|██████▌   | 11086/17030 [30:08<13:15,  7.47pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 113 - Current best internal CV score: 0.0005439058203319322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  66%|██████▌   | 11169/17030 [30:19<14:12,  6.87pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 114 - Current best internal CV score: 0.0005439017659199243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  66%|██████▌   | 11253/17030 [30:31<16:04,  5.99pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 115 - Current best internal CV score: 0.0005439017659199243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  67%|██████▋   | 11350/17030 [30:45<13:19,  7.11pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 116 - Current best internal CV score: 0.0005439017659199243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  67%|██████▋   | 11434/17030 [30:58<12:56,  7.21pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 117 - Current best internal CV score: 0.0005439017622863995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  68%|██████▊   | 11520/17030 [31:09<16:39,  5.51pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 118 - Current best internal CV score: 0.0005439017622863995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  68%|██████▊   | 11614/17030 [31:25<15:46,  5.72pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 119 - Current best internal CV score: 0.0005439017601206995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  69%|██████▊   | 11699/17030 [31:38<19:58,  4.45pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 120 - Current best internal CV score: 0.0005439017601206995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  69%|██████▉   | 11799/17030 [31:53<12:48,  6.81pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 121 - Current best internal CV score: 0.0005439017601206995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  70%|██████▉   | 11887/17030 [32:04<13:26,  6.38pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 122 - Current best internal CV score: 0.0005439017601206995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  70%|███████   | 11990/17030 [32:22<18:47,  4.47pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 123 - Current best internal CV score: 0.000543901760101097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  71%|███████   | 12084/17030 [32:35<13:33,  6.08pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 124 - Current best internal CV score: 0.000543901760101097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best pipeline: ElasticNetCV(Nystroem(ElasticNetCV(LassoLarsCV(CombineDFs(StandardScaler(Nystroem(Binarizer(input_matrix, Binarizer__threshold=0.4), Nystroem__gamma=0.85, Nystroem__kernel=additive_chi2, Nystroem__n_components=10)), input_matrix), LassoLarsCV__normalize=DEFAULT), ElasticNetCV__l1_ratio=0.65, ElasticNetCV__tol=0.0001), Nystroem__gamma=0.4, Nystroem__kernel=linear, Nystroem__n_components=5), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.01)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.866e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.866e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.866e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.581e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.866e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.433e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.433e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.433e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.433e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.581e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.247e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.581e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 12 iterations, alpha=1.303e-04, previous alpha=1.152e-04, with an active set of 7 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.067e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.067e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.067e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.067e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=5.112e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=5.112e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=5.112e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=5.112e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 8 iterations, alpha=4.319e-04, previous alpha=3.929e-04, with an active set of 7 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 23 iterations, alpha=9.005e-04, previous alpha=4.493e-06, with an active set of 10 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=2.776e-06, with an active set of 10 regressors, and the smallest cholesky pivot element being 4.593e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=2.776e-06, with an active set of 10 regressors, and the smallest cholesky pivot element being 4.712e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=2.776e-06, with an active set of 10 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=2.776e-06, with an active set of 10 regressors, and the smallest cholesky pivot element being 5.053e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=2.776e-06, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.581e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=2.776e-06, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=2.776e-06, with an active set of 10 regressors, and the smallest cholesky pivot element being 4.470e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=1.064e-06, with an active set of 10 regressors, and the smallest cholesky pivot element being 4.470e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=2.445e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 3.799e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=2.445e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 5.053e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=2.445e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=2.445e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 4.712e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=2.445e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=2.445e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 4.344e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=2.445e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.356e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=8.906e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 6.322e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=8.906e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 5.960e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=8.906e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=8.906e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 6.495e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=8.906e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 5.475e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=5.679e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 6.664e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=5.679e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 6.909e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=5.679e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=5.679e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 6.747e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=5.679e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=5.679e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 6.580e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.838e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 6.909e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.838e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.838e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 6.664e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.838e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 6.747e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.838e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 6.580e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.036e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 6.829e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.036e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 6.664e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.036e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.036e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 6.747e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.036e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.036e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 6.580e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 10 iterations, alpha=8.138e-05, previous alpha=3.613e-05, with an active set of 9 regressors.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=8.590e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.581e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=8.590e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 5.576e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=8.590e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 6.322e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=8.590e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 6.144e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=8.590e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 6.409e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=8.590e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 5.475e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=8.590e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=3.980e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 5.576e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=3.980e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=3.980e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 6.322e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=3.980e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 6.144e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=3.980e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 5.373e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.229e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 5.576e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.229e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.229e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 6.409e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.229e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 6.322e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.229e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 6.234e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.229e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 5.373e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=7.156e-07, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.581e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=7.156e-07, with an active set of 11 regressors, and the smallest cholesky pivot element being 5.576e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=7.156e-07, with an active set of 11 regressors, and the smallest cholesky pivot element being 6.409e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=7.156e-07, with an active set of 11 regressors, and the smallest cholesky pivot element being 5.268e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.253e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 5.867e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.253e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 3.161e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.253e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 7.224e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.253e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 3.332e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.253e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.253e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.581e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.253e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 7.300e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=5.714e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 7.224e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=5.714e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.581e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=5.714e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=5.714e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 7.300e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=5.714e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 5.867e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=5.714e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 3.332e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=2.043e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 3.161e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=2.043e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 5.867e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=2.043e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 3.332e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=2.043e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=2.043e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 7.224e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=2.043e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.581e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=6.269e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 3.161e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=6.269e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 5.960e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=6.269e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 3.332e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=6.269e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 7.300e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=6.269e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=6.269e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 7.224e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=6.269e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.581e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.522e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 7.068e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.522e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 7.146e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.522e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.581e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.522e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.522e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 7.955e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.522e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.980e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 7 iterations, alpha=3.401e-04, previous alpha=8.137e-05, with an active set of 6 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=8.376e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 5.576e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=8.376e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 6.234e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=8.376e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 3.799e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=8.376e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 4.942e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=8.376e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 6.829e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=8.376e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 6.144e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=8.376e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 3.332e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=4.011e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 6.234e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=4.011e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 5.576e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=4.011e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 4.942e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=4.011e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 3.799e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=4.011e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 6.829e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=4.011e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 6.144e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=3.809e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 3.332e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.474e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 6.234e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.474e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 5.576e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.474e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 3.799e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.474e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 6.829e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.474e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 3.332e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.474e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 6.144e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=1.723e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 6.234e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=1.723e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 5.576e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=1.723e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 4.942e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=1.723e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 6.829e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=1.723e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 3.799e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=1.723e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 6.144e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=6.541e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.581e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=6.541e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 7.451e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=6.541e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 4.215e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=6.541e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 3.332e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 10 iterations, alpha=6.339e-05, previous alpha=2.577e-05, with an active set of 9 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.416e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 4.470e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.416e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 3.161e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.416e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.416e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 3.650e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.416e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 6.495e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.416e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.581e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.319e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.581e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.319e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 4.081e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 9 iterations, alpha=1.214e-04, previous alpha=1.028e-04, with an active set of 8 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=2.314e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=2.314e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=2.314e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 15 iterations, alpha=2.271e-05, previous alpha=7.758e-06, with an active set of 8 regressors.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.735e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.735e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.581e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.735e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.356e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.735e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.825e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.142e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.142e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 3.332e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.142e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.142e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.825e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.142e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 3.942e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 9 iterations, alpha=2.483e-04, previous alpha=8.816e-05, with an active set of 8 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=4.397e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.581e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=4.397e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 4.712e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=4.397e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=4.397e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=2.198e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.581e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=2.198e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 4.470e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=2.198e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=2.198e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 3.942e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=2.198e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=2.198e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.975e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=7.492e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=7.492e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=7.492e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.581e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=7.492e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 4.712e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=7.492e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 3.942e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=2.703e-06, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=2.703e-06, with an active set of 12 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=2.703e-06, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.581e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=2.703e-06, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=2.703e-06, with an active set of 12 regressors, and the smallest cholesky pivot element being 3.942e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.743e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.743e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.743e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.356e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.743e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.980e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=7.668e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.980e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=7.668e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=7.668e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.825e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=7.668e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=7.668e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.356e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 15 iterations, alpha=3.791e-05, previous alpha=3.618e-05, with an active set of 10 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.515e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.515e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.515e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.980e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.515e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 4.081e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.515e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 4.470e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.414e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=8.411e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 4.942e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=8.411e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 4.712e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=6.642e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 4.215e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=6.642e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=6.642e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 4.470e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=7.234e-07, with an active set of 12 regressors, and the smallest cholesky pivot element being 4.712e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=5.713e-07, with an active set of 12 regressors, and the smallest cholesky pivot element being 1.825e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=5.713e-07, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=5.713e-07, with an active set of 12 regressors, and the smallest cholesky pivot element being 4.215e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=5.713e-07, with an active set of 12 regressors, and the smallest cholesky pivot element being 3.650e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=6.480e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 3.799e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=6.480e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 4.942e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=2.749e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 3.650e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=2.749e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.581e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=2.749e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 3.332e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=2.749e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 5.674e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=2.749e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 3.942e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=2.647e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 3.942e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=2.599e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 4.081e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=2.599e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 4.829e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=5.321e-07, with an active set of 12 regressors, and the smallest cholesky pivot element being 3.799e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=5.321e-07, with an active set of 12 regressors, and the smallest cholesky pivot element being 4.942e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=4.222e-07, with an active set of 12 regressors, and the smallest cholesky pivot element being 3.332e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=4.222e-07, with an active set of 12 regressors, and the smallest cholesky pivot element being 3.942e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=4.222e-07, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.581e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=4.222e-07, with an active set of 12 regressors, and the smallest cholesky pivot element being 5.576e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=1.218e-05, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=1.218e-05, with an active set of 11 regressors, and the smallest cholesky pivot element being 5.960e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=8.188e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 6.495e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=8.188e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 6.664e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=8.188e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 8.816e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=8.188e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 6.144e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 9 iterations, alpha=7.731e-05, previous alpha=4.118e-05, with an active set of 8 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.266e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 7.885e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.266e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 8.816e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.266e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 8.093e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.266e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 8.229e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=7.604e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 8.297e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=7.604e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 3.650e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=7.604e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 8.495e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=7.604e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 8.093e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=7.604e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 7.955e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=7.604e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 8.363e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=7.604e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 8.816e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=2.217e-05, with an active set of 11 regressors, and the smallest cholesky pivot element being 8.816e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=2.217e-05, with an active set of 11 regressors, and the smallest cholesky pivot element being 7.885e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=2.217e-05, with an active set of 11 regressors, and the smallest cholesky pivot element being 8.229e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=2.217e-05, with an active set of 11 regressors, and the smallest cholesky pivot element being 3.650e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=2.217e-05, with an active set of 11 regressors, and the smallest cholesky pivot element being 8.495e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=2.217e-05, with an active set of 11 regressors, and the smallest cholesky pivot element being 8.093e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=2.217e-05, with an active set of 11 regressors, and the smallest cholesky pivot element being 4.215e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.042e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 9.828e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.042e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 8.752e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.042e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 3.650e-08\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.042e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 9.424e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 10 iterations, alpha=8.069e-05, previous alpha=3.285e-05, with an active set of 9 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.194e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.194e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.825e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.194e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.581e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.194e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.762e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.825e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.762e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 3.332e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.022e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.022e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.581e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.022e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=6.589e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 3.332e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=6.589e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.825e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=6.589e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=5.518e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 5.162e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 9 iterations, alpha=1.158e-04, previous alpha=3.527e-05, with an active set of 8 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=8.554e-06, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=8.554e-06, with an active set of 8 regressors, and the smallest cholesky pivot element being 3.332e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=4.277e-06, with an active set of 9 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=4.007e-06, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=4.007e-06, with an active set of 9 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=4.007e-06, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.581e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=4.007e-06, with an active set of 9 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=2.445e-06, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=2.310e-06, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=1.399e-06, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.581e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=6.372e-07, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.581e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=3.154e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 3.161e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.577e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.577e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.562e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 14 iterations, alpha=8.989e-05, previous alpha=6.999e-05, with an active set of 7 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=2.210e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=2.210e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 4.593e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=2.210e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 4.470e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=2.210e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=2.210e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 4.712e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=2.210e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.356e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=2.204e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 11 iterations, alpha=2.141e-04, previous alpha=1.589e-04, with an active set of 6 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=2.608e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 4.215e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=2.608e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 3.799e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=2.608e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 3.650e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=2.608e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.980e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.304e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.304e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.980e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.304e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 3.332e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.304e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 3.650e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.304e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 3.942e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.304e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 4.215e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=5.750e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 4.215e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=5.750e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 3.650e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=5.750e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.980e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=5.750e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 3.332e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=5.750e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=5.475e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=3.900e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 4.470e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=2.252e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 5.960e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=2.252e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 5.771e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=2.252e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 5.576e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=2.252e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=2.252e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.980e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 12 iterations, alpha=1.488e-05, previous alpha=1.155e-05, with an active set of 9 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.560e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.560e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=6.447e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=6.447e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=5.843e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 4.593e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 14 iterations, alpha=7.992e-05, previous alpha=3.379e-05, with an active set of 9 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.891e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 5.053e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.891e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 4.712e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.891e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 3.332e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.891e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.891e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 5.674e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=9.456e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 4.942e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=9.456e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 3.650e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=9.456e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 4.712e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=9.456e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 3.332e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=9.456e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 5.576e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=9.456e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=3.148e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 5.268e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=3.148e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 4.942e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=3.148e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=3.148e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 3.650e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=3.148e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 3.495e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=3.148e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 5.576e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=3.148e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 4.712e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 12 iterations, alpha=5.180e-05, previous alpha=2.356e-05, with an active set of 9 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.055e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 5.867e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.055e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 5.373e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.055e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.788e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.055e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.055e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 4.942e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.055e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 6.409e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=5.273e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 6.234e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=5.273e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 5.475e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=5.273e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.356e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=5.273e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 5.867e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=5.273e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 6.322e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=5.273e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.581e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=5.273e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.070e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 5.475e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.070e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 6.234e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.070e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.356e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.070e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.581e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=5.294e-06, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.581e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=5.294e-06, with an active set of 9 regressors, and the smallest cholesky pivot element being 6.409e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=5.294e-06, with an active set of 9 regressors, and the smallest cholesky pivot element being 5.867e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=5.294e-06, with an active set of 9 regressors, and the smallest cholesky pivot element being 5.576e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=5.294e-06, with an active set of 9 regressors, and the smallest cholesky pivot element being 6.234e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=2.059e-06, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.581e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=2.059e-06, with an active set of 10 regressors, and the smallest cholesky pivot element being 6.829e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=2.059e-06, with an active set of 10 regressors, and the smallest cholesky pivot element being 6.322e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=2.059e-06, with an active set of 10 regressors, and the smallest cholesky pivot element being 5.867e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=2.059e-06, with an active set of 10 regressors, and the smallest cholesky pivot element being 5.373e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=1.065e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=8.025e-07, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.581e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=8.025e-07, with an active set of 11 regressors, and the smallest cholesky pivot element being 6.322e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=8.025e-07, with an active set of 11 regressors, and the smallest cholesky pivot element being 5.867e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=8.025e-07, with an active set of 11 regressors, and the smallest cholesky pivot element being 6.989e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=8.025e-07, with an active set of 11 regressors, and the smallest cholesky pivot element being 6.234e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.529e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 7.451e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.529e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 7.598e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.529e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.529e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 8.025e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.529e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 3.332e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.411e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 8.025e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=2.990e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 7.598e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=2.990e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=2.990e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 3.332e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=2.990e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 8.025e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=2.990e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 7.451e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 13 iterations, alpha=1.172e-04, previous alpha=2.614e-05, with an active set of 10 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.072e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 5.053e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.072e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 4.829e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.072e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 4.593e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.072e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.825e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.072e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 3.161e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.072e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 3.332e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.072e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 6.322e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.036e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.825e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.036e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 3.161e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.036e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 4.712e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.036e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 4.942e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.036e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 5.576e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.036e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 4.344e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.036e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 6.144e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.036e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 3.650e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=8.270e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 3.650e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=3.515e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 3.799e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=3.515e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 4.829e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=3.515e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 6.144e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=3.515e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 4.344e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=3.515e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 5.576e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=3.515e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=3.515e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 3.161e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.557e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 3.650e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.557e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 4.829e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.557e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 4.344e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.557e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 6.144e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.557e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 5.576e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.557e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 1.825e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.557e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 3.161e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=5.467e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 3.650e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=5.467e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 6.322e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=5.467e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 4.942e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=5.467e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 4.593e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=5.467e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 4.712e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=5.467e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 5.162e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=5.467e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 3.161e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.607e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.581e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.607e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.788e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.607e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 6.322e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.607e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 7.146e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.607e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 5.771e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.607e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.607e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.607e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.356e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=7.911e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.356e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=5.388e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.356e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=4.800e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 7.068e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=4.800e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=4.800e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.581e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=4.800e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 5.576e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=4.800e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.788e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=4.800e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 6.234e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=4.800e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 9 iterations, alpha=1.107e-04, previous alpha=3.119e-05, with an active set of 8 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.053e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.980e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.053e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 6.747e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.053e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 8.025e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.053e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.053e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 7.146e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.053e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 6.989e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.053e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.356e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=4.893e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 8.025e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=4.893e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.980e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=4.893e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 6.747e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=4.893e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 7.146e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=4.893e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=4.893e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 6.989e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=2.311e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 6.989e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=2.070e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 8.025e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=2.070e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 6.829e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=2.070e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 6.580e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=2.070e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 3.332e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=2.070e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 7.068e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.674e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.788e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=5.548e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.980e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=3.809e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.980e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=3.809e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.356e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=3.809e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 6.747e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=3.809e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=3.809e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 6.989e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=3.809e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 7.224e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=3.809e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 8.162e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=9.272e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 4.712e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=9.272e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 7.300e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=9.272e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 9.064e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=9.272e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=9.272e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.581e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=9.272e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=9.272e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 4.829e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=3.927e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 4.829e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=3.597e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 4.829e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=3.293e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=3.293e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 4.829e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=3.293e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 4.470e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=3.293e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.980e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=3.293e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 7.300e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=3.293e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 9.064e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 11 iterations, alpha=3.830e-05, previous alpha=2.862e-05, with an active set of 8 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=2.702e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 7.376e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=2.702e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 8.752e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=2.702e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 3.332e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=2.702e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 6.234e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=2.702e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 8.878e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=2.702e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 9.424e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.013e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 7.376e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.013e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 8.752e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.013e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 3.332e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.013e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 8.878e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.013e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 9.424e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.013e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 6.747e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=1.713e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 7.451e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=1.713e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 8.878e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=1.713e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 3.332e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=1.713e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 9.424e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=1.713e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 6.664e-08\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.005e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.005e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.005e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=5.027e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=5.027e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=5.027e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.980e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=5.027e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.581e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=5.027e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=4.343e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=4.343e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=3.144e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 13 iterations, alpha=1.687e-04, previous alpha=1.680e-04, with an active set of 8 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=6.899e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=6.899e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=6.899e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.825e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=6.899e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=6.480e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=3.240e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=3.240e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=3.240e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.825e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=3.240e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 13 iterations, alpha=2.891e-04, previous alpha=1.911e-04, with an active set of 8 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=1.397e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.980e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=1.023e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=1.023e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=1.023e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 3.161e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=1.023e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 1.825e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=7.205e-06, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.581e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 21 iterations, alpha=1.023e-05, previous alpha=3.859e-06, with an active set of 10 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=4.364e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=4.364e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 4.829e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=4.364e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.356e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=4.364e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=4.364e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 3.799e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=4.364e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 3.942e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=4.364e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 4.712e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=4.364e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 4.215e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=3.326e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 4.215e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.106e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 4.829e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.106e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 1.825e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.106e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.106e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 4.712e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.106e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 4.081e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.106e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 4.470e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.106e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 3.942e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=7.743e-06, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=3.167e-06, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=3.980e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=3.980e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 4.470e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=3.980e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=3.980e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 4.215e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=3.980e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 3.161e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 11 iterations, alpha=2.826e-05, previous alpha=2.400e-05, with an active set of 8 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=6.865e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 3.332e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=6.865e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=6.865e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=6.865e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 3.495e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=6.865e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 3.799e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=6.865e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=6.865e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.972e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 7 iterations, alpha=6.819e-04, previous alpha=2.084e-04, with an active set of 6 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.478e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 4.081e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.478e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.478e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 3.650e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.478e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 3.799e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.478e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.478e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.980e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=4.396e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 3.650e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=4.396e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 4.215e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=4.396e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 4.081e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=4.396e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=4.396e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=4.396e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.581e-08\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.487e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 4.081e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.487e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.581e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.487e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 4.215e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.487e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 4.470e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.487e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.487e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 3.650e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=4.630e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=4.630e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.980e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=4.630e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 4.470e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=4.630e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 3.942e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=4.630e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 4.215e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=4.630e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.581e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=4.630e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 4.344e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 10 iterations, alpha=4.231e-05, previous alpha=3.127e-05, with an active set of 9 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.353e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 3.332e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.353e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 5.771e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.353e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.581e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.353e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 6.322e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.353e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.825e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.353e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 6.829e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=7.232e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 1.825e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=7.232e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.980e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=7.232e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 6.747e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=7.232e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 3.332e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=7.232e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.581e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=7.232e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 5.771e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=7.232e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 6.409e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 10 iterations, alpha=1.592e-04, previous alpha=6.086e-05, with an active set of 9 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.362e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.356e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.362e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 3.332e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.362e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 9.186e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.181e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 9.599e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.181e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 9.541e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.181e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 9.306e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.181e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 9.996e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.181e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.181e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 8.429e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=5.147e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 9.306e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=5.147e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=5.147e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 9.599e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=5.147e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 8.429e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=5.147e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.788e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 13 iterations, alpha=5.147e-05, previous alpha=3.553e-05, with an active set of 10 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=9.859e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 4.081e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=9.859e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 8.816e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=9.859e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.788e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=9.859e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=9.859e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=9.859e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 9.541e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=2.910e-06, with an active set of 12 regressors, and the smallest cholesky pivot element being 3.942e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=2.910e-06, with an active set of 12 regressors, and the smallest cholesky pivot element being 8.816e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=2.910e-06, with an active set of 12 regressors, and the smallest cholesky pivot element being 9.064e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=2.910e-06, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=2.910e-06, with an active set of 12 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=2.910e-06, with an active set of 12 regressors, and the smallest cholesky pivot element being 9.424e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=2.187e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.980e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=2.187e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.825e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=2.187e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=2.187e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=2.187e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=2.136e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.397e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 4.712e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 15 iterations, alpha=1.782e-04, previous alpha=1.263e-04, with an active set of 6 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 28 iterations, alpha=8.620e-04, previous alpha=6.636e-07, with an active set of 9 regressors.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=5.734e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.788e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=5.734e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=5.734e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 3.942e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=5.734e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 4.081e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=5.734e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=4.990e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 13 iterations, alpha=5.347e-05, previous alpha=4.990e-05, with an active set of 8 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=2.099e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.581e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=2.099e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=7.057e-07, with an active set of 11 regressors, and the smallest cholesky pivot element being 3.332e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=7.057e-07, with an active set of 11 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=7.057e-07, with an active set of 11 regressors, and the smallest cholesky pivot element being 3.942e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=7.057e-07, with an active set of 11 regressors, and the smallest cholesky pivot element being 3.495e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=2.088e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 5.162e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=2.088e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 4.942e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=2.088e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.980e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=2.088e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 5.960e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=2.088e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=7.559e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 5.162e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=7.559e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.980e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=7.559e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.581e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=7.559e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=3.116e-06, with an active set of 12 regressors, and the smallest cholesky pivot element being 5.373e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=3.116e-06, with an active set of 12 regressors, and the smallest cholesky pivot element being 6.144e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=3.116e-06, with an active set of 12 regressors, and the smallest cholesky pivot element being 4.942e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=3.116e-06, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.581e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=3.116e-06, with an active set of 12 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=2.663e-06, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.172e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 4.593e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.172e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.837e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 8 iterations, alpha=3.368e-04, previous alpha=1.748e-04, with an active set of 7 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.656e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 5.771e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.656e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.581e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.656e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.656e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 6.144e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.656e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 5.960e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 10 iterations, alpha=1.482e-04, previous alpha=1.321e-04, with an active set of 9 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=2.758e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.581e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=2.758e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 6.747e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=2.758e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 5.960e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=2.758e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.356e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=2.758e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=2.758e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.980e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 18 iterations, alpha=2.464e-05, previous alpha=8.953e-06, with an active set of 11 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.642e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 3.650e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.642e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 5.162e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.642e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.642e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 5.771e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.602e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 5.162e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.052e-04, with an active set of 9 regressors, and the smallest cholesky pivot element being 5.162e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 10 iterations, alpha=3.023e-04, previous alpha=9.088e-05, with an active set of 9 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=1.822e-05, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.356e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=1.822e-05, with an active set of 11 regressors, and the smallest cholesky pivot element being 7.068e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=1.822e-05, with an active set of 11 regressors, and the smallest cholesky pivot element being 3.650e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=1.822e-05, with an active set of 11 regressors, and the smallest cholesky pivot element being 5.674e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=1.822e-05, with an active set of 11 regressors, and the smallest cholesky pivot element being 4.593e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=1.822e-05, with an active set of 11 regressors, and the smallest cholesky pivot element being 4.829e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=6.716e-06, with an active set of 12 regressors, and the smallest cholesky pivot element being 4.593e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=6.716e-06, with an active set of 12 regressors, and the smallest cholesky pivot element being 5.674e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=6.716e-06, with an active set of 12 regressors, and the smallest cholesky pivot element being 3.495e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=6.716e-06, with an active set of 12 regressors, and the smallest cholesky pivot element being 7.068e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=6.716e-06, with an active set of 12 regressors, and the smallest cholesky pivot element being 4.829e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=6.716e-06, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=2.206e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 6.989e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=2.206e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 7.300e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=2.206e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.581e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=2.206e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 7.525e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=2.206e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 1.825e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=2.206e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.729e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=6.572e-07, with an active set of 12 regressors, and the smallest cholesky pivot element being 6.989e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=6.572e-07, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.581e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=6.572e-07, with an active set of 12 regressors, and the smallest cholesky pivot element being 7.300e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=6.572e-07, with an active set of 12 regressors, and the smallest cholesky pivot element being 7.525e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=6.572e-07, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=7.741e-06, with an active set of 10 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=7.741e-06, with an active set of 10 regressors, and the smallest cholesky pivot element being 3.161e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=7.741e-06, with an active set of 10 regressors, and the smallest cholesky pivot element being 3.332e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=7.741e-06, with an active set of 10 regressors, and the smallest cholesky pivot element being 7.068e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=7.741e-06, with an active set of 10 regressors, and the smallest cholesky pivot element being 5.475e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=7.741e-06, with an active set of 10 regressors, and the smallest cholesky pivot element being 7.598e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=7.741e-06, with an active set of 10 regressors, and the smallest cholesky pivot element being 7.885e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=7.741e-06, with an active set of 10 regressors, and the smallest cholesky pivot element being 7.814e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=3.654e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 3.332e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=3.654e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=3.654e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 7.068e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=3.654e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 7.598e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=3.654e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 5.053e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=3.654e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 7.814e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.053e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 7.743e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.053e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 7.224e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.053e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 7.451e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.053e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.053e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.581e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 9 iterations, alpha=9.906e-05, previous alpha=4.363e-05, with an active set of 8 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=9.350e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 7.300e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=9.350e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 3.161e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=9.350e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=9.350e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 3.495e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=9.350e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=9.350e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 7.224e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=4.399e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 7.224e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=4.399e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=4.399e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 3.161e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=4.399e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=4.399e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 3.650e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=4.399e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 7.300e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.607e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 7.451e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.607e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.607e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 3.161e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.607e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 3.650e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.607e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 3.495e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.607e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 7.376e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=5.618e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 7.300e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=5.618e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=5.618e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=5.618e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 3.650e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=5.618e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 3.495e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=5.618e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 7.224e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.031e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 6.580e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.031e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.031e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 8.429e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.031e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.980e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.031e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 5.373e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.031e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 8.878e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=3.762e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 8.878e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=3.762e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 6.495e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=3.762e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 5.373e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=3.762e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=3.762e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 8.560e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=3.762e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 7.814e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=3.762e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 6.409e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 10 iterations, alpha=6.411e-05, previous alpha=3.133e-05, with an active set of 9 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=3.085e-06, with an active set of 10 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=3.085e-06, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=3.085e-06, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.581e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 21 iterations, alpha=3.074e-06, previous alpha=2.990e-06, with an active set of 10 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=1.378e-06, with an active set of 10 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=1.378e-06, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=1.378e-06, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=1.378e-06, with an active set of 10 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.969e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.969e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.969e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.969e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.581e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.239e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.239e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.239e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.581e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.239e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.356e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 9 iterations, alpha=1.647e-04, previous alpha=6.023e-05, with an active set of 8 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=2.321e-07, with an active set of 11 regressors, and the smallest cholesky pivot element being 4.344e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=2.321e-07, with an active set of 11 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=2.321e-07, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=2.321e-07, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=2.321e-07, with an active set of 11 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=2.120e-07, with an active set of 11 regressors, and the smallest cholesky pivot element being 4.942e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=2.120e-07, with an active set of 11 regressors, and the smallest cholesky pivot element being 1.825e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=2.120e-07, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=2.120e-07, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=2.120e-07, with an active set of 11 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=2.120e-07, with an active set of 11 regressors, and the smallest cholesky pivot element being 4.215e-08\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.389e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 5.162e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.389e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 4.829e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.389e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 5.867e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.389e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 4.593e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.389e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 3.799e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.389e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 4.942e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 8 iterations, alpha=3.924e-04, previous alpha=7.013e-05, with an active set of 7 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=8.138e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 5.771e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=8.138e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=8.138e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.356e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=8.138e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=8.138e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=1.883e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=1.883e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 5.771e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=1.883e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=1.883e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 11 iterations, alpha=5.857e-05, previous alpha=1.131e-05, with an active set of 10 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.583e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.356e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.583e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.583e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 5.373e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.583e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 3.495e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.583e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.583e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.825e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=3.653e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.356e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=3.653e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 3.161e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=3.653e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 3.650e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=3.653e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 5.268e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=3.653e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=3.653e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=3.653e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 1.825e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 11 iterations, alpha=1.131e-04, previous alpha=3.374e-05, with an active set of 10 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.082e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.082e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 7.224e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.082e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.825e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.082e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 3.161e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.082e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.980e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.082e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 7.885e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 8 iterations, alpha=1.305e-04, previous alpha=1.267e-04, with an active set of 7 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=3.690e-06, with an active set of 12 regressors, and the smallest cholesky pivot element being 3.799e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=3.690e-06, with an active set of 12 regressors, and the smallest cholesky pivot element being 1.825e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=3.690e-06, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.980e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=3.690e-06, with an active set of 12 regressors, and the smallest cholesky pivot element being 6.409e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=3.690e-06, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.581e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=3.690e-06, with an active set of 12 regressors, and the smallest cholesky pivot element being 7.451e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=1.689e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.356e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=1.689e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.980e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=1.689e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 8.093e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=1.689e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 4.470e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=1.689e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 4.215e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=1.689e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=4.137e-07, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.356e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=4.137e-07, with an active set of 12 regressors, and the smallest cholesky pivot element being 7.814e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=4.137e-07, with an active set of 12 regressors, and the smallest cholesky pivot element being 4.470e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=4.137e-07, with an active set of 12 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=4.137e-07, with an active set of 12 regressors, and the smallest cholesky pivot element being 3.161e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=4.137e-07, with an active set of 12 regressors, and the smallest cholesky pivot element being 7.598e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.932e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.932e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 8.429e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.932e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 8.560e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.932e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 4.344e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.932e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 7.743e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.932e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 7.598e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=4.958e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.356e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=4.958e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 8.560e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=4.958e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 7.743e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=4.958e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 4.344e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=4.958e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 10 iterations, alpha=1.398e-04, previous alpha=3.873e-05, with an active set of 9 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=4.641e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 3.161e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=4.641e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 4.215e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=4.641e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 3.650e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=4.641e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 7.885e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=4.641e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=4.641e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 9.003e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=3.015e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 6.144e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=3.015e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 3.332e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=3.015e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 6.580e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=3.015e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 3.161e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=3.015e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 7.671e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=3.015e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.788e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.465e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 5.771e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.465e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 3.332e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.465e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.980e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.465e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 7.671e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.465e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.788e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.465e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 7.146e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=4.107e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 5.771e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=4.107e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 6.409e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=4.107e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.980e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=4.107e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 3.332e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=4.107e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 7.671e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=4.107e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.788e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=4.107e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 7.146e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=2.555e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 5.268e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=2.555e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 5.867e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=2.555e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 8.297e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=2.555e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 9.186e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=2.555e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=2.555e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 4.942e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=2.555e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 5.162e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=7.975e-06, with an active set of 10 regressors, and the smallest cholesky pivot element being 5.268e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=7.975e-06, with an active set of 10 regressors, and the smallest cholesky pivot element being 9.064e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=7.975e-06, with an active set of 10 regressors, and the smallest cholesky pivot element being 5.867e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=7.975e-06, with an active set of 10 regressors, and the smallest cholesky pivot element being 4.942e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=7.975e-06, with an active set of 10 regressors, and the smallest cholesky pivot element being 8.297e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=7.975e-06, with an active set of 10 regressors, and the smallest cholesky pivot element being 5.162e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=7.975e-06, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.788e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=1.782e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 5.373e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=1.782e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=1.782e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 5.867e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=1.782e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 5.053e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=1.782e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 5.162e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=1.782e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.788e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=1.782e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 8.162e-08\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=8.868e-04, with an active set of 2 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=4.434e-04, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.333e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 9 iterations, alpha=2.968e-04, previous alpha=1.180e-04, with an active set of 6 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.418e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 6 iterations, alpha=1.376e-03, previous alpha=1.176e-03, with an active set of 5 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=3.937e-06, with an active set of 10 regressors, and the smallest cholesky pivot element being 1.825e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=3.937e-06, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=3.937e-06, with an active set of 10 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.605e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.605e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.303e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.303e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=9.206e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 10 iterations, alpha=1.115e-04, previous alpha=3.395e-05, with an active set of 7 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=2.428e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 6.495e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=2.428e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 6.409e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=2.428e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.980e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=2.428e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 6.989e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=2.428e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=2.428e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 7.300e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.212e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 6.144e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.212e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 7.376e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.212e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 1.825e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.212e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 6.664e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.212e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 6.829e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.212e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=1.897e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 5.576e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=1.897e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 5.268e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=1.897e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=1.897e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 6.829e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=1.897e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=1.897e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.581e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=1.897e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 6.909e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.386e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.581e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.386e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 6.829e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.386e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 7.598e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.386e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.356e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.386e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 7.300e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.386e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.386e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 7.525e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 12 iterations, alpha=1.309e-05, previous alpha=8.623e-06, with an active set of 9 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=3.064e-06, with an active set of 10 regressors, and the smallest cholesky pivot element being 4.942e-08\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=3.064e-06, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=3.064e-06, with an active set of 10 regressors, and the smallest cholesky pivot element being 4.470e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=3.064e-06, with an active set of 10 regressors, and the smallest cholesky pivot element being 4.215e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=3.064e-06, with an active set of 10 regressors, and the smallest cholesky pivot element being 3.650e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=3.064e-06, with an active set of 10 regressors, and the smallest cholesky pivot element being 4.081e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=2.972e-06, with an active set of 10 regressors, and the smallest cholesky pivot element being 4.081e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=1.128e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 3.942e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=1.055e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 4.593e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=1.055e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 3.332e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=1.055e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 3.942e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=1.055e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 4.470e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=1.055e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=8.378e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=4.189e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=2.095e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=7.722e-06, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=3.286e-06, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=4.614e-07, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=5.516e-06, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=5.222e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=2.611e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=3.531e-07, with an active set of 9 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=1.452e-07, with an active set of 9 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=1.391e-07, with an active set of 9 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=3.399e-06, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=3.399e-06, with an active set of 9 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=3.399e-06, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.581e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 20 iterations, alpha=3.256e-06, previous alpha=2.898e-06, with an active set of 9 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=2.168e-03, with an active set of 1 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=2.168e-03, with an active set of 1 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 5 iterations, alpha=1.565e-03, previous alpha=1.361e-03, with an active set of 4 regressors.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=5.668e-06, with an active set of 9 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=5.668e-06, with an active set of 9 regressors, and the smallest cholesky pivot element being 3.495e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=5.668e-06, with an active set of 9 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=5.668e-06, with an active set of 9 regressors, and the smallest cholesky pivot element being 4.829e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=5.668e-06, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=5.668e-06, with an active set of 9 regressors, and the smallest cholesky pivot element being 4.712e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=3.845e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 4.712e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=1.746e-06, with an active set of 12 regressors, and the smallest cholesky pivot element being 4.593e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=1.215e-06, with an active set of 12 regressors, and the smallest cholesky pivot element being 3.495e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=1.215e-06, with an active set of 12 regressors, and the smallest cholesky pivot element being 4.942e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=1.215e-06, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=1.215e-06, with an active set of 12 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.000e-04, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.788e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.000e-04, with an active set of 9 regressors, and the smallest cholesky pivot element being 3.332e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.000e-04, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.356e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.000e-04, with an active set of 9 regressors, and the smallest cholesky pivot element being 6.989e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.000e-04, with an active set of 9 regressors, and the smallest cholesky pivot element being 5.053e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.000e-04, with an active set of 9 regressors, and the smallest cholesky pivot element being 1.825e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 11 iterations, alpha=9.345e-05, previous alpha=2.931e-05, with an active set of 10 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.653e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.653e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 3.495e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.653e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 5.162e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.653e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 4.942e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.653e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.980e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 9 iterations, alpha=1.534e-04, previous alpha=5.142e-05, with an active set of 8 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.192e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 5.268e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.192e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 5.867e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.192e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 4.081e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.192e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 3.650e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.192e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.980e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.192e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=6.169e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 4.344e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=6.169e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=6.169e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 3.650e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=6.169e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 6.234e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=6.169e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.980e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=6.169e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 5.268e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 10 iterations, alpha=2.526e-04, previous alpha=5.722e-05, with an active set of 9 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=1.006e-05, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.788e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=1.006e-05, with an active set of 11 regressors, and the smallest cholesky pivot element being 5.268e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=1.006e-05, with an active set of 11 regressors, and the smallest cholesky pivot element being 3.332e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=1.006e-05, with an active set of 11 regressors, and the smallest cholesky pivot element being 5.960e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=1.006e-05, with an active set of 11 regressors, and the smallest cholesky pivot element being 6.664e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=1.006e-05, with an active set of 11 regressors, and the smallest cholesky pivot element being 3.161e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=1.006e-05, with an active set of 11 regressors, and the smallest cholesky pivot element being 3.942e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=7.284e-06, with an active set of 10 regressors, and the smallest cholesky pivot element being 6.322e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=7.284e-06, with an active set of 10 regressors, and the smallest cholesky pivot element being 4.470e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=7.284e-06, with an active set of 10 regressors, and the smallest cholesky pivot element being 3.650e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=7.284e-06, with an active set of 10 regressors, and the smallest cholesky pivot element being 7.671e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=7.284e-06, with an active set of 10 regressors, and the smallest cholesky pivot element being 5.960e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=7.284e-06, with an active set of 10 regressors, and the smallest cholesky pivot element being 5.867e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=7.284e-06, with an active set of 10 regressors, and the smallest cholesky pivot element being 6.909e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=7.284e-06, with an active set of 10 regressors, and the smallest cholesky pivot element being 3.332e-08\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=7.273e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=7.273e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=7.273e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.325e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.325e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 8 iterations, alpha=3.429e-04, previous alpha=1.837e-04, with an active set of 7 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 27 iterations, alpha=8.202e-04, previous alpha=5.796e-07, with an active set of 8 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=2.100e-07, with an active set of 11 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=2.100e-07, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=2.100e-07, with an active set of 11 regressors, and the smallest cholesky pivot element being 3.332e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=2.100e-07, with an active set of 11 regressors, and the smallest cholesky pivot element being 3.161e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=9.960e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 5.576e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=9.960e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=9.960e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 5.373e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=9.960e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 5.674e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=9.960e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=9.960e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=9.960e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 5.268e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.653e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 6.234e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.653e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.653e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 5.576e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.653e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.653e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 4.942e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.653e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.653e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 6.053e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 7 iterations, alpha=1.540e-04, previous alpha=1.302e-04, with an active set of 6 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.982e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 4.081e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.982e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.581e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.982e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 6.053e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.982e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 3.495e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.982e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 5.268e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.982e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 4.470e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.782e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 4.470e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 16 iterations, alpha=1.606e-05, previous alpha=1.556e-05, with an active set of 9 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=3.482e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=3.482e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.788e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=3.482e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.825e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=3.482e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 3.495e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=3.482e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.581e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=3.482e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=3.482e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=6.882e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.788e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=6.882e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 3.650e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=6.882e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=6.882e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=6.882e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 3.161e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=6.882e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.581e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=2.068e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.581e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.010e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 6.989e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.010e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 5.867e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.010e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 3.495e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.010e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.010e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 4.215e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.010e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 3.942e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.010e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 10 iterations, alpha=7.791e-05, previous alpha=3.961e-05, with an active set of 9 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.534e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.980e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.534e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.534e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 5.268e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.534e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 5.771e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 10 iterations, alpha=1.328e-04, previous alpha=4.291e-05, with an active set of 9 regressors.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.753e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.753e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 8.429e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.753e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 8.025e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.753e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 3.332e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.753e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.356e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.753e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 7.671e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.753e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 3.942e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=6.315e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 8.429e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=6.315e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 7.955e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=6.315e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=6.315e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 7.671e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=6.315e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 3.942e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=6.315e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 8.162e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=8.589e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 4.712e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=8.589e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 5.576e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=8.589e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 6.053e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=8.589e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 7.146e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 10 iterations, alpha=6.596e-05, previous alpha=3.850e-05, with an active set of 9 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.089e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.089e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.581e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.089e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 5.576e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 10 iterations, alpha=1.025e-04, previous alpha=3.969e-05, with an active set of 9 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.441e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 9.541e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.441e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 8.093e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.441e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 3.161e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.441e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.441e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 4.712e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 10 iterations, alpha=1.339e-04, previous alpha=3.254e-05, with an active set of 9 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.113e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 7 iterations, alpha=2.081e-04, previous alpha=2.034e-04, with an active set of 4 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=8.364e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=4.182e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=7.136e-06, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=6.371e-06, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=1.968e-07, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=8.228e-04, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=5.783e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=4.114e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.669e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 8 iterations, alpha=4.022e-04, previous alpha=1.275e-04, with an active set of 7 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.871e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.436e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=2.472e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 10 iterations, alpha=1.175e-04, previous alpha=1.836e-05, with an active set of 9 regressors.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.679e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.679e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.980e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.679e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.356e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.679e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.679e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 3.332e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.376e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 3.332e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=4.749e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=4.749e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.980e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=4.749e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=4.749e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 4.942e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=4.749e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 3.332e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=4.749e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=4.749e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.356e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=2.176e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 4.593e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=2.176e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=2.176e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.581e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=2.176e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=2.176e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 3.161e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 13 iterations, alpha=2.334e-05, previous alpha=9.660e-06, with an active set of 10 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=1.283e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.581e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=1.283e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 4.344e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=1.283e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 3.799e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=1.283e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=1.283e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 3.650e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=1.283e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.788e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=7.577e-06, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.788e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=2.993e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 4.344e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=2.993e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.581e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=2.993e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=2.993e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=2.993e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 3.799e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=2.993e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.788e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=2.993e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 3.650e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.911e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 5.674e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.911e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 5.960e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.911e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.911e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.911e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.145e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.145e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 3.332e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.145e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.145e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.145e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 5.674e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 9 iterations, alpha=2.678e-04, previous alpha=7.625e-05, with an active set of 8 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.887e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 3.332e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.887e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.581e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.887e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 5.576e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.887e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.887e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.212e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.356e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.212e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.980e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.212e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 5.162e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.212e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 5.771e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.212e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.212e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 3.332e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 10 iterations, alpha=2.555e-04, previous alpha=7.984e-05, with an active set of 7 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=4.866e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=2.433e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=5.557e-06, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 13 iterations, alpha=2.427e-05, previous alpha=2.452e-06, with an active set of 8 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=8.022e-06, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=4.780e-06, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.581e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=4.780e-06, with an active set of 12 regressors, and the smallest cholesky pivot element being 3.650e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=4.780e-06, with an active set of 12 regressors, and the smallest cholesky pivot element being 3.161e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=4.780e-06, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=4.780e-06, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.356e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=2.893e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 3.161e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=2.893e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.581e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=2.893e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 8.093e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=2.893e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 6.495e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=2.893e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=2.893e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 3.650e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 11 iterations, alpha=2.749e-05, previous alpha=5.780e-06, with an active set of 10 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.603e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.788e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.603e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.581e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.603e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 4.829e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.603e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 6.495e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.603e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 6.664e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=6.794e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.788e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=6.794e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.581e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=6.794e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 4.712e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=6.794e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 6.580e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=6.794e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 11 iterations, alpha=8.731e-05, previous alpha=4.690e-05, with an active set of 10 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=4.075e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 7.451e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=4.075e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 9.424e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=4.075e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.980e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=4.075e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 7.743e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=4.075e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 9.246e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=4.075e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 8.025e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 11 iterations, alpha=3.319e-05, previous alpha=3.157e-05, with an active set of 10 regressors.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acetyl-CoA (uM)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAFlCAYAAADComBzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n0/xBIAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x107cfc320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=7.836e-05, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=7.836e-05, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=7.836e-05, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.825e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=3.918e-05, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=3.918e-05, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=3.189e-05, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 10 iterations, alpha=1.744e-05, previous alpha=9.971e-06, with an active set of 7 regressors.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HMG-CoA (uM)\n",
      "Warning: xgboost.XGBRegressor is not available and will not be used by TPOT.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:   1%|▏         | 242/17030 [00:36<31:29,  8.89pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:   2%|▏         | 365/17030 [00:58<34:19,  8.09pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 2 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:   3%|▎         | 481/17030 [01:15<47:37,  5.79pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 3 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:   4%|▎         | 600/17030 [01:35<33:32,  8.16pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 4 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:   4%|▍         | 725/17030 [02:02<36:58,  7.35pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 5 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:   5%|▍         | 845/17030 [02:15<25:44, 10.48pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 6 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:   6%|▌         | 963/17030 [02:21<11:12, 23.90pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 7 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:   6%|▋         | 1082/17030 [02:24<08:46, 30.29pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 8 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:   7%|▋         | 1203/17030 [02:28<14:27, 18.25pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 9 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:   8%|▊         | 1326/17030 [02:33<16:26, 15.92pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 10 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:   8%|▊         | 1444/17030 [02:37<12:52, 20.17pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 11 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:   9%|▉         | 1562/17030 [02:42<12:22, 20.84pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 12 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  10%|▉         | 1673/17030 [02:46<17:31, 14.60pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 13 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  11%|█         | 1799/17030 [02:51<17:09, 14.79pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 14 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  11%|█▏        | 1923/17030 [02:55<11:42, 21.50pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 15 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  12%|█▏        | 2046/17030 [02:58<12:33, 19.88pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 16 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  13%|█▎        | 2158/17030 [03:01<08:49, 28.06pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 17 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  13%|█▎        | 2280/17030 [03:04<08:54, 27.57pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 18 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  14%|█▍        | 2400/17030 [03:09<08:24, 29.00pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 19 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  15%|█▍        | 2522/17030 [03:13<16:00, 15.11pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 20 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  16%|█▌        | 2643/17030 [03:17<14:32, 16.49pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 21 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  16%|█▌        | 2764/17030 [03:22<16:47, 14.16pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 22 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  17%|█▋        | 2885/17030 [03:25<10:34, 22.30pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 23 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  18%|█▊        | 3004/17030 [03:29<12:13, 19.12pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 24 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  18%|█▊        | 3124/17030 [03:32<12:38, 18.34pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 25 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  19%|█▉        | 3245/17030 [03:36<12:55, 17.76pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 26 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  20%|█▉        | 3367/17030 [03:40<13:53, 16.39pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 27 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  20%|██        | 3482/17030 [03:43<13:52, 16.28pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 28 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  21%|██        | 3595/17030 [03:44<09:25, 23.78pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 29 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  22%|██▏       | 3715/17030 [03:48<10:19, 21.49pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 30 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  22%|██▏       | 3829/17030 [03:51<11:29, 19.14pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 31 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  23%|██▎       | 3953/17030 [03:54<13:08, 16.59pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 32 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  24%|██▍       | 4069/17030 [03:58<12:37, 17.12pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 33 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  25%|██▍       | 4188/17030 [04:00<10:51, 19.72pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 34 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  25%|██▌       | 4301/17030 [04:04<15:21, 13.81pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 35 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  26%|██▌       | 4419/17030 [04:07<08:59, 23.38pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 36 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  27%|██▋       | 4538/17030 [04:10<10:32, 19.75pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 37 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  27%|██▋       | 4664/17030 [04:13<10:13, 20.15pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 38 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  28%|██▊       | 4783/17030 [04:15<08:33, 23.84pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 39 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  29%|██▉       | 4901/17030 [04:19<11:13, 18.01pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 40 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  29%|██▉       | 5022/17030 [04:22<11:50, 16.90pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 41 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  30%|███       | 5140/17030 [04:25<08:32, 23.19pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 42 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  31%|███       | 5255/17030 [04:28<09:24, 20.88pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 43 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  32%|███▏      | 5374/17030 [04:31<10:09, 19.11pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 44 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  32%|███▏      | 5497/17030 [04:35<09:01, 21.29pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 45 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  33%|███▎      | 5616/17030 [04:37<08:38, 22.02pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 46 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  34%|███▎      | 5734/17030 [04:40<08:28, 22.21pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 47 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  34%|███▍      | 5853/17030 [04:43<07:36, 24.46pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 48 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  35%|███▌      | 5978/17030 [04:47<14:09, 13.01pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 49 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  36%|███▌      | 6101/17030 [04:49<09:19, 19.54pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 50 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  37%|███▋      | 6222/17030 [04:52<09:50, 18.32pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 51 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  37%|███▋      | 6344/17030 [04:55<10:05, 17.66pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 52 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  38%|███▊      | 6465/17030 [04:59<11:12, 15.71pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 53 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  39%|███▊      | 6581/17030 [05:03<11:45, 14.81pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 54 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  39%|███▉      | 6702/17030 [05:06<12:15, 14.04pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 55 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  40%|████      | 6823/17030 [05:10<11:36, 14.66pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 56 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  41%|████      | 6938/17030 [05:13<10:20, 16.26pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 57 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  41%|████▏     | 7055/17030 [05:16<07:36, 21.87pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 58 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  42%|████▏     | 7174/17030 [05:19<12:00, 13.67pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 59 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  43%|████▎     | 7291/17030 [05:23<07:43, 21.00pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 60 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  44%|████▎     | 7412/17030 [05:26<10:31, 15.23pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 61 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  44%|████▍     | 7538/17030 [05:29<09:25, 16.79pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 62 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  45%|████▍     | 7654/17030 [05:33<10:26, 14.97pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 63 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  46%|████▌     | 7774/17030 [05:35<08:17, 18.59pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 64 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  46%|████▋     | 7894/17030 [05:37<09:30, 16.01pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 65 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  47%|████▋     | 8013/17030 [05:41<12:23, 12.13pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 66 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  48%|████▊     | 8133/17030 [05:45<08:09, 18.17pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 67 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  48%|████▊     | 8253/17030 [05:49<13:51, 10.56pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 68 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  49%|████▉     | 8375/17030 [05:52<13:55, 10.35pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 69 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  50%|████▉     | 8497/17030 [05:54<09:21, 15.20pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 70 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  51%|█████     | 8619/17030 [05:57<06:39, 21.04pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 71 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  51%|█████▏    | 8737/17030 [06:01<06:38, 20.79pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 72 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  52%|█████▏    | 8862/17030 [06:06<09:19, 14.61pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 73 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  53%|█████▎    | 8981/17030 [06:09<08:49, 15.20pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 74 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  53%|█████▎    | 9106/17030 [06:14<09:19, 14.17pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 75 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  54%|█████▍    | 9228/17030 [06:15<04:52, 26.70pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 76 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  55%|█████▍    | 9348/17030 [06:18<04:26, 28.85pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 77 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  56%|█████▌    | 9470/17030 [06:22<05:44, 21.96pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 78 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  56%|█████▋    | 9594/17030 [06:27<11:35, 10.70pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 79 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  57%|█████▋    | 9711/17030 [06:29<05:53, 20.71pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 80 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  58%|█████▊    | 9835/17030 [06:32<08:59, 13.34pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 81 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  58%|█████▊    | 9957/17030 [06:34<05:13, 22.54pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 82 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  59%|█████▉    | 10076/17030 [06:37<06:09, 18.82pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 83 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  60%|█████▉    | 10199/17030 [06:39<05:13, 21.78pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 84 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  61%|██████    | 10319/17030 [06:44<06:25, 17.40pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 85 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  61%|██████▏   | 10435/17030 [06:46<07:08, 15.40pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 86 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  62%|██████▏   | 10560/17030 [06:50<06:59, 15.43pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 87 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  63%|██████▎   | 10682/17030 [06:54<09:13, 11.46pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 88 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  63%|██████▎   | 10801/17030 [06:56<06:16, 16.55pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 89 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  64%|██████▍   | 10925/17030 [07:01<08:17, 12.28pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 90 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  65%|██████▍   | 11046/17030 [07:05<12:33,  7.94pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 91 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  66%|██████▌   | 11168/17030 [07:09<10:33,  9.25pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 92 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  66%|██████▋   | 11289/17030 [07:11<07:22, 12.98pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 93 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  67%|██████▋   | 11408/17030 [07:16<06:38, 14.11pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 94 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  68%|██████▊   | 11534/17030 [07:19<05:14, 17.48pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 95 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  68%|██████▊   | 11652/17030 [07:21<04:47, 18.72pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 96 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  69%|██████▉   | 11775/17030 [07:25<04:14, 20.63pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 97 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  70%|██████▉   | 11894/17030 [07:28<05:56, 14.41pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 98 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  71%|███████   | 12012/17030 [07:32<05:41, 14.68pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 99 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  71%|███████▏  | 12136/17030 [07:35<03:10, 25.70pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 100 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  72%|███████▏  | 12259/17030 [07:41<08:26,  9.43pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 101 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  73%|███████▎  | 12378/17030 [07:46<08:15,  9.39pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 102 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  73%|███████▎  | 12494/17030 [07:49<05:18, 14.23pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 103 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  74%|███████▍  | 12617/17030 [07:52<03:30, 20.98pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 104 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  75%|███████▍  | 12738/17030 [07:55<04:40, 15.32pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 105 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  76%|███████▌  | 12858/17030 [07:57<03:13, 21.56pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 106 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  76%|███████▌  | 12983/17030 [08:02<06:56,  9.71pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 107 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  77%|███████▋  | 13102/17030 [08:04<05:01, 13.03pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 108 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  78%|███████▊  | 13228/17030 [08:08<03:01, 20.92pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 109 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  78%|███████▊  | 13350/17030 [08:11<03:48, 16.12pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 110 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  79%|███████▉  | 13470/17030 [08:13<03:15, 18.21pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 111 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  80%|███████▉  | 13592/17030 [08:15<02:47, 20.54pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 112 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  81%|████████  | 13712/17030 [08:18<04:42, 11.73pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 113 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  81%|████████  | 13831/17030 [08:22<03:51, 13.82pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 114 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  82%|████████▏ | 13952/17030 [08:25<04:19, 11.84pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 115 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  83%|████████▎ | 14074/17030 [08:28<03:02, 16.22pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 116 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  83%|████████▎ | 14199/17030 [08:32<03:24, 13.82pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 117 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  84%|████████▍ | 14321/17030 [08:36<03:07, 14.44pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 118 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  85%|████████▍ | 14443/17030 [08:40<03:40, 11.72pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 119 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  86%|████████▌ | 14566/17030 [08:43<04:50,  8.49pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 120 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  86%|████████▌ | 14688/17030 [08:47<03:09, 12.33pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 121 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  87%|████████▋ | 14811/17030 [08:52<03:17, 11.22pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 122 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  88%|████████▊ | 14934/17030 [08:56<03:29, 10.00pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 123 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  88%|████████▊ | 15056/17030 [08:59<02:05, 15.71pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 124 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  89%|████████▉ | 15181/17030 [09:00<02:11, 14.09pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 125 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  90%|████████▉ | 15303/17030 [09:04<01:38, 17.58pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 126 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  91%|█████████ | 15427/17030 [09:07<01:39, 16.17pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 127 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  91%|█████████▏| 15547/17030 [09:09<01:19, 18.63pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 128 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  92%|█████████▏| 15670/17030 [09:12<01:01, 22.23pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 129 - Current best internal CV score: 0.00011380667959142894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 130 - Current best internal CV score: 0.00011380667959142894\n",
      "\n",
      "Best pipeline: LinearSVR(input_matrix, LinearSVR__C=0.1, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=DEFAULT, LinearSVR__tol=0.01)\n",
      "HMG-CoA (uM)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAFlCAYAAADComBzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x113c740b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IPP/DMAPP (uM)\n",
      "Warning: xgboost.XGBRegressor is not available and will not be used by TPOT.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:   1%|▏         | 242/17030 [00:31<31:23,  8.92pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1 - Current best internal CV score: 0.11476788945449712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:   2%|▏         | 365/17030 [01:00<31:49,  8.73pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 2 - Current best internal CV score: 0.11465897237976246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:   3%|▎         | 488/17030 [01:34<47:05,  5.86pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 3 - Current best internal CV score: 0.11465897237976246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:   4%|▎         | 604/17030 [02:00<47:55,  5.71pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 4 - Current best internal CV score: 0.11465897237976246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:   4%|▍         | 722/17030 [02:36<46:29,  5.85pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 5 - Current best internal CV score: 0.11465897237976246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:   5%|▍         | 845/17030 [03:08<35:49,  7.53pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 6 - Current best internal CV score: 0.11465897237976246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:   6%|▌         | 964/17030 [03:49<50:24,  5.31pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 7 - Current best internal CV score: 0.11465897237976246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:   6%|▋         | 1083/17030 [04:19<39:17,  6.76pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 8 - Current best internal CV score: 0.11465897237976246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:   7%|▋         | 1197/17030 [04:41<24:47, 10.64pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 9 - Current best internal CV score: 0.11465897237976246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:   8%|▊         | 1313/17030 [05:09<39:04,  6.71pipeline/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 10 - Current best internal CV score: 0.11465897237976246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:   8%|▊         | 1434/17030 [05:46<51:51,  5.01pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 11 - Current best internal CV score: 0.11465897237976246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:   9%|▉         | 1555/17030 [06:30<1:04:12,  4.02pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 12 - Current best internal CV score: 0.11465897237976246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  10%|▉         | 1680/17030 [07:31<1:00:04,  4.26pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 13 - Current best internal CV score: 0.11465897237976246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  11%|█         | 1797/17030 [08:18<1:07:45,  3.75pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 14 - Current best internal CV score: 0.11149600885226371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  11%|█         | 1915/17030 [09:25<50:56,  4.95pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 15 - Current best internal CV score: 0.11149600885226371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  12%|█▏        | 2030/17030 [10:05<1:15:25,  3.31pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 16 - Current best internal CV score: 0.11149600885226371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  13%|█▎        | 2154/17030 [10:46<48:24,  5.12pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 17 - Current best internal CV score: 0.11149600885226371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  13%|█▎        | 2274/17030 [11:29<1:02:03,  3.96pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 18 - Current best internal CV score: 0.11149600885226371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  14%|█▍        | 2392/17030 [12:13<51:48,  4.71pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 19 - Current best internal CV score: 0.10739380693523201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  15%|█▍        | 2506/17030 [12:56<55:04,  4.39pipeline/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 20 - Current best internal CV score: 0.10739380693523201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  15%|█▌        | 2627/17030 [13:46<1:00:49,  3.95pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 21 - Current best internal CV score: 0.10739380693523201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  16%|█▌        | 2743/17030 [14:39<58:31,  4.07pipeline/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 22 - Current best internal CV score: 0.10739380693523201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  17%|█▋        | 2859/17030 [15:17<56:14,  4.20pipeline/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 23 - Current best internal CV score: 0.10739380693523201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  17%|█▋        | 2971/17030 [16:10<1:32:57,  2.52pipeline/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 24 - Current best internal CV score: 0.10739380693523201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  18%|█▊        | 3086/17030 [17:00<52:56,  4.39pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 25 - Current best internal CV score: 0.10739380693523201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  19%|█▉        | 3203/17030 [17:44<1:14:09,  3.11pipeline/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 26 - Current best internal CV score: 0.10739380693523201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  20%|█▉        | 3323/17030 [18:25<54:38,  4.18pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 27 - Current best internal CV score: 0.10265969350506825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  20%|██        | 3444/17030 [19:02<54:58,  4.12pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 28 - Current best internal CV score: 0.10265969350506825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  21%|██        | 3554/17030 [19:35<1:08:51,  3.26pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 29 - Current best internal CV score: 0.10265969350506825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  22%|██▏       | 3670/17030 [20:07<43:34,  5.11pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 30 - Current best internal CV score: 0.10265969350506825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  22%|██▏       | 3780/17030 [20:33<35:05,  6.29pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 31 - Current best internal CV score: 0.10265969350506825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  23%|██▎       | 3884/17030 [20:54<35:41,  6.14pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 32 - Current best internal CV score: 0.10265969350506825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  23%|██▎       | 3986/17030 [21:17<42:43,  5.09pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 33 - Current best internal CV score: 0.10265969350506825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  24%|██▍       | 4095/17030 [21:36<38:39,  5.58pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 34 - Current best internal CV score: 0.10265969350506825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  25%|██▍       | 4195/17030 [21:53<34:41,  6.17pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 35 - Current best internal CV score: 0.10265969350506825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  25%|██▌       | 4293/17030 [22:08<32:37,  6.51pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 36 - Current best internal CV score: 0.10265969350506825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  26%|██▌       | 4392/17030 [22:23<26:25,  7.97pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 37 - Current best internal CV score: 0.10265969350506825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  26%|██▋       | 4489/17030 [22:34<28:22,  7.37pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 38 - Current best internal CV score: 0.10265969350506825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  27%|██▋       | 4587/17030 [22:46<24:40,  8.41pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 39 - Current best internal CV score: 0.10265969350506739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 32 iterations, alpha=7.694e-05, previous alpha=4.798e-05, with an active set of 7 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 31 iterations, alpha=2.392e-03, previous alpha=1.476e-06, with an active set of 10 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 34 iterations, alpha=6.684e-04, previous alpha=1.281e-07, with an active set of 9 regressors.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best pipeline: LassoLarsCV(PolynomialFeatures(LassoLarsCV(MaxAbsScaler(input_matrix), LassoLarsCV__normalize=True), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=DEFAULT), LassoLarsCV__normalize=DEFAULT)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 32 iterations, alpha=2.474e-02, previous alpha=3.128e-05, with an active set of 11 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 37 iterations, alpha=6.897e-03, previous alpha=1.971e-05, with an active set of 16 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 32 iterations, alpha=2.966e-03, previous alpha=7.649e-05, with an active set of 11 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 31 iterations, alpha=1.242e-02, previous alpha=4.298e-05, with an active set of 14 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 32 iterations, alpha=2.189e-04, previous alpha=2.527e-05, with an active set of 13 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 34 iterations, alpha=1.428e-03, previous alpha=2.259e-05, with an active set of 13 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 35 iterations, alpha=2.328e-04, previous alpha=4.584e-05, with an active set of 14 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 31 iterations, alpha=1.857e-03, previous alpha=9.809e-05, with an active set of 10 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 33 iterations, alpha=1.891e-04, previous alpha=5.753e-05, with an active set of 10 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 33 iterations, alpha=8.457e-04, previous alpha=2.492e-04, with an active set of 10 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 33 iterations, alpha=1.881e-04, previous alpha=1.529e-04, with an active set of 10 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 32 iterations, alpha=5.747e-04, previous alpha=7.483e-05, with an active set of 11 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 31 iterations, alpha=4.506e-04, previous alpha=8.752e-06, with an active set of 14 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 32 iterations, alpha=6.867e-04, previous alpha=9.291e-05, with an active set of 11 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 36 iterations, alpha=4.726e-03, previous alpha=6.904e-05, with an active set of 11 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 41 iterations, alpha=1.330e-02, previous alpha=9.739e-07, with an active set of 10 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 36 iterations, alpha=3.717e-03, previous alpha=8.423e-06, with an active set of 9 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 31 iterations, alpha=1.724e-02, previous alpha=4.950e-05, with an active set of 12 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 29 iterations, alpha=2.501e-03, previous alpha=4.178e-05, with an active set of 12 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 30 iterations, alpha=5.840e-04, previous alpha=5.206e-05, with an active set of 9 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 32 iterations, alpha=2.362e-04, previous alpha=5.401e-05, with an active set of 13 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 32 iterations, alpha=1.004e-02, previous alpha=4.527e-05, with an active set of 11 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 31 iterations, alpha=4.267e-04, previous alpha=5.504e-05, with an active set of 12 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 34 iterations, alpha=1.333e-04, previous alpha=6.421e-05, with an active set of 11 regressors.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 32 iterations, alpha=5.237e-04, previous alpha=6.975e-05, with an active set of 11 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 33 iterations, alpha=2.873e-03, previous alpha=5.558e-05, with an active set of 12 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 32 iterations, alpha=3.327e-04, previous alpha=3.599e-05, with an active set of 11 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 34 iterations, alpha=3.471e-03, previous alpha=4.073e-05, with an active set of 11 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 35 iterations, alpha=6.395e-05, previous alpha=5.968e-05, with an active set of 12 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 31 iterations, alpha=2.056e-02, previous alpha=2.004e-06, with an active set of 10 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 36 iterations, alpha=2.879e-04, previous alpha=7.704e-05, with an active set of 7 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 32 iterations, alpha=7.738e-04, previous alpha=2.729e-05, with an active set of 11 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 31 iterations, alpha=1.177e-03, previous alpha=6.700e-05, with an active set of 10 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 31 iterations, alpha=1.071e-04, previous alpha=7.050e-06, with an active set of 12 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 32 iterations, alpha=9.174e-03, previous alpha=2.484e-05, with an active set of 13 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 30 iterations, alpha=1.366e-03, previous alpha=3.340e-05, with an active set of 13 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 31 iterations, alpha=1.399e-03, previous alpha=2.818e-05, with an active set of 10 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 29 iterations, alpha=2.793e-04, previous alpha=3.109e-05, with an active set of 12 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 36 iterations, alpha=1.244e-02, previous alpha=3.244e-06, with an active set of 17 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 34 iterations, alpha=2.980e-03, previous alpha=3.873e-05, with an active set of 11 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 32 iterations, alpha=1.869e-03, previous alpha=3.901e-05, with an active set of 11 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 32 iterations, alpha=7.073e-05, previous alpha=4.720e-05, with an active set of 11 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 34 iterations, alpha=4.104e-05, previous alpha=3.583e-05, with an active set of 13 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 32 iterations, alpha=7.346e-04, previous alpha=8.239e-05, with an active set of 9 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 34 iterations, alpha=2.668e-03, previous alpha=1.801e-05, with an active set of 15 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 32 iterations, alpha=3.049e-04, previous alpha=6.468e-05, with an active set of 9 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 32 iterations, alpha=1.719e-05, previous alpha=1.495e-05, with an active set of 13 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 35 iterations, alpha=4.611e-04, previous alpha=1.578e-05, with an active set of 12 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 30 iterations, alpha=3.467e-03, previous alpha=4.291e-05, with an active set of 11 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 34 iterations, alpha=2.250e-03, previous alpha=8.577e-06, with an active set of 15 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 32 iterations, alpha=7.968e-05, previous alpha=7.584e-05, with an active set of 13 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 31 iterations, alpha=1.958e-04, previous alpha=5.899e-05, with an active set of 10 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 34 iterations, alpha=2.422e-02, previous alpha=9.893e-06, with an active set of 15 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 34 iterations, alpha=1.161e-03, previous alpha=9.000e-05, with an active set of 11 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 31 iterations, alpha=1.042e-04, previous alpha=2.866e-05, with an active set of 12 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 32 iterations, alpha=2.946e-04, previous alpha=9.597e-05, with an active set of 11 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 32 iterations, alpha=1.917e-04, previous alpha=1.071e-05, with an active set of 15 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 32 iterations, alpha=1.738e-04, previous alpha=5.043e-05, with an active set of 11 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 33 iterations, alpha=1.987e-04, previous alpha=7.739e-05, with an active set of 10 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 31 iterations, alpha=1.474e-03, previous alpha=4.396e-05, with an active set of 8 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=9.239e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=7.652e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.825e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=7.652e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=7.627e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=3.882e-03, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=3.770e-03, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=3.770e-03, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 23 iterations, alpha=3.026e-03, previous alpha=3.019e-03, with an active set of 8 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=9.578e-03, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=5.313e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=2.657e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.581e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=2.657e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=2.510e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 13 iterations, alpha=2.124e-03, previous alpha=2.062e-03, with an active set of 8 regressors.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=5.470e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.709e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 8 iterations, alpha=1.692e-03, previous alpha=1.580e-03, with an active set of 5 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 33 iterations, alpha=2.416e-03, previous alpha=9.006e-05, with an active set of 12 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 31 iterations, alpha=1.651e-03, previous alpha=2.835e-05, with an active set of 14 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 35 iterations, alpha=1.821e-03, previous alpha=2.566e-05, with an active set of 14 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 31 iterations, alpha=8.343e-04, previous alpha=5.880e-05, with an active set of 12 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 33 iterations, alpha=1.265e-03, previous alpha=5.706e-05, with an active set of 12 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 35 iterations, alpha=7.313e-05, previous alpha=5.824e-05, with an active set of 12 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 31 iterations, alpha=8.393e-04, previous alpha=1.144e-04, with an active set of 10 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 37 iterations, alpha=4.119e-03, previous alpha=3.709e-05, with an active set of 14 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 33 iterations, alpha=3.937e-04, previous alpha=5.005e-05, with an active set of 10 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 32 iterations, alpha=1.601e-03, previous alpha=5.488e-05, with an active set of 11 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 33 iterations, alpha=7.199e-04, previous alpha=6.820e-05, with an active set of 10 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 35 iterations, alpha=3.399e-03, previous alpha=4.900e-05, with an active set of 6 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 47 iterations, alpha=4.329e-02, previous alpha=3.778e-06, with an active set of 10 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 32 iterations, alpha=1.612e-03, previous alpha=9.307e-07, with an active set of 7 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 31 iterations, alpha=2.433e-03, previous alpha=1.264e-04, with an active set of 10 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 35 iterations, alpha=1.123e-03, previous alpha=8.904e-05, with an active set of 12 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 30 iterations, alpha=5.284e-04, previous alpha=3.887e-04, with an active set of 7 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.941e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.941e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.356e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 22 iterations, alpha=1.481e-04, previous alpha=9.716e-05, with an active set of 7 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=6.582e-03, with an active set of 1 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.291e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.291e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 8 iterations, alpha=3.160e-03, previous alpha=3.160e-03, with an active set of 7 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=3.098e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=3.091e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=3.091e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.542e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.527e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=1.454e-04, with an active set of 9 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 16 iterations, alpha=1.538e-04, previous alpha=1.260e-04, with an active set of 9 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.484e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.484e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.788e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.242e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.788e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.242e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.233e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.825e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=6.163e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.788e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=6.163e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=5.832e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 16 iterations, alpha=5.054e-04, previous alpha=4.683e-04, with an active set of 9 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 31 iterations, alpha=1.206e-02, previous alpha=1.107e-05, with an active set of 16 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 31 iterations, alpha=1.532e-02, previous alpha=2.159e-05, with an active set of 16 regressors.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 34 iterations, alpha=2.239e-04, previous alpha=5.796e-05, with an active set of 11 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 36 iterations, alpha=6.107e-04, previous alpha=4.681e-05, with an active set of 13 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 32 iterations, alpha=1.624e-04, previous alpha=1.081e-04, with an active set of 11 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 35 iterations, alpha=9.526e-04, previous alpha=5.699e-05, with an active set of 12 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 47 iterations, alpha=7.926e-03, previous alpha=5.778e-07, with an active set of 10 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 35 iterations, alpha=1.076e-03, previous alpha=1.688e-05, with an active set of 14 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 30 iterations, alpha=3.085e-03, previous alpha=2.514e-05, with an active set of 11 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 30 iterations, alpha=1.702e-04, previous alpha=2.888e-05, with an active set of 11 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 35 iterations, alpha=3.438e-03, previous alpha=5.209e-05, with an active set of 12 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 31 iterations, alpha=1.114e-02, previous alpha=3.113e-05, with an active set of 12 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 29 iterations, alpha=1.864e-03, previous alpha=5.320e-05, with an active set of 12 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 32 iterations, alpha=2.054e-03, previous alpha=1.294e-04, with an active set of 11 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 32 iterations, alpha=1.011e-04, previous alpha=7.967e-05, with an active set of 11 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 35 iterations, alpha=2.229e-03, previous alpha=6.407e-05, with an active set of 14 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 32 iterations, alpha=5.373e-04, previous alpha=5.669e-05, with an active set of 11 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 34 iterations, alpha=1.050e-03, previous alpha=5.299e-05, with an active set of 13 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 32 iterations, alpha=2.263e-04, previous alpha=9.447e-05, with an active set of 11 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 50 iterations, alpha=2.801e-03, previous alpha=1.519e-07, with an active set of 9 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=6.768e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.825e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=6.754e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.825e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=4.817e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 13 iterations, alpha=4.817e-03, previous alpha=4.248e-03, with an active set of 6 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=1.300e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.825e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=1.300e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 18 iterations, alpha=9.455e-05, previous alpha=5.830e-05, with an active set of 9 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.638e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 39 iterations, alpha=4.576e-03, previous alpha=2.992e-03, with an active set of 16 regressors.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 32 iterations, alpha=3.770e-03, previous alpha=1.276e-05, with an active set of 15 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 31 iterations, alpha=1.826e-03, previous alpha=2.648e-05, with an active set of 12 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 35 iterations, alpha=9.081e-03, previous alpha=4.022e-06, with an active set of 16 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 32 iterations, alpha=5.029e-04, previous alpha=6.302e-05, with an active set of 11 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 33 iterations, alpha=4.147e-04, previous alpha=3.262e-05, with an active set of 12 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 36 iterations, alpha=1.472e-02, previous alpha=1.157e-05, with an active set of 13 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 33 iterations, alpha=9.834e-05, previous alpha=4.259e-05, with an active set of 12 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 34 iterations, alpha=3.632e-04, previous alpha=8.945e-05, with an active set of 11 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 34 iterations, alpha=7.968e-04, previous alpha=7.728e-05, with an active set of 11 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 41 iterations, alpha=6.051e-03, previous alpha=2.342e-07, with an active set of 10 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 30 iterations, alpha=8.225e-03, previous alpha=2.035e-04, with an active set of 9 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 33 iterations, alpha=1.187e-03, previous alpha=1.368e-05, with an active set of 14 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 35 iterations, alpha=1.409e-04, previous alpha=1.084e-05, with an active set of 14 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 31 iterations, alpha=1.241e-03, previous alpha=4.986e-05, with an active set of 12 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 29 iterations, alpha=1.570e-02, previous alpha=8.603e-05, with an active set of 10 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 30 iterations, alpha=1.557e-02, previous alpha=1.645e-04, with an active set of 9 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 32 iterations, alpha=1.239e-03, previous alpha=1.079e-04, with an active set of 11 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 32 iterations, alpha=8.083e-05, previous alpha=5.404e-05, with an active set of 11 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 30 iterations, alpha=2.245e-02, previous alpha=6.684e-05, with an active set of 11 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 33 iterations, alpha=7.978e-04, previous alpha=8.611e-05, with an active set of 10 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 32 iterations, alpha=1.825e-03, previous alpha=4.602e-05, with an active set of 9 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 29 iterations, alpha=2.172e-03, previous alpha=5.257e-05, with an active set of 10 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=3.636e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=1.693e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 20 iterations, alpha=1.242e-04, previous alpha=1.242e-04, with an active set of 7 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=6.044e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=4.164e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=4.164e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=3.326e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 5 iterations, alpha=3.384e-02, previous alpha=3.184e-02, with an active set of 4 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 30 iterations, alpha=8.258e-03, previous alpha=1.548e-05, with an active set of 13 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 30 iterations, alpha=2.371e-03, previous alpha=4.213e-05, with an active set of 11 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 32 iterations, alpha=2.676e-03, previous alpha=2.474e-04, with an active set of 11 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 34 iterations, alpha=3.801e-03, previous alpha=1.368e-04, with an active set of 11 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 31 iterations, alpha=2.390e-03, previous alpha=2.571e-05, with an active set of 12 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 34 iterations, alpha=2.445e-04, previous alpha=8.256e-05, with an active set of 11 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 33 iterations, alpha=3.365e-04, previous alpha=6.694e-05, with an active set of 12 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 33 iterations, alpha=4.701e-04, previous alpha=1.132e-04, with an active set of 10 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 30 iterations, alpha=9.112e-04, previous alpha=4.679e-05, with an active set of 11 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 39 iterations, alpha=7.516e-05, previous alpha=1.924e-05, with an active set of 16 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 33 iterations, alpha=9.219e-04, previous alpha=1.004e-04, with an active set of 10 regressors.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 34 iterations, alpha=1.809e-03, previous alpha=3.090e-05, with an active set of 13 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 36 iterations, alpha=3.688e-03, previous alpha=4.521e-05, with an active set of 11 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 32 iterations, alpha=1.958e-03, previous alpha=4.388e-05, with an active set of 13 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 33 iterations, alpha=3.170e-04, previous alpha=5.011e-05, with an active set of 10 regressors.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IPPDMAPP (uM)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAFlCAYAAADComBzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzs3Xl80/X9B/DX95uzTdqmaUs5Ww65FJRDRQU8UHSgDBAV\nPAAVr226zVvnbw4nQxTdVDaPOXWACiiKgIpjKAoUUQ5RYQpyFVquAi1tmrZJvt/P74+kadKmJ0m+\nOV7PPVib7zf95t0ifeVzfD8fSQghQERERHFD1roAIiIiah2GNxERUZxheBMREcUZhjcREVGcYXgT\nERHFGYY3ERFRnNFrXQBRMhkxYgReeOEF9O/fHyNGjIDBYIDZbIYkSXC73Rg6dCgeeeQRyLKM3r17\no1evXpBlGZIkwePxYMyYMbjzzjv91/vtb3+Lu+++G08++SSKi4uRlpYGAHC73TjnnHPw4IMPwmq1\noqioCJdeeinOPvtsvP3220E1Pfroo/jggw/w1VdfwW63+7/+kksuQe/evfH666/7n1tUVISRI0ei\nV69e/mNCCEyZMgXXXHMNvv76a9x+++3o1q0bJEmCEAI6nQ533303RowYEfJn8s4770Cn02HixImN\n/ty+/vprTJkyBWPHjsUzzzwTdG7y5MnYtm0bvv32Wxw+fBhPPvkk5syZA1lm24QSF8ObSEPPPvss\n+vfvDwBwuVyYPHky3nnnHdx0000AgLlz5/oD1eFwYOzYsejVqxcuueQSuFwuFBYW+oP0oYcewi9+\n8QsA3vCdMWMGHnjgAbzyyisAAJPJhH379qG4uBidOnUCADidTmzevLlBXf/973/Ru3dvbN++Hbt3\n70aPHj3858xmM5YuXep/fOTIEVx11VXo168fACAvLy/o/E8//YTrr78en332mf97qVVcXIwlS5bg\n3XffbfZnlZOTgy+++AJVVVVISUnxf/3evXv9z2nfvj369u0b9DMkSkR8a0oUI4xGIwYPHow9e/aE\nPG+1WtGvXz//+fXr1+P8888P+VyDwYBHH30UGzduxO7duwEAOp0Oo0aNwvLly/3PW7lyJS699NIG\nX79gwQJcdtllGD16NObOndtk3bm5ucjPz8e+fftCnu/Tpw/MZjOKi4sbnHv11VcxduxYSJKEoqIi\nDBw40H+u/mObzYbBgwdj1apV/mMffvghxowZE3TNa6+9Fq+++ipcLleTdRPFM4Y3UYw4cuQIVq9e\njSFDhoQ8v2fPHmzcuBHnnHMOAOCzzz7DZZdd1uj1zGYzunbtip07d/qPjRs3DsuWLfM//vDDDzF+\n/Pigr9u1axe2bt2KUaNGYdy4cVi6dClKS0sbfZ1vv/0W+/fvx1lnnRXy/MqVKyHLMk477bSg40II\nrFy5EhdffHGj166vtp5aK1aswFVXXRX0nNzcXLRr1w5btmxp8XWJ4g27zYk09MADD8BsNkNVVRgM\nBlx77bW44oor/OenTp0KWZahqipSUlLw0EMP4cwzz4Sqqti6dSumT5/e5PUlSfJ3MQNAv379IMsy\ntm3bhqysLFRWVgaNXwPeVvfFF18Mm80Gm82Gzp07Y9GiRbjrrrsAANXV1Rg7diwAQFEUZGZmYvbs\n2ejQoQP279+P/fv3+897PB60b98eL730UlAdAFBaWoqKigp07ty5xT+vSy65BNOnT8fx48exb98+\ndO/eHRkZGQ2el5eXh7179+K8885r8bWJ4gnDm0hDgWPeoQSOeQf67rvv0K9fP+h0uka/tqqqCrt3\n70bPnj0RuIXBL3/5Syxbtgx2u90fsrWcTic+/PBDmEwm/wQzh8OBt99+G9OmTQPQcMy7vvpj3o2R\nZRlCCKiq6p+UF1in2+1u8DVGoxGXX345PvroI+zatatBr0EtRVGa/NkQxTt2mxPFoVWrVoUcq65V\nXV2NmTNn4sILL/RPTqs1duxYfPrpp/jkk08adDkvX74cmZmZWLt2LT7//HN8/vnnWLVqFZxOJ1as\nWBHW78FmsyE9Pd0/Fp6eng63241du3YB8E6aC2XcuHFYsmQJNm7ciOHDh4d8TlFREbp37x7Weoli\nCcObKA6tX78ew4YNCzr2zDPPYOzYsRg/fjyuvfZapKam4umnn27wtbm5uejRowe6du0Km80WdG7B\nggW45ZZbglqt6enpmDx5crMT19ri8ssvx9q1awEAaWlpePDBB3H77bdjwoQJkCQp5NcMHDgQVVVV\nuOSSS6DXN+w8PHbsGI4fP45BgwaFvV6iWCFxS1Ai0sqBAwfwu9/9Du+//36jYd1ac+bMgd1ux403\n3hiW6xHFIra8iUgzXbp0wbhx47Bw4cKwXO/QoUPYvn07Jk2aFJbrEcUqtryJiIjiDFveREREcYbh\nTUREFGcY3kRERHEmbhZpKSmp0PT1j1cdg1ttuGgEEVEkGSQ9slJztC6DNJKTkxbyOFveLcV5fUSk\nATYaKBSGdwsJMLyJKPo8qgreFET1MbxbSBWq1iUQUTKSBDyqR+sqKMYwvFuI73yJSAt6WY8apUbr\nMijGMLxbiN3mRKQVjntTfQzvFlKZ3USkEZXd5lQPw7uF2PImIq1wzJvqY3i3EMe8iUgrblXRugSK\nMXGzSIuWhBCAxPAmoujKWP4xcv7xCsy7dsPTqzeqfv8AasZf0+brzZnzN+zY8SNOnDiO6upqdOzY\nCTZbJmbMaLjve30//7wD69atwS233B7y/IYN63HkyGGMHXt1m+ujloubXcW0XGFNURUcqjwIvcz3\nOkQUHRnLP0beb+9rcLz81TdOKcAB4JNPlqOwcB9+9at7Tuk6FHmNrbDGNGoBjncTUbi1n/k0Mj75\nT6Pn9UeOhDyedvedsMyYHvJczZhxqJw+o9W1bNmyCS+/PAcGgwG//OV4mEwmfPDBe/B4PJAkCTNn\nPos9e3Zh6dL38cQTT2HSpPHo3/8s7N9fCLvdjhkznsF//vMJCgv3Ydy4CZg+/TG0a5eL4uIinH76\nGXjggUdRVlaGJ554DG63G1265GPLlo1YtOjDutpravD444+gsrIS1dXVuOOOX+Pcc8/DRx99iCVL\n3oeqKhg27CJMm3YnVq5cgXffXQCDwYAuXfLw0EOPYeXKFfj442VQVRXTpt2J8vJyLFr0NmRZxpln\nDki4NyoM7xZQVRUSJK3LIKIkInkamaTmjsxtYy6XC6+9NhcAMG/eG5g9+wWYzWY888xf8M03XyE7\nu2599YMHi/HCCy8jN7c9fvWrW/Hjj/8LutaBA/vxt7/9HSaTGdddNxbHjx/D22/PxfDhF+Pqq6/F\nxo0bsHHjhqCvKS4uwsmTJ/Hccy+itLQUBw4UorT0BN56ay7mzl0Ao9GEV175Ow4fPoTXX38Vb775\nNlJTLXjxxeewdOn7SElJRVpaGmbN+ivKy0/i17++Df/613yYzWY8+eQfsXHjBpxzznkR+dlpgeHd\nAh7hgSxxbh8Rhc/hPzyMw394uNHzp/1iDFJ27GxwXDm9H0q/WB/2evLy8v2fZ2baMWPGn5CamorC\nwn3o1+/MoOdmZNiQm9seANCuXS5cruBFZDp16ozUVAsAICsrGy6XC/v27cOoUVcBAM48c2CD1+/e\nvQfGjr0a06c/Bo/Hg2uumYTi4mJ069YDJpMZAPCrX92DH3/cjm7duvuvf9ZZg7Bx4wacfno///dQ\nVHQAZWWleOCB3wIAnE4niouLcM45p/xjihlMpBZQhQpJYsubiKKn5Dd3hTzu/F3DcfBwkGXv7ziH\nw4HXX38VTzwxEw8//H8wmUwN7rZp7vdhqPPdu/fAtm0/AAC2b/+hwfndu3fB6azE7Nkv4LHHnsDz\nz89Gp06dsX//PrhcLgDA//3fQ8jMtGPfvr2oqqoCAGzdugVduuT5XtcbaR06dEK7drl4/vmX8Pe/\n/xPXXDMRZ5zRvzU/jpjHlncLxMmcPiJKICfHXIn9AHJeehXmXbtRfVoPVP/+AXhOcbJacywWC/r3\nPwt33XULdDo90tLScOxYCTp06HhK173pppvx5JOP4/PP/4vs7Bzo9cHx07lzF7z55j/x+eer/OPW\nmZmZuPHGqbj77jsgSRKGDh2O9u074NZb78Rvf3snJElG585dcNddd+Ozz1b6r5WZmYmJE2/E3Xff\nAUVR0KFDR4wYMfKU6o81nG3eAidrTqLK49Ts9YmIhBBIM6bDarRqXUqbfPXVOthsmejb9wxs3Pg1\n5s9/Ey+++IrWZcU8zjY/BXHy/oaIEpgkSXG90lqHDp3w1FN/hk6ng6qq+P3vH9C6pLjGlncLlFaX\nokap1uz1iYgAwCDpkZWa0/wTKWyEEFCFCkVVoEKFR/VAFSpUoUJAQAgVwvc4xWAJe88IW96nIE7e\n3xBRgovnlreWAgPYI+rCN1QACwjfOe9k5drVNSVIkCW5ycl60fz7YXi3gICqdQlERPCI5P1dpAoV\nQoiQAawKFfCFsBoUwr6vk7xrdbQkgAEAkgRZAmToovK9tQXDuyXY8iaimCDgUT1xu1Rz/QBWVCWg\npat6G0pChAzg1rSAvWtqBQZw7IZwW0X0v4DvvvsOzz77LObPnx90/PPPP8c//vEP6PV6TJgwAddd\nd10kyzhlXB6ViGKBLMmoUWo0De/WBrAKARGWAKZAEfsv4LXXXsOyZcuQkpISdNztduOpp57C4sWL\nkZKSguuvvx4jRoxAdnZ2pEo5ZWoSd1URkXZW7P0Y//rhFewp243uth64rf9dGH/aNYCh7dfcs2c3\nXnrpRVTXVKHK6cQ5Q87DDVOmQEjCPzbcXADXLhfdkgCWIEFqRQDfNmky/rVwPt58+TVcNWEsctq1\n858r3n8Ar855CX+e/VSjX79i6UcYNfYqfLtxM46VlGDk6F+07AcTZyIW3nl5eZgzZw4eeuihoOO7\nd+9GXl4eMjIyAACDBw/Gxo0bMWrUqEiVcsq8W4JqXQURJZMVez/Gw2vqVlP7uXQnHl5zH3SQMaXf\nrXXBqqr+FrAKNSiAhah7rELAUeHA4398BPf98WF06NQRQlHxt5mzsWTpYlxxVSO/g9sQwOFwy69C\nbz3anPcXLMKosVdh4DmDw1xRbIlYeF9xxRUoKipqcNzhcCAtrW7qu8VigcPhiFQZYeHtNmd6E1H4\nPLfpafx3X+O7ih11ht5V7JF1D+C5zc8EHZN8/z+y6xW4/+wQ66X7Anjz19+g/8Cz/MuJQgbueeg+\n6PV6bPvuB7z1+r+h1+sxcvQVsGVmYsHct2A0GmBNT8dv7vstPB4Ff5v5DFShwu1y447f/hqdunTG\nX//yNJyVlaipqcH1N0/GgMGD/C/t8Xjw+9t+jWdfeRFmsxlL3/sAsk7GWYMG4t+v/guqqqLiZDlu\nv+fX6HNGX//XPf7go7jznl8j1WLBC08/ByEEbPZM//mv1hbg02UfQ1E8ACQ89Kc/4L8ffwpHhQOv\nzXkJp/XuheIDRbhp2s1YtngJCr5cA51Oh779+mHybTdj0fx3cPTwEZwsK8OxoyW4+c7bMODsurpd\nLlfI7+uzT1fiPx+tgKqqOOe8czFxyo1Y8/kX+HjJUpgMJnTN7x6VXc6iPnBitVpRWVnpf1xZWRkU\n5rGI0U1E0eYRoW878qieNu9yeOL4CeS2bx90LHBo0+1yYdaL3qD8zdTb8ORfn0FWdhY+XrIMi99Z\nhH5nnQlrehruefBeFO0/gJrqahw5eAjlJ8vxf395AifLynCo+GDQ9fV6PYYMuwAb1q7HxSNHYN0X\nX+Lxp57E91u+w9Q7piG/W1es/fwLrF65Kii8a72/4F0MvfhCjBx9BQq+WIv/fPwJAOBgUTH+8OTj\nMJnNePWFv2Pr5m8x4YaJWLHsI9x+z6+xeuUqAEDh3n1Yv2Yd/vK32dDpdJj95FPYtOEbAIDBYMD/\n/eUJfLf5Wyx//8Og8A71fZ0sK8OSRYvx3CtzYDQa8fYbc1Fy5CjenfcOZr/0PDLTsjDvn69HZZez\nqId3jx49UFhYiLKyMqSmpmLTpk2YNm1atMtoFVUI6LgxCRGF0f1nPxy6lewzYdkY/FzacFexXpm9\nsfiXy9r0mjntcrBn1+6gY0cOH8bxkmMAgI6dOwEAyk+WIyU1FVnZWQCAvv3PwDtvzsPk227BoeKD\neHr6DO+E4+snokvXfFx+5S/w/KzZ8Hg8GD12DH7cth0L5r4FABh7zdW4bNTl+OeLL6FTl87o2Lkz\n0tLTYc+2Y/HbC2E0mVBVVYXU1OD5UbUOFhfjslFXAAD6nNHXH94ZtgzMefZ5pJjNKC4qQq++fUJ+\nffGBIvTq29u/lnrffqfjQOF+AEC3Ht0BANk52XC5XUFfF+r7OnLoMPK65sNkMgEAbpp2M3bt2InO\n+XlISU0FEL1dzqK2q9jy5cuxaNEiGAwGPPLII5g2bRomTZqECRMmIDc3N1pltAkXaSGiaLutf+hd\nxab1v7PN1xw85Fxs3bQFhw8eAuDt0p776uvYv68QACDJ3khIz0hHldOJ0uMnAAD/+34bOnbuhO3f\n/4BMux2PP/UkJlw/Ee+8OQ+Fe/ehylmFPzz5J9zzwL1446VX0bffGfjz7Kfw59lPYfCQc7zj6xBY\nuvgDXDbqcgDAGy//ExOn3Ih7HrwX+V3zG70jt0teHnb8+BMAYNfOnwF4e2zfnf8O7n30Qdx17z0w\nGk3+r69/nU5dOuPnn3ZCURQIIfDjD9v9b1LQRKMs1PeV26EDig8Uwe3y7qn+7JNPIcNmQ9H+/aiu\n9q7CGa1dziLa8u7cuTPeffddAMCYMWP8x0eMGIERI0ZE8qXDxjtZjeEdVkIAqgooivePx+N9DHj/\nMcmy96Ne7/2o09UdI0oSo7pdCQB4/YdX/bPNp/W/03+8LVItqbj7wXvxyvN/hypUVDurMPi8c3HF\nVaOx/ftt/udJkoS7fn8PZj85E5Ikw2K14O4Hfg9JkvC3mbPxn48/gaoouObGSejQqSPee2sB1q9Z\nByFUTJxyY8jXvvSKkVg07x30O8u7N/iFIy7BczNmwWK1IisnGxUny0N+3YQbrsMLTz+Hgi/X+Bt6\nqamp6H16Xzz2+wch63SwpllRevw4AKBzXhe88PRzOHPgWQCA/G5dccGFw/DYvQ9BCBV9zjgd515w\nHvbt2dvkzyrU95Vhy8C46ybg8QcfhSR53wzl5LbDxMk3YPqDf4BO1iE/r1tUdjnj2ubNUFQFhyoP\nxu2iCFGhqnVhXBvEqgoIFZJQ6x6rAt7F6mpvvZMAWWo8lFXhexvt+yj5AlxC3dfJMkTtcci+c3Ld\nn9rgrw1/vgEgoggx61JgM9vCek2ubd5GSbdAi/AFpcdT1zKuDV4EhLGieDO4NlwBb7jKTd3zKfkW\nOmrh7SayhEanCgrfa6sKJCiNfy9qQH3ei3oHi2pDvvYNAKSANwf1Wv86XfAxIiKNMbyboapqm2d2\nxoTGuqiFCNEy9j0fKrwh11RLVfLNmIjhn40kAbom6qsNdkUN/V0I4XuTELhIjxTc+vcFelDrP6BX\nwN/6D3wDQER0ihjezVCEAlmKoV+4keqiDgrjGPp+tVQb0o31FAgAivfn23zrv/YLfL0Jga3/wDcA\nqNfyr+32Dxz3Z+ufKOkxvJuhCKX5HWjaKpa6qCkyWtL6V73B3/LWv1z3Hiughe/v+g98oybLwRP/\n2PonSggM72a0aj5fq7uovdvYJUQXNUVGS1v/jXX9A8ET/7wXDTnxr671HzDxL7D1H1hTrcaOt+Rz\nImozhndTVBXC4wZqaiLURc1WMUVBCyf+obFn1bb+Ax8HXaC1pNCfN3K4QVGtfZMQ8LmofzGpBS8a\ndLiJNyK1j+sfD3yDcypvgpp6DUo6yRveQgDl5Q3DuLaLWvW1il3lkISLXdSUvPytf/8BrSrxOoU3\nEppUHlhjo7XXP9mcEBMn6981EdhzwkmTCSd5w1tRIFeUe8cDg/haxbI3iIWqAxS+yyWiNpIaacWH\n661Ec0MnoSZNMvjjXvKGdwvFyRo2REShNTVpksEftxjezVChNv8kIqJEFJbgrzdRsrG7JALXRuD6\nCM1ieDeHLW8iotaLRvDXXxuhfvAn8KqIDG8iIootDP5mMbybkXRrmxMRxbNwBX/gZkgtDX6THjBH\n6hsLxvBuhio45k1ElBROMfglUQWk5USwwDoM72aw5U1ERM2SfBPvooRT+JrBW8WIiCjWMLybobLl\nTUREMYbh3Qy2vImIKNYwvJvA4CYioljE8G6C4HQ1IoohGSs/w2k33owzLrgEp914MzJWfqZ1SYS6\nv5ceF1yAzIvOh2nJ4oi/JmebN0H177dNRKStjJWfocsfn/A/Nu/a43988vJLI/viQgT/ASAFfF63\nbazw3TIl6rabRd1xCN/XBTwOvL4UeDzwtVD/WOC1gp/rv1bQVraB9deroV79kqhXQ8DXSyFqsGzc\njNw35vp/VPoftyP9zltRDqBm/DWn/rNvhCTipG+4pKQivBf0eCAfPhRiV7E6LsWFozXHoJO41ScR\nRZbkdkNXWgb9iRPQ1348Uer/k/blWuiqqhp8nZBlKGlp/kCRQoRfUNgFBE9geNUPQik+oiFmeU7v\nh9Iv1p/ydXJy0kIeZ8u7CSpUSFrvXUxE8UkIyM4q6EtLoasN5OO+QC4tDfqoKy2FvryNDRRVhcee\nWbfblyT5NvuQ6pb3lHz/J0kQAc/zb0vqOyZqjwU9J/CaqLtuwNKhov4x/+sE7gVf9xwhBT8OqiHw\neGANCFF7wPX9rxdQp6hXZ2DtIvBxwOvXvU7D1w/6+fke57w5L+QbHd3On5r/uzsFDO8meFQFchRv\nuieiGKeq0J0s94XuCehPeFvIusCWcmlda1muqWnyckKSoGSkw5Odjeqep8Fjt8Njz4Riz4THnglP\nZt3H/PsfgXnP3gbXqD6tB3a//WakvmNqRvqatTDv2tPguNKrT0Rfl+HdBE5XI0p8ksvlax3X67IO\nCGFdbSiXnYSkKE1eTzUYoGTaUNOtKzyZNl8g2/zB7A1kOxS7DZ6MjCaH7gKV3DIlaMy71rGpN7Xl\n26YwKZk6OeTfi/N390X0dRneTWB4E8UhISBXOgPCt17LuF63ta7C0ewlldRUeOyZcHbq5A9iJbNe\n6zjLDk+mDarVGpHdqGonpWXPfQvmvftQ3a0rjk29KfKT1ahJ9f9elN594fzdfRGdrAZwwlqT73rL\nXOWoUhpOECGiKFOUgO7qulCubSXr6gWyXONq8nJCkqDYMoJawoFBHNRtnZkJYTZF6RuleGaGCRnd\n+ob1mpyw1gZseRNFjuRy1U3WOl5aN45cf8b18VLoTp6EpDa9w59qMMBjz0R19+5QGu2u9n5UbBne\nPZuJ4hTDuwlx0ilBFBEZKz9Dztz5MO0tRE23fJRMndx0F60QkCsrQ7eMa2daBwSzztGC7mqrFZ5M\nG2ryunjHj7PsdcGcGRDM9kyoFktEuquJYhHDuwkquJc3JafGFgRJ+f57uDt1Cj3TurQMsquZ7mpZ\nhpKRAXf7dqiy96kLYP/HTHiyMr3jyZk2CBO7q4lCYXg3hS1vSlI5c+eHPJ793pIGx1Sj0dtdfVp3\nb5d0YBd1QMvYk5kJJSOd3dVEYcDwJqI6QsDyzSaYQty3CnhbzgdmPhE0jqxaUtldTYlLFYCo7YUN\nWLxFlgBZBmQZQpIByBAmS9TKYng3gRPWKGkIAev6DWj3+lykbv9fo0+r7t4N5ZdcFMXCiMJECG8Q\n+3+vy96tuSQEh7AkewPadwx6vbe3SJYbrCzXgC4l8t+HD8O7CargmDclOFVF2toCtHtjHlJ+2gEA\nKL9oOJx9+6D9K681eDoXBKGYEKo1LKMucCXJ3xr2t5B1uro/tUEcxxjeTWDLmxKWqiJ99ZfIeWMe\nUnbthpAknLzsEhy9eQpqevYAALg7deSCIBRZjbWGA7ukITXdGpaTcwlrLtLSxCIth5yH4/7dGVEQ\nRUHGqs+R88Y8mPcVQsgyTl5+KUpunoyabl21ro7ilX8ns8Zbw0Fd0lJitobNuhTYzLawXpOLtLSB\nCgGZu4pRIvB4YPv0v8j593yYDhRB6HQovWo0SqbeCFdeF62ro1iiBmwbCiBkaziwS1qSGoZwkraG\no4nh3QQhBJjdFM8ktxu2j1YgZ+5bMB46DFWvx4lxY1Ay9Ua4O3bUujyKJH+XNOAN4mZaw7WhWxvA\nCdIaTlQM70bEyWgCUUhSTQ0yl32M7PnvwHjkKFSjEcevGY9jU26AOzdX6/KorYQAVBXNtobrd0nX\nhjMlDIZ3I4Tvf0TxRKquhn3JMmS/tQCGY8ehmkw4dv11OHbTJHiys7Uuj1qjdka1TgYMBghZDxiN\ngMnEIKbIhbeqqpg+fTp27NgBo9GIGTNmID8/339+2bJlePPNNyHLMiZMmIAbbrghUqW0CW8To3gi\nO52wv/8hst9eBH1pKZSUFJRMuQHHrp8IxZ6pdXnUHEUBIAMGGUKnB3R6b0gbjey2ppAiFt6rVq2C\ny+XCokWLsHXrVsyaNQsvv/yy//wzzzyDjz76CKmpqbjyyitx5ZVXIiMjI1LltJoqVEj8R0MxTnY4\nkPXeB8h6513oy8uhWCw4essUHL/+Wigx9O+JfGq7vSUZMOggdAbvHS8mE2AwaF0dxZGIhffmzZsx\nfPhwAMCAAQOwbdu2oPO9e/dGRUUF9Ho9hBAxF5QqVEicrUYxSneyHFmLFiPr3cXQVTjgSU/Dkdtv\nxfGJE6Cmhb61hKIsVLe3weANaq7vTqcoYuHtcDhgtVr9j3U6HTweD/S++6p79uyJCRMmICUlBSNH\njkR6enqkSmkTj6pAljiuRLFFV1qG7AXvwv7eB9A5nfDYMnD413fgxITxUK3RW1eZ6lF8w2x6GUJv\nYLc3RVzEwttqtaKystL/WFVVf3D/9NNP+OKLL/DZZ58hNTUVDz74IFasWIFRo0ZFqpxW42Q1iiX6\n48eR/dZC2D9YCrm6Gm67HUdvuxknrh4LkRK99ZSTXlPd3no9g5qiJmLhPWjQIKxevRqjR4/G1q1b\n0atXL/+5tLQ0mM1mmEwm6HQ62O12lJeXR6qUNmF4UyzQHy1B9vx3YF+6HHKNC+6cHBz+zZ0o/eVV\nEGbudR2A5X5bAAAgAElEQVRRqgBUxdvFbdBDyAbAyG5vig0RC++RI0eioKAAkyZNghACM2fOxPLl\ny+F0OjFx4kRMnDgRN9xwAwwGA/Ly8jB+/PhIldImKu/zJg0ZDh1G9ry3kbn8E8huN1wd2qNkyo0o\nu2oUhNGodXmJp7Fub4OBt2VRTOLa5o2sbV7qOolqpTq8r0nUDGNRMbLnvoXMjz+FpChwdeqIozdP\nRtnoK5pch59aKLDbO1RQs9ubTgHXNo8BcfKehhKEsXA/ct6cD9vKVZAUBTX5eSi5eTLKLr+Uod1W\ntUEty3Xd3gY9YDaz25viHn8rNIJj3hQNpt17kPPmfGSs+hySEKju0Q1Hb5mC8hEXM2BaI6DbG3o9\nVyOjhMfwboTgCmsUQeadPyPn9bnI+GINAKCqV0+U3DoF5RcNZ9g0R1EBSN7VyNjtTUmK4U0URSn/\n+xE5b8xF+tr1AADn6X1QMu1mVAw9n8FTX6hub73O2+3NoQRKcvwX0Ah2m1M4pXz/A9q9MQ9pX30N\nAKg8sz9Kpk2FY8g5DG2Am3AQtRLDuxHcmIROmRCwbNmKnDfmwrppCwDAMWggSqZNReXggckb2qG6\nvY1GrkZG1AoM70aw5U1tJgQs32xCu9fnwvLd9wCAiiHnoOTWqXAOOFPj4qKIm3AQRQzDuxFCCLYC\nqHWEgLVgA9q9MRep2/8HACgfej5Kbp2Kqn6na1xchLHbmyiqGN6NUCEgc1cxaglVRdraArR7fS5S\nduwEAJy8eDhKbpmC6j69NS4uAhQFgMRNOIg0xPBuhLflrXUVFNMUBemrv0S7N+fBvGsPhCTh5GWX\n4OgtU1BzWg+tqwsfRQBGdnsTxRKGdwhcXY2a5PEgY9XnyHlzPsz7CiFkGWW/uBwlN9+Emm5dta4u\nvBQBkZ4GWLjdKFEsYXiHICAg2Oqm+jwe2FasRM6/58NUVAyh06H0qtEomXojXHldtK4u/BQVIo3B\nTRSLGN4hqEL1zpRlgBMAyeWC7eNPkTP3LRgPHYaq1+PEuDEomXoj3B07al1eZKgCwmoFrFatKyGi\nEBjeIahChcSJN0lPqqlB5rKPkDPvHRiOlkA1GnH82qtxbPL1cOfmal1e5KgCIjUVSAu9mxERaY/h\nHYIKFRKb3UlLqq6GfckyZL+1AIZjx6GaTDh2w3U4duMkeLKztS4vsoSASEkB0tO1roSImsDwDsGj\nKpAl3puabORKJ+wffIjstxdCX1oGJTUFJVNuwLHrJ0KxZ2pdXuSpvuDOyNC6EiJqBsM7BK6ullxk\nhwNZ776PrAXvQV9eDsViwdFbpuD49ddCSZYgUwWE2czgJooTDO8QGN7JQXeyHFkL30PWu+9D53DA\nk56GI3dMw/HrroaaTOO9QkCYjIDNpnUlRNRCDO8QeJt3YtOVliH7nUWwL/4AOmcVPLYMHP71HTgx\nYTxUa5LdFiUEhMEAZNq1roSIWoHhHYIK7iiWiPTHjyP7rYWwf7AUcnU13Fl2HL39VpwY/0vvWG+y\nEQJCrwfsWVpXQkStxPAOgSusJRb9kaPIeWsBMpcuh1zjgjsnB4d/cydKf3kVhNmkdXnaEMK71CmD\nmyguMbxD4Jh3YjAcOozseW8jc/knkN1uuDq0R8mUG1F21SgIo1Hr8rSl00HYs7iRCFGcYniHIAS7\nzeOZ8UARcua+Bdsn/4GkKKjp3AklU29C2egrvK3NZCdJEFnZDG6iOMbfZJQwjPsKkfPvt2D7z38h\nqSpq8vNw9JYpODlyBEM7gMjOYXATxTn+RguB3ebxxbR7D3LemIeMz1ZDEgLVPbrh6C1TUD7iYkCn\n07q8mMLgJkoMDO8QVHabxwXzjp3e0P5iDQCgqndPHL11KiouHAbIXCEviBAQOe34cyFKEAzvENjy\njm0p2/+HnDfmIX3degCA84y+KLl1KiqGns9WZShCeFvcDG6ihMHwDkEIwRCIQanf/YCcN+YibcM3\nAIDKM/ujZNpUOIacw7+vxgjhnZzG4QOihMLwDkFAcFexWCEELFu2Iuf1f8O6+VsAgGPwQJTcOhWV\ngwcytJuiCojsbE7WI0pA/FcdgioEdMwEbQkB69cbkfPGPFi++x4AUDHkHJTcOhXOAWdqXFwcUAVE\nVhaDmyhB8V92PVxdLfoyVn6GnLnzYdpbiJqu+ag4fwgs336H1O3/AwCUD7sAJbdOQdUZp2tcaZxQ\nBYTdDhgMWldCRBHC8K5HQECw1R01GSs/Q5c/PuF/bN69B+bdewAAJy8ejpJbpqC6T2+tyos/ii+4\nk30FOaIEx/CuRxWqd91nBnjbeTzQORzQVTggOyq9nzsckCscvs8rIVdUQFdZiXTfbV711eR1wYGn\n/xLlwuOcIiDsmYCJwU2U6Bje9ahChZTMk6CEgFRTA12FL3wrvR91jkrIDgd0FRUBnzvqgrk2pCsc\nkKurT7kMY/HBMHwzSURRITIzAVOSbrRClGQY3vWoUON7prmiQK50BodqRUXA5w7ofIEsVzigq6z0\ntZAd/vCVFKVVLyl0OihWK1SrBTX5eVCsFihpVqjWtIDPrVCsVu/nFu8xJc2K/PsegXnvvgbXrO7W\nNSw/jqSgCAibDTCbta6EiKKE4V2PKrS9TUxyuQJCtcLX+m0YsP7Wb1CXdCV0lZWtfk3VZIKSZoXH\nlgFX587eYLVYoKZZveFrTQv43BfEaVZ/YKspKW2+Zavk1qlBY961jk29qU3XSzqK6g3uZNyPnCiJ\nJV14m5YsRurzz0G38yco3Xqg6va74Bp1pf+8IpS2d5sLAdlZBZ2joq6V63AEdTEHtn6Dx4B9x2pc\nrXtJSfK3ZF0d2vtC1hes/s99QWyx+lu83q9Jg2q1QGg4K/nk5ZcCALLnvgXz3n2o7tYVx6be5D9O\nTVAERHo6g5soCUkiTu6NKimpOOVrmJYsRvqdtzY4XvH0X70B7vHAUXoINWUl/jANmnRVr/UbNBnL\nF8KS2rp10VW9vi5kA7uXrZZ6LVyrvwtasXpDV0mzQk1N5bKXyUhRvcFtsWhdCRH5mHUpsJltYb1m\nTk5ayONJFd6ZF50P/Y/bGxwXej1gMEKqcrb6mkpqClRLcDdyUOD6uqCVNCvUtLS6z33d0cJk5Cph\n1DqqgLBYgLTQ/6iJSBvRDO+k6jbX7fwp9AmPB8ppvSDS0lBjMcNtSQ0I2Pqt4truZisUSypXsKLo\nUgVEaiqDmyjJJVXyKL36hGx5K7164+S7SwAAJ2rKUKPWRLs0oubVBnd6utaVEJHGkmqw1Pn7+0Me\nr5p2p/9zbgdKMUkVECkpDG4iAhDBlreqqpg+fTp27NgBo9GIGTNmID8/33/++++/x6xZsyCEQE5O\nDmbPng1ThBeYqBl/DcoBpL7wV99s8+6oui14trkQrZtwRhRxqoAwm4GMDK0rIaIYEbHwXrVqFVwu\nFxYtWoStW7di1qxZePnllwF4N//44x//iBdffBH5+fl47733UFxcjO7du0eqHL+a8degZvw1gMcD\n+fAhjllTbBPCO6nRFt5JMEQU3yKWXJs3b8bw4cMBAAMGDMC2bdv85/bu3QubzYZ///vf+Pnnn3HR\nRRdFJbhbgt3mFDOE8N6Dn2nXuhIiijERG/N2OBywWq3+xzqdDh6PBwBQWlqKb7/9FjfddBPefPNN\nbNiwAV999VWkSmkVld3mFAuE8N7CaM/SuhIiikERC2+r1YrKgKU6VVWF3tdFbbPZkJ+fjx49esBg\nMGD48OFBLXMtseVNmhPCO5zD4CaiRkQsvAcNGoQ1a7zbPW7duhW9evXyn+vSpQsqKytRWFgIANi0\naRN69uwZqVJaJU7WrKFEptNB2LO4eA8RNSpiY94jR45EQUEBJk2aBCEEZs6cieXLl8PpdGLixIn4\ny1/+gvvvvx9CCAwcOBAXX3xxpEppFQFtNyahJCdJEFnZDG4ialJSLY8apJHZ5kWVB6GTdeF9LaIW\nEjntGNxEcSqay6Mm1SItzYmT9zGUoER2DoObiFqE4R1AQEDwdydpQGTncHc4Imox/rYIICC8M32J\nokUI7xg3g5uIWoG/MQIoqgKJ3ZYULbXBreMcCyJqHYZ3ABUqZ5pTdKi+4ObyvETUBgzvAKrgbWIU\nBUJAZGUxuImozRjeARTBbnOKMFVAZNoBg0HrSogojjG8A3BpVIooRYWw2wGjUetKiCjOtTi8i4qK\n8MUXX0BRFBw4cCCSNWmGE80pYhThXfKUwU1EYdCi8P7kk0/wq1/9CjNmzEBZWRkmTZqEpUuXRrq2\nqFPBHcUoAhQVwp4JmBjcRBQeLQrv1157DQsWLIDVakVWVhaWLFmCf/7zn5GuLeq4whqFnSIgMjMB\nk0nrSogogbQovGVZDtqbu127dpATcFEJjnlTWCkqhC0DMJu1roSIEkyL7lXp2bMn3nrrLXg8Hvz4\n449455130KdPn0jXFnVCsNucwkQREOnpQEqK1pUQUQJqUfP58ccfx5EjR2AymfCHP/wBVqsVf/rT\nnyJdG1F8UgREehpgsWhdCSWYFXs/xoRlYzBw3umYsGwMVuz9WOuSSCMt2hL00UcfxVNPPRWNehoV\njS1BS6qPwSOU8L4OJRdVQFgsQFrobfyI2mrF3o/x8Jr7Ghx/+sK/YlS3KzWoiOqLuS1Bd+7cicrK\nyrAWFIs4YY1OiSogUlMZ3BQR//rhlZDHX//h1ShXQrGgRWPesizjkksuQbdu3WAKmDU7b968iBWm\nBd4qRm1WG9zp6VpXQglqT9nukMd3lu7APZ/dhfz0bshPz0deej7y07uhXWo7yFLiTSwmrxaF94MP\nPhjpOmKCEALg8qjUWqqASElhcFNEdbf1wM+lOxsclyUZXxatBrA66LhZZ0aX9DxvqKd5Qz0vvSu6\npneF3ZzFpaDjXIvC+9xzz8WXX36JDRs2wOPxYMiQIbjssssiXVvUCXBjEmolVUCYzUBGhtaVUIKb\n1u9OPLL2/gbHZw6bjfM7XoD95YXYV74P+8v3YX9FIQrL96GwvDBk4FsMFuSldUV+eteg1np+ej4y\nTOEds6XIaFF4v/baa1i5ciXGjBkDIQReeeUV7Nq1C3fddVek64sqVQjomN3UUkJAmE2Ajb/sKPL6\n2PsCANIMaajyVKG7rQem9b/TP1kt02zHWe0GBn2NEALHqkr8Qb6/Yp8/5HeX/YwfT2xv8DoZJhvy\n07siLy3fF+x1IW8xWBs8n7TRotnmY8aMwXvvvQezb7GJqqoqXH311VixYkXEC6wV6dnmQggUOw9B\nJ+vC+zqUmISAMBgAe5bWlVCSmL/935i96Sk8OfQpjD3t6lO+nipUHKk87Av2wNb6PhRXFMEjPA2+\nJsucHdBa94Z6Xno+8tLyYdZzMaJozjZvUctbCOEPbgAwmUzQJ9hexAICgq1uagkhIPR6BjdF1bri\nNQCACzoOC8v1ZElGB2tHdLB2xHkdLwg651E9OOQ4iH3le7G/ohD7y+uCfWvJFmw5uqnB9dpbOvhb\n6/np3Xxd8fnobO0Cg47r+odbixL4vPPOwz333IPx48cDAJYsWYIhQ4ZEtLBoExCQBMAhb2qSEN7e\nmqxsrSuhJOJ0O7HpyDfoY++LnNR2EX89vaxHl/Q8dEnPa3DOpbhQVHHA31rfX77PN9ZeiG8Ob8A3\nhzcEPV+WZHS0dPKFeV1rPT+tKzpYO0IvJ1ZDMFpa9FN77LHHsGDBAnz44YcQQuC8887DxIkTI11b\nVCmqwuCm5ul03q09iaJo85GNcKtuDO04XOtSYNQZ0d3WA91tPRqcq/JU4UB5IQr9rfW93rH28kKs\nP7gO6w+uC3q+Xjags7WzfxZ87Yz4/LR85Fra81a3JrQovJ1OJ4QQePHFF3HkyBEsXLgQbrc7obrO\nVaicaU5Nk2WIrGzeTkhRV1C8FgAwtJP24d2UFH0Ketn7oJe94d4XDpcD+yt8E+d8XfCFvu74feV7\nsabe8006E7r4u+Frx9a7omtGV2SZs5P+VrcWpe/999+P3r17AwAsFgtUVcVDDz2EOXPmRLS4aFIF\nbxOjJkgSg5s0s654DSwGS4PZ5PHEarTi9Kx+OD2rX4NzZdWlvta6b/JceaF/dvyusoa3uqXqU0O2\n1vPTu8JmzozGt6O5FoX3wYMH8cor3qX5rFYr7r33XowdOzaihUWbIpSkfydHjZAkiOwcBjdpYn95\nIfZXFOLSvJEwyAaty4kImzkTNnMmzsoZEHRcCIHj1ccatNb3V+zD3pO78dOJ/zW4VroxA3np+XXB\n7mut56V1hdWYOLe6tSi8JUnCjh07/K3v3bt3J1SXOcC9vKlxbHGTlmrHiWO9yzwSJElCdkoOslNy\nMDj37KBzqlBx1HkkoKVed8vbTyd+xLZj3ze4nt2c1eBWt/y0fHRJz0eKPr62721RAj/88MO49dZb\nkZubCwAoLS3F7NmzI1pYtHFPEmpACIicdoDMSTOknXW1490xMFktlsiSjPaWDmhv6YAhHc4POudR\nPThceagu0MsLUehboOa7km/x7dHNDa7XLjW3bjZ8Wt3M+M5pXWBs5la3FXs/xr9+eAV7ynajV2Yf\n/H7w/Rjf85qwfr/1NbtIy+rVq3HaaachNzcX8+bNw5o1a9CvXz/cd999UW19R3qRllLXSVQr1eF9\nDYpfQnhb3AnWw0TxpUapwfCFQ9DJ0glLxnHv7nBwKy4UOYoatNYLy/fhcOWhBs+XJRkdLB3rQj2j\nLtw7Wjvhv4X/CblV66sj3whLgDe2SEuT4f3666/jk08+wdNPPw2Px4NJkybhsccew65du6CqKh57\n7LFTLqylIh3eJ2rKUKPWhPc1KD4xuClGbDi4Hnf89xZMOf0WPHDOI1qXk/CqPdU4ULG/7ja3gAVq\njlWVNHi+XtJDkiS4VXeDc6dn9cMXE9efck1tWmFt6dKlWLRoEVJSUvDss89ixIgRuPbaayGEwOjR\no0+5qFjCMW8C4N1oJCuLwU0xYV2c3CKWKMx6M3pm9kLPzF4NzlW6Hdhfvt/XDb8Phb7W+vclW0Ne\na2fpTxGttcnfUJIkISXFO4j/9ddf44YbbvAfTzQMb4IqIOx2wJCYM3op/hQcXAuzPgWD6k3Wouiz\nGKzom3U6+madHnR8wrIxIXdu65XZ8F73cGpyJo5Op0N5eTkOHz6MH3/8EUOHDgUAFBcXJ9xsc85Y\nS3K1wW3kGswUGw5XHsLusp9xbvshMOlMWpdDjbitf+jdNX83qOE4eDg1mcB33HEHxo0bB4/Hg2uu\nuQbt2rXDJ598gr/97W/4zW9+E9HCoo0t7ySmMLgp9nCWeXyo3ZL19R9e9c42t/fB7wbdp/1s8yNH\njqC0tBR9+ni7AL788kuYzeaob0wS6QlrR6tKoEAN72tQ7FNUb3Cb2LKh2HLv6nvw2f6V+Gj8SuSl\n52tdDrVATG0Jmpub67+/GwAuuuii8FUVQ1QGd/JRBERmJoObYo5bdePrQ+vRJS2PwU0hcfUJn2Y6\nICjRKCqELQMI2KeeKFZ8d3QrHG4HhnW6UOtSKEYxvH045p1EFAGRkQGkxNdyiJQ8Cg7yFjFqGsPb\nR2XLOzkoAiI9DUhN1boSokYVFK+FQTbg7NxztS6FYhTDG+wyTxqKCpFmBSwWrSshatSxqhL8dOJ/\nGJx7DlINfJNJoTG84e0yF4m37gwFUgWE1QpYE2dLQEpMBcXeXcQ43k1NiVh4q6qKxx9/HBMnTsTk\nyZNRWFgY8nl//OMf8eyzz0aqjBYREOCQdwJTBURqKpAW+pYLolhSwCVRqQUiFt6rVq2Cy+XCokWL\ncP/992PWrFkNnrNw4ULs3NlwWbloU1SF2zUnKiEgUlKA9HStKyFqlqIq+OpQAdpbOqB7Rg+ty6EY\nFrHw3rx5M4YP975zHDBgALZt2xZ0fsuWLfjuu+8wceLESJXQYipUSGB6JxxVQJjMQEaG1pUQtci2\n4z/gZE0ZhnYcnpB7SFD4RCy8HQ4HrAHjizqdDh6PBwBw9OhR/OMf/8Djjz8eqZdvFVUIhneiUQWE\n2QzYwrvaEVEk1XaZc7ybmhOx3UWsVisqKyv9j1VV9W9m8umnn6K0tBR33HEHSkpKUF1dje7du+Pq\nq6+OVDlNUoXKd7mJRAgIk5HBTXGnoHgt9JIe53Y4T+tSKMZFLLwHDRqE1atXY/To0di6dSt69arb\nH3XKlCmYMmUKAOCDDz7Anj17NAtugEujJhQhIAwGINOudSVErVJWXYptx77HoNyzkWbk5EpqWsTC\ne+TIkSgoKMCkSZMghMDMmTOxfPlyOJ3OmBjnDsTbvBOEEBB6PWDP0roSolZbf7AAAoKzzKlFIhbe\nsizjz3/+c9CxHj0azp7UssVdi0ujJgAhvDvEMbgpTtUuicrxbmoJLtIC75g3xTmdDsKeBd7zR/FI\nFSrWF69DdkoOemf20bocigMMb7DlHfckCSIrm8FNcWvHiZ9wvPoYLug4jJNnqUUY3mB4xzuRncPg\npri2rngNAK6qRi3H8AY4Yy2OMbgpERQUr4UsyTi/wwVal0JxguENtrzjkhDe4Jb5nzDFtwpXBb4r\n+Rb9ss+EzZypdTkUJ/ibD9wSNO6owjvGzeCmBLDh0HooQsHQjuwyp5bjbz9wkZa4IgREdrb3tjCi\nBMBdxKgtGN5gyztu1La4GdyUIIQQKCheC5vJhjOy+mldDsURhjc45h0XVAGRlcXgpoSyu2wXjjgP\n4/yOw6CTdVqXQ3GE4Q0wumOdKiDsdsBg0LoSorDiLWLUVkkf3kIIdpvHMkVAZNoBo1HrSojCrnZJ\n1KEdh2lcCcUbhjcEBG8Tjk2KgLBnAiYGNyUep7sSW45sQl/7GchKyda6HIozDG8I9pvHIkWFyLQB\nJpPWlRBFxMbD38CtutllTm2S9OGtqAoX6Io1ioCw2QCzWetKiCKG4910KpI+vFWokMD0jhmKCmHL\nAFJStK6EKGKEEFhXvAZWgxVn5pyldTkUhxjeQjC8Y4UiINLTGdyU8PZXFKLYUYTzOg6FQeZdFNR6\nSX/TrCpUbsEXLsI3f0AI+D4BINX9kQIfSg3+CIsZsFg0K58oWvyrqnFJVGojhneyLI3aIFgDNRGu\nsuwNVvgeBz23XgDLcvCfwHNE5Fc33s1bxKhtkj68Y+YW7+bCVZKabrU2Fa71A7V+sDJciaKm2lON\nTYe/QQ9bT7S3dNC6HIpTDO+W3ifmD9f6LfUWdglDAiQZDYK1fsAyXIkS2pYjm1CtVGMYZ5nTKUje\n8JYkCJMRirsaQjY03yWs04XuDmawElEr+FdV63ShxpVQPEve8NbpIHJyIar0gOrSuhoiShLritcg\nRZ+KQe0Ga10KxbGkv1WMO4oRUbQUO4qw9+QenNt+CIw6LvtLbcfwbjCGTUQUGeuL1wHgqmp06hje\nMTPdnIgSXe0tYsM43k2niOHNbnMiigK34sLXh75CfnpXdE7ronU5FOcY3mx5E1EUbC35Fk6Pk13m\nFBZJH94qW95EFAVcEpXCKenDmw1vIoqGdcVrYJSNOLv9uVqXQgmA4c2WNxFF2FHnEews3YGz25+L\nFD13zaNTl9ThLYSASJaNSYhIM7xFjMItucObrW4iioLaJVG5njmFS3KHNwe8iSjCPKoH6w8WoKOl\nE7qmd9e6HEoQSR3eqlAhgRuLEFHkbDv2PSpc5RjaaRgkbmREYZLU4a0Ihf+YiCiiam8R46pqFE5J\nHd5seRNRpBUUr4Ve0uPcDudpXQolEIY3W95EFCEnqk9g+/FtGJg7CBaDVetyKIEkfXgTEUXKVwfX\nQUBwVTUKu6QOb94qRkSRxPFuipTkDm/eKkZEEaIKFQUH1yEnJQc9M3trXQ4lmKQOb3abE1Gk/Hh8\nO0qrT2Bop+GcW0Nhl9ThDXabE1GE+HcR46pqFAH6SF1YVVVMnz4dO3bsgNFoxIwZM5Cfn+8//9FH\nH2Hu3LnQ6XTo1asXpk+fDlmO7nsJjnkTUaQUHFwLWZJxfoehWpdCCShiablq1Sq4XC4sWrQI999/\nP2bNmuU/V11djeeffx7z5s3DwoUL4XA4sHr16kiV0jiOeRNRBJTXnMR3JVtxZvZZSDdlaF0OJaCI\nhffmzZsxfLi3u2jAgAHYtm2b/5zRaMTChQuRkuLdGs/j8cBkMkWqlEZxzJuIImHDofVQhcouc4qY\niIW3w+GA1Vq3KIFOp4PH4/G+qCwjOzsbADB//nw4nU4MHRr9riV2mxNRJHC8myItYmPeVqsVlZWV\n/seqqkKv1wc9nj17Nvbu3Ys5c+ZoMhtTCAGujkpE4SSEQEHxWmSaMnF6Vj+ty6EEFbGW96BBg7Bm\nzRoAwNatW9GrV6+g848//jhqamrw0ksv+bvPo01ly5uIwuzn0h04WnUUF3QaBllK8ht6KGIi1vIe\nOXIkCgoKMGnSJAghMHPmTCxfvhxOpxP9+vXD4sWLcfbZZ2Pq1KkAgClTpmDkyJGRKickIQDefklE\n4bTuoK/LnEuiUgRJIk6WGSspqQj7NYsriqGL8u1pRJTYpv1nCjYe/hqrr1uPrJQsrcuhKDLrUmAz\n28J6zZyctJDHkza5hBAQ4GxzIgqfSrcD3x7dgjOy+jG4KaKSN7w53k1EYfbNoQ3wqG7OMqeIS97w\njo/RAiKKI+t4ixhFSfKGNwQk3idGRGEihEDBwbVIM6ajf/ZZWpdDCS5pw9ujerjTDxGFzb7yvTjo\nKMb5HS6AXo7YjTxEAJI4vFWhsuVNRGGzrti7rgW7zCkakju82fImojCpXRL1At7fTVGQ1OFNRBQO\n1Z5qbD6yET0zeyHXkqt1OZQEkja8easYEYXLpiPfoEapwbBOF2pdCiWJ5A1v3ipGRGHiH+9mlzlF\nSfKGN1veRBQmBcVrkaJPxcB2g7QuhZJE8oY3x7yJKAyKKg6gsHwfhnQ4DwadUetyKEkkb3iz5U1E\nYVDbZc7xboqmpA1vcMybiMKg7haxYRpXQskkecObiOgUuRQXvjn8Nbqmd0PntC5al0NJhOFNRNRG\n35As5zoAABO6SURBVB7djCqPk6uqUdQxvImI2ojj3aQVhjcRURsVFK+DSWfC4NxztC6FkgzDm4io\nDQ5XHsausp04u/25MOvNWpdDSYbhTUTUBusPemeZc1U10gLDm4ioDdb5bhHjeDdpgeFNRNRKbtWN\nDQcL0MnaGfnpXbUuh5IQw5uIqJV+KPkODrcDQzsNhyRJWpdDSYjhTUTUSgXsMieNMbyJiFppXfFa\n6GUDzm0/ROtSKEkxvImIWuF41TH8eGI7BrUbjFSDRetyKEkxvImIWmH9wXUAwCVRSVMMbyKiVuB4\nN8UChjcRUQspqoL1B9ehXWouTrP11LocSmIMbyKiFvrf8e0oqynjLWKkOYY3EVELFRz07iLGJVFJ\nawxvIqIWWle8FjpJh/M6XqB1KZTkGN5ERC1QVl2Kbce+x5k5A5BuTNe6HEpyDG8iohbYcOgrqELl\nLWIUExjeREQtUHeLGMObtMfwJiJqhipUFBxcC7s5C33sp2tdDhHDm4ioOTtLd+BYVQku6DgMsqTt\nr02PUKCoiqY1kPYY3kREzajtMo+F8W6zbERWSjaMshGKqkIIoXVJpAG91gUQEcW6guK1kCDhgo5D\ntS4FkiTDrDfDrDdDCAGnx4lqtxPVSg1kSebiMUmC4U1E1ASHy4GtR7fgjOz+yDTbtS4HUkCHqSRJ\nsBgssBgsUIUKh8uBGk8ValQX9DJ/vScy/u0SETXh60NfwSM8MdFlDqDRMXdZkpFuSgdM6VBUBRWu\nCtR4quARCnSyLspVUqQxvImImlBw0HeLWIwsidqSCXM6WQeb2QbABrfi9ga5Ug0BofmEOwqPiIW3\nqqqYPn06duzYAaPRiBkzZiA/P99//vPPP8c//vEP6PV6TJgwAdddd12kSiEiahMhBAqK1yLdmIF+\n2WdqXQ6AloV3IIPOAHuKt7u/2lONSrcD1Z4ayJLE8fE4FrHwXrVqFVwuFxYtWoStW7di1qxZePnl\nlwEAbrcbTz31FBYvXoyUlBRcf/31GDFiBLKzsyNVDhFRq+05uRuHKg/iF11Hx0TXsyrUUxrLDjXR\nrUqp5vh4HIpY/8nmzZsxfLi3m2nAgAHYtm2b/9zu3buRl5eHjIwMGI1GDB48GBs3boxUKUREbRJL\nt4gBpx7etWonumWl5qCjtRNS9RboIMMjeP94vIjY2y2HwwGr1ep/rNPp4PF4oNfr4XA4kJaW5j9n\nsVjgcDgiVQoRUZvUhvcFHYdpXEmdcI9ZB0508ygeONwOTnSLAxELb6vVisrKSv9jVVWh1+tDnqus\nrAwKcyIirTndTmw68g16Z/ZBTmo7rcvxElJEJ5zpdXrYdN6Jbi6PyxvknOgWkyL2tzFo0CCsWePd\nuH7r1q3o1auX/1yPHj1QWFiIsrIyuFwubNq0CQMHDoxUKURErbbpyDdwq24M63Sh1qX4RTNAjXoj\n7Cl2dLB2RKbJDoNs4IpuMSRiLe+RI0eioKAAkyZNghACM2fOxPLly+F0OjFx4kQ88sgjmDZtGoQQ\nmDBhAnJzcyNVChFRq8XaeDcAyBpNDk8xpCDFkOKf6FblrkS1UsOJbhqSRJy8jSopqQjr9Y47S+AW\nnrBek4gSx1UfXI7j1cewZuIGGHRGrcsBAMiQ0M7SXusyAMC/olu1xwmX8EAvcXzcrEvx3V8fPjk5\noYeU+baJiKie/eWF2F9RiBFdLouZ4Aai223enNqJbukBE92qPU6oQkCWY6fORMXwJiKqp7bLfFjn\n2BnvBrybksSiwIluNZ4aVLorOdEtwhjeRET11C6JGku3iAHBm5LEKpPeBJPeBACoclfB6alEjVID\nCVzRLZwY3kREAWqUGnxz+Gt0z+iBjtZOWpcTJN5asYET3Srdlaj2OFGjuHj/eBgwvImIAmw5sgnV\nnqqYmmVeK97Cu5YkSbAarbAarVCF6t/xjBPd2o7hTUQUwD/eHUP3d9eK1/AOJEsyMkwZgCmDE91O\nAcObiChAQfFamHVmDMo9W+tSggghEu6+6lAT3ao91YDEiW7NSaz/EoiITsEhx0HsPrkLwztdBJPO\npHU5QVShQpfAXcyBE92cbieqPE5Ue6ohSzInuoXA8CYi8qmdZR6L490CImkmeqUaUpFqSIUQAg6X\nAzVKFSe61cPwJiLyieXxbgCQkFwtUEmSkGZKQxrSgie6qe6EG0JoreT+7omIfNyqGxv+v717D4qq\n/v84/txYwBFEkrxh+VVEC8dhugGBqKGmgSJmgroMiPWHiYpXBMXrCE7miCkNJdWMpVRD2kiMpjaK\nqYCX0Qm/KNBoZYqIP/HCTWTZ/fz+IDdJ9Mv3W8Puad+Pv5bd4/H92uXw5pzz4fOpLOSZLn3p6/Yv\na5fzEB32ffn4wYFuRpPRsnSpvQ50k+YthBBA8fUfqTfWE+410dqltOkJO27cf+bo4MiTDk8CT/4+\n0K2OxuZ7djXQTZq3EEIABRUtSxjb4v1uaFmURDzsoYFuv6949k8f6CbNWwghaBms5viEI369Aqxd\nSpvs5Yzyr7CngW7SvIUQdu//Gq5TdrOUV3oH0dmxs7XLaZM07/Z71EA3o7n5H9PIpXkLIexe4dUC\nwHYvmYPtrihm6/6pA92keQsh7J7lfrenNO9/sgcHujU2N9JgrNfsQDdp3kIIu2Yymyi8WkAvl94M\ncPe2djmPpLXmYus66TvRSd8JpRQNzQ00Ghs0NdBNmrcQwq6VVP+bmqY7vPavsTb9Q/sJDazlrUU6\nnQ4XRxdcHF0sA90amxu4Z26y6YlgbLcyIYToAPdnVbPl+91K2c/UqNZkGejm3AWT2dSy4pmxgWZl\nsrn3X6eUUtYuQgghhBDtJ9dhhBBCCI2R5i2EEEJojDRvIYQQQmOkeQshhBAaI81bCCGE0Bhp3kII\nIYTG2G3zNhqNJCYmYjAYmDx5MgcPHuTSpUtMmzYNg8HAqlWrMJvN1i6z3aqrqxkxYgQXL17UbI6t\nW7cyZcoUJk2axNdff63ZHEajkUWLFjF16lQMBoMmP5Pi4mJiYmIAHll7Tk4OkyZNIioqivz8fGuW\n+0gP5igtLcVgMBATE8Pbb7/NjRs3AO3luC8vL48pU6ZYvtZajurqambNmkV0dDRTp07lt99+A7SX\no7S0lKioKKZNm8bSpUs77vhQdmrnzp0qNTVVKaXUrVu31IgRI9TMmTPV8ePHlVJKrVixQh04cMCa\nJbZbU1OTio+PV2PGjFEXLlzQZI7jx4+rmTNnKpPJpOrq6tSWLVs0mUMppb7//nuVkJCglFLq2LFj\nas6cOZrKkpWVpcaPH68iIyOVUqrN2q9fv67Gjx+v7t27p2pqaiyPbcmfc0RHR6vz588rpZT68ssv\n1bp16zSZQymlzp07p2JjYy3PaTFHUlKS2rNnj1JKqaKiIpWfn6/JHPHx8erw4cNKKaUWLlyoDh48\n2CE57PbM+/XXX2fevHnA77MXOThw7tw5/P39ARg+fDiFhYXWLLHd1q9fz9SpU+nRoweAJnMcO3aM\nQYMGMXv2bN555x1effVVTeYA6N+/PyaTCbPZTF1dHXq9XlNZ+vbtS0ZGhuXrtmo/e/YsL7zwAk5O\nTnTp0oW+fftSVlZmrZLb9Occ6enp+Pj4AGAymXB2dtZkjlu3bpGens6yZcssz2kxx5kzZ6iqqiIu\nLo68vDz8/f01mcPHx4fbt2+jlKK+vh69Xt8hOey2ebu4uODq6kpdXR0JCQnMnz8fpZRlbmMXFxdq\na2utXOV/9s0339CtWzeGDftjakct5rh16xYlJSVs3ryZNWvWsHjxYk3mAOjcuTMVFRWEhoayYsUK\nYmJiNJVl7Nix6PV/zJzcVu11dXV06dLFso2Liwt1dXUdXuvj/DnH/V9uz5w5w44dO4iLi9NcDpPJ\nREpKCkuXLsXFxcWyjdZyAFRUVODm5sa2bdvo3bs3H3/8sSZz9OvXj7S0NEJDQ6muriYgIKBDctht\n8waorKwkNjaWiIgIwsPDW63tWl9fj5ubmxWra59du3ZRWFhITEwMpaWlJCUlcfPmTcvrWsnh7u5O\ncHAwTk5OeHl54ezs3KrBaSUHwLZt2wgODmb//v3k5uaSnJyM0Wi0vK6lLECbx4Wrqyv19fWtnn/w\nh5Wt2rt3L6tWrSIrK4tu3bppLse5c+e4dOkSq1evZuHChVy4cIG0tDTN5YCWY37kyJEAjBw5kpKS\nEk3mSEtLIzs7m3379jFx4kTefffdDslht837xo0bvPXWWyQmJjJ58mQABg8ezIkTJwA4cuQIL7/8\nsjVLbJfs7Gx27NjB9u3b8fHxYf369QwfPlxzOV566SWOHj2KUoqqqiru3r1LYGCg5nIAuLm5WQ7U\nrl270tzcrMnvrfvaqt3X15fTp09z7949amtruXjxIoMGDbJypY+Xm5trOVaeeeYZAM3l8PX1Zc+e\nPWzfvp309HS8vb1JSUnRXA5oOeZ/+OEHAE6dOoW3t7cmc3Tt2hVXV1eg5epOTU1Nh+Sw21XFPvro\nI2pqasjMzCQzMxOAlJQUUlNTSU9Px8vLi7Fjx1q5yv9NUlISK1as0FSOkJAQTp06xeTJk1FKsXLl\nSp5++mnN5QCIi4tj2bJlGAwGjEYjCxYsYMiQIZrMAm1/Pzk4OBATE4PBYEApxYIFC3B2drZ2qY9k\nMplIS0ujd+/ezJ07FwA/Pz8SEhI0leNRunfvrrkcSUlJLF++nK+++gpXV1c2btxI165dNZcjNTWV\nBQsWoNfrcXR0ZO3atR3yeciqYkIIIYTG2O1lcyGEEEKrpHkLIYQQGiPNWwghhNAYad5CCCGExkjz\nFkIIITRGmrcQHWjNmjVEREQQFhbGkCFDiIiIICIigl27drV7H5s3b+bgwYOP3SYiIuKvlmoTrly5\nYpnIQwjxB/lTMSGs4MqVK8TGxnLo0CFrl2LT5H0Som12O0mLELYmIyODH3/8kcrKSqKjoxk4cCCb\nNm2isbGRO3fukJiYSGhoKMnJyfj7++Pv78+cOXMYOHAgpaWleHh4sHnzZtzd3Xn22WcpLy8nIyOD\nqqoqLl26REVFBZGRkcyaNQuj0ciqVas4ffo0PXv2RKfTER8fT0BAQKuasrKy+O677zCZTAQHB5OY\nmMihQ4dYv349eXl5XLt2jZiYGHJycqipqWHt2rU0NDRw8+ZNZsyYQWxsLBkZGVy9epXy8nKqq6uZ\nP38+x48fp7i4mOeee45NmzZx8uRJMjIy0Ov1VFZW4uvrS1paWqtabty4wcqVK7l27Ro6nY5FixYR\nFBREUVERGzZsAFpmu9q4cSPdunXrsM9NCGuQ5i2EDWlqamLv3r0AJCQkkJqayoABAygqKmLdunWE\nhoa22r6srIx169YxePBg5s6dS15e3kPrPpeXl5OdnU1tbS2jR48mOjqa3Nxc7t69y759+7h69Srh\n4eEP1XLkyBFKSkrYuXMnOp2OxMREvv32WyIiIjhw4AAffvghJ0+eJCkpiV69evHpp58SHx9PYGAg\nly9fZsKECcTGxgLw008/kZOTw5kzZ5g+fTp5eXn069ePsLAwysvLgZaVsXbv3k3//v2ZN28e2dnZ\nvPbaa5Z60tLSePPNNxk1ahTXr1/HYDCwe/duMjMzWb16Nb6+vnz++eecP3+e4ODgv/VzEcLWSPMW\nwob4+vpaHm/YsIH8/Hz27dtHcXFxq4UO7vPw8GDw4MEADBw4kDt37jy0TUBAAE5OTnh4eODu7k5t\nbS0FBQVERUWh0+no06cPgYGBD/27oqIizp49y6RJkwBobGzE09MTaJlKOCwsjBdffJFx48YBkJyc\nzNGjR9m6dSvl5eU0NDRY9jV06FD0ej2enp50794db29vAHr27Gmp2c/PDy8vL6Dlnn1OTk6r5l1Y\nWMjPP//Mli1bAGhububy5cuMGjWKOXPmMHr0aEaNGsXQoUPb+3YLoVnSvIWwIZ06dbI8NhgMBAQE\nEBAQQGBgIIsXL35o+wfnS9bpdLQ1hKWtbRwcHDCbzY+txWQyMX36dGbMmAFATU0NDg4OQMslbAcH\nB3755ReamppwcnJi/vz5uLm5ERISQlhYGHv27LHsy9HR0fL4weUUH3R/34ClxgeZzWY+++wz3N3d\nAaiqquKpp57Cx8eHkJAQ8vPz2bBhA2fPnmXWrFmPzSaE1slocyFs0O3bt/n111+ZN28eI0aMoKCg\nAJPJ9LftPygoiL1791pWcTt58qRlze77XnnlFXJzc6mvr6e5uZnZs2ezf/9+TCYTS5cuJSUlBT8/\nP95//30ACgoKSEhIYPTo0Zw6dQrgv6r59OnTVFVVYTab2b17N8OHD3+oni+++AKACxcuMGHCBO7e\nvUtkZCT19fXExcURFxfH+fPn/8pbI4QmyJm3EDbI3d2dyMhIxo0bh6urK88//zyNjY2tLkX/FVFR\nUZSVlREeHk737t3x9PRsddYPLWssl5WVERUVhclkYtiwYbzxxht88skneHh4MGbMGIKCghg/fjxj\nxoxh7ty5GAwG3Nzc6N+/P3369OHKlSvtrqlHjx4sWbKEqqoqhg4dSmRkJJWVlZbXly9fzsqVKy33\n59977z1cXV1ZuHAhycnJ6PV6nJ2dWbNmzd/yHglhy+RPxYSwQ4cPH0YpRUhICLW1tUycOJFdu3ZZ\nLkl3tBMnTvDBBx+wfft2q/z/QmiNnHkLYYcGDBjAkiVLLJe8ExISrNa4hRD/PTnzFkIIITRGBqwJ\nIYQQGiPNWwghhNAYad5CCCGExkjzFkIIITRGmrcQQgihMdK8hRBCCI35f9s5ar4zxL12AAAAAElF\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x106175a20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.927e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.356e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.927e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 6 iterations, alpha=1.461e-03, previous alpha=1.256e-03, with an active set of 5 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=8.796e-05, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=7.417e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 10 iterations, alpha=6.001e-05, previous alpha=5.306e-05, with an active set of 7 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=6.900e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.397e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.397e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 16 iterations, alpha=1.278e-03, previous alpha=1.246e-03, with an active set of 7 regressors.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intracellular Mevalonate (uM)\n",
      "Warning: xgboost.XGBRegressor is not available and will not be used by TPOT.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:   1%|▏         | 232/17030 [00:54<36:40,  7.63pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1 - Current best internal CV score: 0.000599119464603647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:   2%|▏         | 341/17030 [01:22<25:40, 10.83pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 2 - Current best internal CV score: 0.0005669557864563479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:   3%|▎         | 446/17030 [01:38<30:43,  9.00pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 3 - Current best internal CV score: 0.0005272704479917016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:   3%|▎         | 556/17030 [01:57<35:57,  7.63pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 4 - Current best internal CV score: 0.0005272704479917016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:   4%|▍         | 661/17030 [02:17<30:43,  8.88pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 5 - Current best internal CV score: 0.0005203097506327673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:   4%|▍         | 765/17030 [02:28<22:39, 11.96pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 6 - Current best internal CV score: 0.0005203097506327673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:   5%|▌         | 867/17030 [02:43<48:18,  5.58pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 7 - Current best internal CV score: 0.0005203097506327673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:   6%|▌         | 962/17030 [02:56<27:52,  9.61pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 8 - Current best internal CV score: 0.0005203097506327673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:   6%|▌         | 1059/17030 [03:09<39:53,  6.67pipeline/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 9 - Current best internal CV score: 0.0005203097506327673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:   7%|▋         | 1139/17030 [03:19<19:45, 13.41pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 10 - Current best internal CV score: 0.0005203097506327673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:   7%|▋         | 1230/17030 [03:31<24:44, 10.65pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 11 - Current best internal CV score: 0.0005171237779571973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:   8%|▊         | 1321/17030 [03:46<25:58, 10.08pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 12 - Current best internal CV score: 0.0005148224734659406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:   8%|▊         | 1403/17030 [04:01<32:03,  8.12pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 13 - Current best internal CV score: 0.0005148224734659406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:   9%|▉         | 1491/17030 [04:18<20:43, 12.50pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 14 - Current best internal CV score: 0.0005148224734659406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:   9%|▉         | 1577/17030 [04:39<36:49,  6.99pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 15 - Current best internal CV score: 0.0005148224734659406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  10%|▉         | 1663/17030 [04:59<28:04,  9.12pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 16 - Current best internal CV score: 0.0005148224734659406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  10%|█         | 1758/17030 [05:22<43:06,  5.90pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 17 - Current best internal CV score: 0.0005148224734659406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  11%|█         | 1840/17030 [05:37<33:52,  7.47pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 18 - Current best internal CV score: 0.0005148224734659406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  11%|█▏        | 1924/17030 [05:56<33:52,  7.43pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 19 - Current best internal CV score: 0.0005148224734659406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  12%|█▏        | 2016/17030 [06:15<25:14,  9.91pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 20 - Current best internal CV score: 0.0005148224734659406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  12%|█▏        | 2092/17030 [06:34<22:57, 10.85pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 21 - Current best internal CV score: 0.0005148224734659406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  13%|█▎        | 2174/17030 [06:50<29:20,  8.44pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 22 - Current best internal CV score: 0.0005148224734659406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  13%|█▎        | 2265/17030 [07:13<39:07,  6.29pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 23 - Current best internal CV score: 0.0005148224734659406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  14%|█▍        | 2366/17030 [07:36<24:37,  9.92pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 24 - Current best internal CV score: 0.0005148224734659406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  14%|█▍        | 2452/17030 [07:56<27:25,  8.86pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 25 - Current best internal CV score: 0.0005148224734659406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  15%|█▍        | 2542/17030 [08:15<21:33, 11.20pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 26 - Current best internal CV score: 0.0005148224734659406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  15%|█▌        | 2632/17030 [08:38<37:58,  6.32pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 27 - Current best internal CV score: 0.0005148224734659406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  16%|█▌        | 2717/17030 [08:55<20:54, 11.41pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 28 - Current best internal CV score: 0.0005148224734659406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  16%|█▋        | 2796/17030 [09:13<29:50,  7.95pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 29 - Current best internal CV score: 0.0005148224734659406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  17%|█▋        | 2877/17030 [09:30<26:48,  8.80pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 30 - Current best internal CV score: 0.0005148224734659406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  17%|█▋        | 2963/17030 [09:45<25:55,  9.04pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 31 - Current best internal CV score: 0.0005148224734659406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  18%|█▊        | 3046/17030 [10:00<32:19,  7.21pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 32 - Current best internal CV score: 0.0005148224734659406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  18%|█▊        | 3134/17030 [10:18<26:39,  8.69pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 33 - Current best internal CV score: 0.0005148224734659406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  19%|█▉        | 3219/17030 [10:35<22:12, 10.36pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 34 - Current best internal CV score: 0.0005148224734659406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  19%|█▉        | 3310/17030 [10:56<19:31, 11.71pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 35 - Current best internal CV score: 0.0005148224734659406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  20%|█▉        | 3399/17030 [11:16<26:23,  8.61pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 36 - Current best internal CV score: 0.0005148224734659406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  21%|██        | 3494/17030 [11:32<24:33,  9.19pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 37 - Current best internal CV score: 0.0005148224734659406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  21%|██        | 3575/17030 [11:51<44:33,  5.03pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 38 - Current best internal CV score: 0.0005148224734659406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  22%|██▏       | 3662/17030 [12:13<27:34,  8.08pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 39 - Current best internal CV score: 0.0005148224734659406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  22%|██▏       | 3753/17030 [12:34<27:30,  8.04pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 40 - Current best internal CV score: 0.0005148224734659406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  23%|██▎       | 3834/17030 [12:49<26:00,  8.45pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 41 - Current best internal CV score: 0.0005148224734659406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  23%|██▎       | 3915/17030 [13:08<33:34,  6.51pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 42 - Current best internal CV score: 0.0005148224734659406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  23%|██▎       | 3997/17030 [13:27<34:54,  6.22pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 43 - Current best internal CV score: 0.0005148224734659406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  24%|██▍       | 4090/17030 [13:44<20:07, 10.71pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 44 - Current best internal CV score: 0.0005148224734659406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  25%|██▍       | 4186/17030 [14:00<25:14,  8.48pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 45 - Current best internal CV score: 0.0005148224734659406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  25%|██▌       | 4275/17030 [14:16<16:43, 12.71pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 46 - Current best internal CV score: 0.0005148224734659406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  26%|██▌       | 4360/17030 [14:35<25:11,  8.38pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 47 - Current best internal CV score: 0.0005148224734659406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  26%|██▌       | 4451/17030 [14:52<16:38, 12.60pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 48 - Current best internal CV score: 0.0005148224734659406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  27%|██▋       | 4538/17030 [15:06<17:21, 11.99pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 49 - Current best internal CV score: 0.0005148224734659406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  27%|██▋       | 4631/17030 [15:24<24:56,  8.28pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 50 - Current best internal CV score: 0.0005148224734659406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  28%|██▊       | 4723/17030 [15:40<26:19,  7.79pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 51 - Current best internal CV score: 0.0005148224734659406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  28%|██▊       | 4816/17030 [15:58<31:58,  6.37pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 52 - Current best internal CV score: 0.0005148224734659406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  29%|██▉       | 4906/17030 [16:14<18:44, 10.78pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 53 - Current best internal CV score: 0.0005148224734659406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  29%|██▉       | 4978/17030 [16:27<20:29,  9.80pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 54 - Current best internal CV score: 0.0005148224734659406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  30%|██▉       | 5062/17030 [16:39<09:32, 20.91pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 55 - Current best internal CV score: 0.0005148224734659406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  30%|███       | 5150/17030 [16:55<23:20,  8.48pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 56 - Current best internal CV score: 0.0005148224734659406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  31%|███       | 5228/17030 [17:09<24:37,  7.99pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 57 - Current best internal CV score: 0.0005148224734659406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  31%|███       | 5312/17030 [17:27<17:01, 11.47pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 58 - Current best internal CV score: 0.0005148224734659406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  32%|███▏      | 5396/17030 [17:41<20:32,  9.44pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 59 - Current best internal CV score: 0.0005148224734659406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  32%|███▏      | 5491/17030 [17:59<19:26,  9.89pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 60 - Current best internal CV score: 0.0005148224734659406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  33%|███▎      | 5586/17030 [18:17<17:23, 10.97pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 61 - Current best internal CV score: 0.0005148224734659406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  33%|███▎      | 5665/17030 [18:34<20:55,  9.05pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 62 - Current best internal CV score: 0.0005148224734659406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  34%|███▎      | 5737/17030 [18:48<19:52,  9.47pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 63 - Current best internal CV score: 0.0005148224734659406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  34%|███▍      | 5814/17030 [19:02<23:41,  7.89pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 64 - Current best internal CV score: 0.0005148224734659406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  35%|███▍      | 5906/17030 [19:23<30:29,  6.08pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 65 - Current best internal CV score: 0.0005148224734659406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  35%|███▌      | 5993/17030 [19:42<35:33,  5.17pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 66 - Current best internal CV score: 0.0005148224734659406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  36%|███▌      | 6081/17030 [20:01<17:01, 10.72pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 67 - Current best internal CV score: 0.0005148224734659406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  36%|███▌      | 6159/17030 [20:15<22:46,  7.96pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 68 - Current best internal CV score: 0.0005148224734659406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  37%|███▋      | 6253/17030 [20:35<19:56,  9.00pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 69 - Current best internal CV score: 0.0005148224734659406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  37%|███▋      | 6345/17030 [20:52<21:00,  8.47pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 70 - Current best internal CV score: 0.0005148224734659406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  38%|███▊      | 6434/17030 [21:12<28:02,  6.30pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 71 - Current best internal CV score: 0.0005148224734659406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  38%|███▊      | 6520/17030 [21:29<18:08,  9.65pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 72 - Current best internal CV score: 0.0005039417772389942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  39%|███▉      | 6607/17030 [21:47<16:42, 10.39pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 73 - Current best internal CV score: 0.0005039417772389942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  39%|███▉      | 6691/17030 [22:04<20:31,  8.40pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 74 - Current best internal CV score: 0.0005039417772389942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  40%|███▉      | 6770/17030 [22:18<25:10,  6.79pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 75 - Current best internal CV score: 0.0005039417772389942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  40%|████      | 6851/17030 [22:29<17:06,  9.92pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 76 - Current best internal CV score: 0.0005039417772389942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  41%|████      | 6936/17030 [22:44<21:32,  7.81pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 77 - Current best internal CV score: 0.0005039417772389942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  41%|████▏     | 7027/17030 [23:10<21:18,  7.82pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 78 - Current best internal CV score: 0.0005039417772389942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  42%|████▏     | 7116/17030 [23:28<19:33,  8.45pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 79 - Current best internal CV score: 0.0005039417772389942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  42%|████▏     | 7199/17030 [23:44<15:55, 10.29pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 80 - Current best internal CV score: 0.0005039417772389942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  43%|████▎     | 7282/17030 [24:03<24:30,  6.63pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 81 - Current best internal CV score: 0.0005039417772389942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  43%|████▎     | 7360/17030 [24:17<20:26,  7.88pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 82 - Current best internal CV score: 0.0005039417772389942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  44%|████▍     | 7454/17030 [24:35<20:52,  7.64pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 83 - Current best internal CV score: 0.0005039417772389942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  44%|████▍     | 7544/17030 [24:52<22:23,  7.06pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 84 - Current best internal CV score: 0.000502662287490285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  45%|████▍     | 7632/17030 [25:07<20:17,  7.72pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 85 - Current best internal CV score: 0.000502662287490285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  45%|████▌     | 7717/17030 [25:22<15:34,  9.97pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 86 - Current best internal CV score: 0.000502662287490285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  46%|████▌     | 7800/17030 [25:33<18:23,  8.37pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 87 - Current best internal CV score: 0.000502662287490285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  46%|████▋     | 7887/17030 [25:48<15:06, 10.08pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 88 - Current best internal CV score: 0.000502662287490285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  47%|████▋     | 7975/17030 [26:04<20:34,  7.34pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 89 - Current best internal CV score: 0.000502662287490285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  47%|████▋     | 8072/17030 [26:22<19:20,  7.72pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 90 - Current best internal CV score: 0.000502662287490285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  48%|████▊     | 8153/17030 [26:35<13:58, 10.58pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 91 - Current best internal CV score: 0.000502662287490285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  48%|████▊     | 8243/17030 [26:52<19:04,  7.68pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 92 - Current best internal CV score: 0.000502662287490285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  49%|████▉     | 8332/17030 [27:08<18:55,  7.66pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 93 - Current best internal CV score: 0.000502662287490285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  49%|████▉     | 8412/17030 [27:20<18:38,  7.71pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 94 - Current best internal CV score: 0.000502662287490285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  50%|████▉     | 8500/17030 [27:40<13:09, 10.80pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 95 - Current best internal CV score: 0.000502662287490285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  50%|█████     | 8595/17030 [27:58<14:55,  9.42pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 96 - Current best internal CV score: 0.0005024791704461172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  51%|█████     | 8686/17030 [28:17<23:17,  5.97pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 97 - Current best internal CV score: 0.0005024791704461172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  52%|█████▏    | 8772/17030 [28:37<18:32,  7.42pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 98 - Current best internal CV score: 0.0005024791704461172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  52%|█████▏    | 8868/17030 [28:52<18:30,  7.35pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 99 - Current best internal CV score: 0.0005008849387939929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  53%|█████▎    | 8943/17030 [29:05<22:30,  5.99pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 100 - Current best internal CV score: 0.0005008849387939929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  53%|█████▎    | 9027/17030 [29:22<19:16,  6.92pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 101 - Current best internal CV score: 0.0005008849387939929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  54%|█████▎    | 9117/17030 [29:43<13:30,  9.76pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 102 - Current best internal CV score: 0.0005008849387939929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  54%|█████▍    | 9204/17030 [29:56<16:00,  8.15pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 103 - Current best internal CV score: 0.0005008849387939929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  55%|█████▍    | 9292/17030 [30:13<13:29,  9.56pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 104 - Current best internal CV score: 0.0005008849387939929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  55%|█████▌    | 9389/17030 [30:28<14:38,  8.69pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 105 - Current best internal CV score: 0.0005008849387939929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  56%|█████▌    | 9480/17030 [30:48<14:40,  8.57pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 106 - Current best internal CV score: 0.0005008849387939929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  56%|█████▌    | 9572/17030 [31:07<14:04,  8.83pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 107 - Current best internal CV score: 0.0005008849387939929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  57%|█████▋    | 9659/17030 [31:25<10:53, 11.28pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 108 - Current best internal CV score: 0.0005008849387939929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  57%|█████▋    | 9741/17030 [31:42<12:57,  9.37pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 109 - Current best internal CV score: 0.0005008849387939929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  58%|█████▊    | 9821/17030 [31:53<14:20,  8.38pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 110 - Current best internal CV score: 0.0005008849387939929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  58%|█████▊    | 9909/17030 [32:10<15:42,  7.55pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 111 - Current best internal CV score: 0.0005008849387939929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  59%|█████▊    | 9997/17030 [32:30<12:25,  9.44pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 112 - Current best internal CV score: 0.0005003625645318897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  59%|█████▉    | 10083/17030 [32:47<21:45,  5.32pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 113 - Current best internal CV score: 0.0005003625645318897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  60%|█████▉    | 10168/17030 [33:07<16:11,  7.06pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 114 - Current best internal CV score: 0.0005003625645318897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  60%|██████    | 10257/17030 [33:23<20:55,  5.40pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 115 - Current best internal CV score: 0.0005003625645318897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  61%|██████    | 10343/17030 [33:42<13:10,  8.46pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 116 - Current best internal CV score: 0.0005003625645318897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  61%|██████▏   | 10432/17030 [34:03<13:37,  8.07pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 117 - Current best internal CV score: 0.0005003625645318897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  62%|██████▏   | 10525/17030 [34:20<16:34,  6.54pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 118 - Current best internal CV score: 0.0005003625645318897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  62%|██████▏   | 10617/17030 [34:41<17:27,  6.12pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 119 - Current best internal CV score: 0.0005003625645318897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  63%|██████▎   | 10701/17030 [34:59<12:33,  8.40pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 120 - Current best internal CV score: 0.0005003625645318897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  63%|██████▎   | 10785/17030 [35:15<09:25, 11.04pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 121 - Current best internal CV score: 0.0005003625645318897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  64%|██████▍   | 10865/17030 [35:31<15:58,  6.43pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 122 - Current best internal CV score: 0.0005003625645318897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  64%|██████▍   | 10960/17030 [35:51<15:42,  6.44pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 123 - Current best internal CV score: 0.0005003625645318897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  65%|██████▍   | 11042/17030 [36:09<14:07,  7.07pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 124 - Current best internal CV score: 0.0005003625645318897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  65%|██████▌   | 11122/17030 [36:25<14:35,  6.75pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 125 - Current best internal CV score: 0.0005003625645318897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  66%|██████▌   | 11214/17030 [36:41<13:05,  7.41pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 126 - Current best internal CV score: 0.0005003625645318897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  66%|██████▋   | 11295/17030 [36:54<07:20, 13.01pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 127 - Current best internal CV score: 0.0005003625645318897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  67%|██████▋   | 11386/17030 [37:12<10:02,  9.37pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 128 - Current best internal CV score: 0.0005003625645318897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  67%|██████▋   | 11476/17030 [37:28<12:57,  7.14pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 129 - Current best internal CV score: 0.0004952805053835381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 130 - Current best internal CV score: 0.0004952805053835381\n",
      "\n",
      "Best pipeline: RidgeCV(SelectPercentile(GradientBoostingRegressor(RidgeCV(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.8)), GradientBoostingRegressor__alpha=DEFAULT, GradientBoostingRegressor__learning_rate=0.001, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=DEFAULT, GradientBoostingRegressor__max_features=0.4, GradientBoostingRegressor__min_samples_leaf=20, GradientBoostingRegressor__min_samples_split=5, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.4), SelectPercentile__percentile=75))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [0] are constant.\n",
      "  UserWarning)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py:109: RuntimeWarning: invalid value encountered in true_divide\n",
      "  msw = sswn / float(dfwn)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py:109: RuntimeWarning: invalid value encountered in true_divide\n",
      "  msw = sswn / float(dfwn)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py:109: RuntimeWarning: invalid value encountered in true_divide\n",
      "  msw = sswn / float(dfwn)\n",
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intracellular Mevalonate (uM)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAFlCAYAAADComBzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nvIUQQgiN+Q/LTfG1x7WN7gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x115e82c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limonene g/L\n",
      "Warning: xgboost.XGBRegressor is not available and will not be used by TPOT.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:   1%|▏         | 229/17030 [00:47<36:49,  7.60pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1 - Current best internal CV score: 3.9582644351301124e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:   2%|▏         | 345/17030 [01:20<43:16,  6.43pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 2 - Current best internal CV score: 3.9582644351301124e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:   3%|▎         | 468/17030 [02:09<34:10,  8.08pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 3 - Current best internal CV score: 3.6431325054475387e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:   3%|▎         | 585/17030 [02:46<55:49,  4.91pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 4 - Current best internal CV score: 3.6431325054475387e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:   4%|▍         | 701/17030 [03:36<54:31,  4.99pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 5 - Current best internal CV score: 3.6431325054475387e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:   5%|▍         | 821/17030 [04:33<52:39,  5.13pipeline/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 6 - Current best internal CV score: 3.2400341376483317e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:   6%|▌         | 940/17030 [05:13<1:20:50,  3.32pipeline/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 7 - Current best internal CV score: 3.2400341376483317e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:   6%|▌         | 1063/17030 [05:53<47:28,  5.61pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 8 - Current best internal CV score: 3.2400341376483317e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:   7%|▋         | 1191/17030 [06:25<45:08,  5.85pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 9 - Current best internal CV score: 3.2400341376483317e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:   8%|▊         | 1311/17030 [07:08<50:41,  5.17pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 10 - Current best internal CV score: 3.2400341376483317e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:   8%|▊         | 1431/17030 [07:49<1:01:32,  4.22pipeline/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 11 - Current best internal CV score: 3.2400341376483317e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:   9%|▉         | 1552/17030 [08:38<41:21,  6.24pipeline/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 12 - Current best internal CV score: 3.2400341376483317e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best pipeline: ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=DEFAULT, ExtraTreesRegressor__max_features=0.75, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=16, ExtraTreesRegressor__n_estimators=DEFAULT)\n",
      "Limonene gL\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAFlCAYAAADComBzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABs5v8BuNuDJyUxLA4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x107cfc320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mev-P (uM)\n",
      "Warning: xgboost.XGBRegressor is not available and will not be used by TPOT.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:   1%|▏         | 232/17030 [00:32<31:56,  8.76pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1 - Current best internal CV score: 0.023696674862724594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:   2%|▏         | 344/17030 [01:03<35:47,  7.77pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 2 - Current best internal CV score: 0.023696674862724594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:   3%|▎         | 463/17030 [01:53<1:25:20,  3.24pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 3 - Current best internal CV score: 0.023696674862724594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:   3%|▎         | 583/17030 [02:58<1:04:58,  4.22pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 4 - Current best internal CV score: 0.021213521807359775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:   4%|▍         | 712/17030 [03:56<53:51,  5.05pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 5 - Current best internal CV score: 0.021213521807359775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:   5%|▍         | 838/17030 [05:19<1:22:27,  3.27pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 6 - Current best internal CV score: 0.019904462321667708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:   6%|▌         | 961/17030 [06:35<1:14:26,  3.60pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 7 - Current best internal CV score: 0.019904462321667708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:   6%|▋         | 1084/17030 [07:41<49:27,  5.37pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 8 - Current best internal CV score: 0.019904462321667708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:   7%|▋         | 1205/17030 [08:44<1:05:14,  4.04pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 9 - Current best internal CV score: 0.019904462321667708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:   8%|▊         | 1325/17030 [09:23<45:12,  5.79pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 10 - Current best internal CV score: 0.019904462321667708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:   8%|▊         | 1447/17030 [10:29<58:18,  4.45pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 11 - Current best internal CV score: 0.019904462321667708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:   9%|▉         | 1572/17030 [11:24<49:20,  5.22pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 12 - Current best internal CV score: 0.019904462321667708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  10%|▉         | 1691/17030 [12:10<54:13,  4.72pipeline/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 13 - Current best internal CV score: 0.019904462321667708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  11%|█         | 1813/17030 [12:48<52:57,  4.79pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 14 - Current best internal CV score: 0.019904462321667708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  11%|█▏        | 1932/17030 [13:17<39:57,  6.30pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 15 - Current best internal CV score: 0.019904462321667708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  12%|█▏        | 2055/17030 [13:48<42:53,  5.82pipeline/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 16 - Current best internal CV score: 0.019904462321667708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  13%|█▎        | 2176/17030 [14:18<49:38,  4.99pipeline/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 17 - Current best internal CV score: 0.019859333980125824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  13%|█▎        | 2294/17030 [14:45<36:29,  6.73pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 18 - Current best internal CV score: 0.019859333980125824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  14%|█▍        | 2419/17030 [15:16<41:13,  5.91pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 19 - Current best internal CV score: 0.01983957776918317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  15%|█▍        | 2541/17030 [15:46<45:13,  5.34pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 20 - Current best internal CV score: 0.019829184993509663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  16%|█▌        | 2665/17030 [16:18<43:48,  5.47pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 21 - Current best internal CV score: 0.019829184993509663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  16%|█▋        | 2791/17030 [16:50<38:01,  6.24pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 22 - Current best internal CV score: 0.019829184993509663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  17%|█▋        | 2919/17030 [17:22<42:15,  5.57pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 23 - Current best internal CV score: 0.019829184993509663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  18%|█▊        | 3043/17030 [17:54<40:39,  5.73pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 24 - Current best internal CV score: 0.019829184993509663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  19%|█▊        | 3166/17030 [18:20<31:12,  7.40pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 25 - Current best internal CV score: 0.019318506038698603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  19%|█▉        | 3287/17030 [18:50<45:38,  5.02pipeline/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 26 - Current best internal CV score: 0.019318506038698603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  20%|█▉        | 3402/17030 [19:17<29:59,  7.57pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 27 - Current best internal CV score: 0.019318506038698603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  21%|██        | 3518/17030 [19:45<35:46,  6.29pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 28 - Current best internal CV score: 0.019318506038698603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  21%|██▏       | 3642/17030 [20:13<30:40,  7.28pipeline/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 29 - Current best internal CV score: 0.019318506038698603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  22%|██▏       | 3768/17030 [20:41<27:21,  8.08pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 30 - Current best internal CV score: 0.019318506038698603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  23%|██▎       | 3893/17030 [21:09<32:55,  6.65pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 31 - Current best internal CV score: 0.019318506038698603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  24%|██▎       | 4015/17030 [21:37<31:49,  6.82pipeline/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 32 - Current best internal CV score: 0.019318506038698603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  24%|██▍       | 4136/17030 [22:07<33:40,  6.38pipeline/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 33 - Current best internal CV score: 0.019318506038698603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  25%|██▍       | 4256/17030 [22:31<37:50,  5.63pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 34 - Current best internal CV score: 0.019318506038698603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  26%|██▌       | 4375/17030 [22:57<33:10,  6.36pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 35 - Current best internal CV score: 0.019318506038698603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  26%|██▋       | 4493/17030 [23:24<37:14,  5.61pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 36 - Current best internal CV score: 0.019318506038698603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  27%|██▋       | 4609/17030 [23:53<35:12,  5.88pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 37 - Current best internal CV score: 0.019318506038698603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  28%|██▊       | 4728/17030 [24:19<30:23,  6.75pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 38 - Current best internal CV score: 0.019318506038698603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  28%|██▊       | 4843/17030 [24:44<33:57,  5.98pipeline/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 39 - Current best internal CV score: 0.019318506038698603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  29%|██▉       | 4961/17030 [25:16<34:42,  5.80pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 40 - Current best internal CV score: 0.019318506038698603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  30%|██▉       | 5081/17030 [25:41<28:25,  7.01pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 41 - Current best internal CV score: 0.019318506038698603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  30%|███       | 5193/17030 [26:11<36:20,  5.43pipeline/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 42 - Current best internal CV score: 0.019318506038698603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  31%|███       | 5308/17030 [26:36<35:48,  5.46pipeline/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 43 - Current best internal CV score: 0.019318506038698603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  32%|███▏      | 5431/17030 [27:00<27:37,  7.00pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 44 - Current best internal CV score: 0.019318506038698603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  33%|███▎      | 5548/17030 [27:28<35:32,  5.38pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 45 - Current best internal CV score: 0.019318506038698603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  33%|███▎      | 5661/17030 [27:52<36:26,  5.20pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 46 - Current best internal CV score: 0.019318506038698603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  34%|███▍      | 5782/17030 [28:19<37:00,  5.06pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 47 - Current best internal CV score: 0.019318506038698603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  35%|███▍      | 5900/17030 [28:43<22:03,  8.41pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 48 - Current best internal CV score: 0.019318506038698603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  35%|███▌      | 6015/17030 [29:08<33:26,  5.49pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 49 - Current best internal CV score: 0.019318506038698603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  36%|███▌      | 6134/17030 [29:29<27:21,  6.64pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 50 - Current best internal CV score: 0.019318506038698603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  37%|███▋      | 6254/17030 [29:52<22:37,  7.94pipeline/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 51 - Current best internal CV score: 0.019318506038698603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  37%|███▋      | 6374/17030 [30:15<21:22,  8.31pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 52 - Current best internal CV score: 0.019318506038698603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  38%|███▊      | 6493/17030 [30:44<31:23,  5.59pipeline/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 53 - Current best internal CV score: 0.019318506038698603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  39%|███▉      | 6601/17030 [31:08<29:26,  5.90pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 54 - Current best internal CV score: 0.019318506038698603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  39%|███▉      | 6716/17030 [31:35<27:33,  6.24pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 55 - Current best internal CV score: 0.019318506038698603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  40%|████      | 6838/17030 [31:57<26:50,  6.33pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 56 - Current best internal CV score: 0.019318506038698603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  41%|████      | 6956/17030 [32:20<33:36,  5.00pipeline/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 57 - Current best internal CV score: 0.019318506038698603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  41%|████▏     | 7065/17030 [32:41<31:17,  5.31pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 58 - Current best internal CV score: 0.019318506038698603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  42%|████▏     | 7183/17030 [33:04<23:09,  7.09pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 59 - Current best internal CV score: 0.019318506038698603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  43%|████▎     | 7296/17030 [33:25<29:26,  5.51pipeline/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 60 - Current best internal CV score: 0.019318506038698603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  43%|████▎     | 7407/17030 [33:49<35:30,  4.52pipeline/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 61 - Current best internal CV score: 0.019279419402718663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  44%|████▍     | 7521/17030 [34:16<28:55,  5.48pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 62 - Current best internal CV score: 0.019279419402718663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  45%|████▍     | 7643/17030 [34:39<27:14,  5.74pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 63 - Current best internal CV score: 0.019279419402718663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  46%|████▌     | 7764/17030 [35:03<26:03,  5.93pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 64 - Current best internal CV score: 0.019279419402718663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  46%|████▋     | 7883/17030 [35:24<29:46,  5.12pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 65 - Current best internal CV score: 0.019279419402718663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  47%|████▋     | 8003/17030 [35:49<27:15,  5.52pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 66 - Current best internal CV score: 0.019279419402718663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  48%|████▊     | 8118/17030 [36:10<26:11,  5.67pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 67 - Current best internal CV score: 0.019279419402718663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  48%|████▊     | 8241/17030 [36:29<21:46,  6.73pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 68 - Current best internal CV score: 0.019279419402718663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  49%|████▉     | 8361/17030 [36:50<32:10,  4.49pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 69 - Current best internal CV score: 0.019279419402718663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  50%|████▉     | 8472/17030 [37:14<31:47,  4.49pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 70 - Current best internal CV score: 0.019279419402718663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  50%|█████     | 8584/17030 [37:39<28:50,  4.88pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 71 - Current best internal CV score: 0.019279419402718663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  51%|█████     | 8705/17030 [37:59<29:45,  4.66pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 72 - Current best internal CV score: 0.019279419402718663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  52%|█████▏    | 8816/17030 [38:23<29:37,  4.62pipeline/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 73 - Current best internal CV score: 0.019279419402718663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  52%|█████▏    | 8935/17030 [38:46<33:49,  3.99pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 74 - Current best internal CV score: 0.019279419402718663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  53%|█████▎    | 9055/17030 [39:06<30:34,  4.35pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 75 - Current best internal CV score: 0.019279419402718663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  54%|█████▍    | 9174/17030 [39:27<31:52,  4.11pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 76 - Current best internal CV score: 0.019279419402718663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  55%|█████▍    | 9291/17030 [39:53<26:42,  4.83pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 77 - Current best internal CV score: 0.019279419402718663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  55%|█████▌    | 9405/17030 [40:13<28:54,  4.39pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 78 - Current best internal CV score: 0.019279419402718663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  56%|█████▌    | 9518/17030 [40:34<29:53,  4.19pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 79 - Current best internal CV score: 0.019279419402718663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  57%|█████▋    | 9641/17030 [41:06<26:47,  4.60pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 80 - Current best internal CV score: 0.019279419402718663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  57%|█████▋    | 9757/17030 [41:27<24:45,  4.90pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 81 - Current best internal CV score: 0.019279419402718663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  58%|█████▊    | 9877/17030 [41:46<26:31,  4.50pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 82 - Current best internal CV score: 0.019279419402718663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  59%|█████▊    | 9993/17030 [42:05<22:11,  5.28pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 83 - Current best internal CV score: 0.019279419402718663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  59%|█████▉    | 10117/17030 [42:24<28:04,  4.10pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 84 - Current best internal CV score: 0.019279419402718663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  60%|██████    | 10237/17030 [42:50<18:52,  6.00pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 85 - Current best internal CV score: 0.019279419402718663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  61%|██████    | 10352/17030 [43:11<45:11,  2.46pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 86 - Current best internal CV score: 0.019279419402718663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  61%|██████▏   | 10469/17030 [43:31<30:06,  3.63pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 87 - Current best internal CV score: 0.019279419402718663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  62%|██████▏   | 10587/17030 [43:51<22:38,  4.74pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 88 - Current best internal CV score: 0.019279419402718663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  63%|██████▎   | 10703/17030 [44:12<24:01,  4.39pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 89 - Current best internal CV score: 0.019279419402718663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  64%|██████▎   | 10823/17030 [44:34<23:35,  4.39pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 90 - Current best internal CV score: 0.019279419402718663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  64%|██████▍   | 10947/17030 [44:54<33:09,  3.06pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 91 - Current best internal CV score: 0.019279419402718663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  65%|██████▍   | 11064/17030 [45:19<19:01,  5.23pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 92 - Current best internal CV score: 0.019279419402718663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  66%|██████▌   | 11183/17030 [45:37<33:40,  2.89pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 93 - Current best internal CV score: 0.019279419402718663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  66%|██████▋   | 11298/17030 [45:57<20:36,  4.63pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 94 - Current best internal CV score: 0.019279419402718663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  67%|██████▋   | 11421/17030 [46:15<21:33,  4.34pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 95 - Current best internal CV score: 0.019279419402718663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  68%|██████▊   | 11540/17030 [46:34<18:53,  4.84pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 96 - Current best internal CV score: 0.019279419402718663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  68%|██████▊   | 11663/17030 [46:57<19:09,  4.67pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 97 - Current best internal CV score: 0.019279419402718663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  69%|██████▉   | 11785/17030 [47:22<24:59,  3.50pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 98 - Current best internal CV score: 0.019279419402718663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  70%|██████▉   | 11909/17030 [47:36<15:51,  5.38pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 99 - Current best internal CV score: 0.019279419402718663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  71%|███████   | 12026/17030 [47:55<24:04,  3.46pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 100 - Current best internal CV score: 0.019279419402718663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  71%|███████▏  | 12141/17030 [48:13<16:40,  4.89pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 101 - Current best internal CV score: 0.019279419402718663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  72%|███████▏  | 12254/17030 [48:32<25:05,  3.17pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 102 - Current best internal CV score: 0.019279419402718663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  73%|███████▎  | 12375/17030 [48:54<22:59,  3.37pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 103 - Current best internal CV score: 0.019279419402718663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  73%|███████▎  | 12488/17030 [49:16<19:50,  3.82pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 104 - Current best internal CV score: 0.019279419402718663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  74%|███████▍  | 12608/17030 [49:40<16:16,  4.53pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 105 - Current best internal CV score: 0.019279419402718663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  75%|███████▍  | 12724/17030 [50:00<11:55,  6.02pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 106 - Current best internal CV score: 0.019279419402718663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  75%|███████▌  | 12846/17030 [50:25<24:58,  2.79pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 107 - Current best internal CV score: 0.019279419402718663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  76%|███████▌  | 12959/17030 [50:44<13:04,  5.19pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 108 - Current best internal CV score: 0.019279419402718663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  77%|███████▋  | 13077/17030 [51:03<17:35,  3.74pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 109 - Current best internal CV score: 0.019279419402718663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  77%|███████▋  | 13192/17030 [51:23<13:29,  4.74pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 110 - Current best internal CV score: 0.019279419402718663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  78%|███████▊  | 13317/17030 [51:45<12:43,  4.86pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 111 - Current best internal CV score: 0.019279419402718663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  79%|███████▉  | 13437/17030 [52:08<14:47,  4.05pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 112 - Current best internal CV score: 0.019279419402718663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  80%|███████▉  | 13560/17030 [52:27<13:59,  4.13pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 113 - Current best internal CV score: 0.019279419402718663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  80%|████████  | 13674/17030 [52:43<14:22,  3.89pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 114 - Current best internal CV score: 0.019279419402718663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  81%|████████  | 13794/17030 [53:03<15:06,  3.57pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 115 - Current best internal CV score: 0.019279419402718663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  82%|████████▏ | 13912/17030 [53:27<14:44,  3.52pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 116 - Current best internal CV score: 0.019279419402718663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  82%|████████▏ | 14034/17030 [53:44<11:44,  4.25pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 117 - Current best internal CV score: 0.019279419402718663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  83%|████████▎ | 14155/17030 [54:04<13:25,  3.57pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 118 - Current best internal CV score: 0.019279419402718663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  84%|████████▍ | 14275/17030 [54:21<12:10,  3.77pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 119 - Current best internal CV score: 0.019279419402718663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  84%|████████▍ | 14388/17030 [54:39<13:05,  3.36pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 120 - Current best internal CV score: 0.019279419402718663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  85%|████████▌ | 14510/17030 [54:57<09:59,  4.20pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 121 - Current best internal CV score: 0.019279419402718663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  86%|████████▌ | 14625/17030 [55:16<09:27,  4.24pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 122 - Current best internal CV score: 0.019279419402718663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  87%|████████▋ | 14750/17030 [55:36<10:59,  3.46pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 123 - Current best internal CV score: 0.019279419402718663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  87%|████████▋ | 14868/17030 [55:55<14:59,  2.40pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 124 - Current best internal CV score: 0.019279419402718663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  88%|████████▊ | 14984/17030 [56:18<11:35,  2.94pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 125 - Current best internal CV score: 0.019279419402718663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  89%|████████▊ | 15098/17030 [56:45<10:40,  3.02pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 126 - Current best internal CV score: 0.019279419402718663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  89%|████████▉ | 15216/17030 [57:03<07:46,  3.89pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 127 - Current best internal CV score: 0.019279419402718663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  90%|█████████ | 15328/17030 [57:23<11:45,  2.41pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 128 - Current best internal CV score: 0.019279419402718663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  91%|█████████ | 15446/17030 [57:45<07:13,  3.65pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 129 - Current best internal CV score: 0.019279419402718663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 130 - Current best internal CV score: 0.019279419402718663\n",
      "\n",
      "Best pipeline: GradientBoostingRegressor(StandardScaler(input_matrix), GradientBoostingRegressor__alpha=0.95, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=lad, GradientBoostingRegressor__max_depth=9, GradientBoostingRegressor__max_features=0.75, GradientBoostingRegressor__min_samples_leaf=3, GradientBoostingRegressor__min_samples_split=20, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.45)\n",
      "Mev-P (uM)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAFlCAYAAADComBzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n8BZCCCHCzP8DsapBDlCbqxoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10b794390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Score: 0.488308177003 n: 6\n"
     ]
    }
   ],
   "source": [
    "#1. Calculate Learning Curves for Derivatives\n",
    "\n",
    "#Pick number of strains so that the total number is equal to max tested strains...\n",
    "if data_type == 'simulated':\n",
    "    strains = tsdf.index.get_level_values(0).unique()\n",
    "    tsdf_max_strains = tsdf.loc[slice(strains[0],strains[max(strain_numbers)]),slice(None)]\n",
    "else:\n",
    "    tsdf_max_strains = tsdf\n",
    "\n",
    "average_training_score = 0\n",
    "n = 0\n",
    "for target_index in tsdf_max_strains.columns:\n",
    "    if target_index[0] == 'feature':\n",
    "        continue\n",
    "    n += 1\n",
    "    target = target_index[1]\n",
    "    print(target)\n",
    "\n",
    "\n",
    "    feature_indecies = [('feature', feature) for feature in specific_features[target]]\n",
    "    X = tsdf_max_strains[feature_indecies].values.tolist()\n",
    "    y = tsdf_max_strains[target_index].values.tolist()\n",
    "\n",
    "    #if fit_log_targets:\n",
    "    #    y = [math.log(val) for val in y]\n",
    "    \n",
    "    #print(X)\n",
    "    #print(y)\n",
    "    if model_str == 'tpot':\n",
    "        modelDict[target] = clone(mlmodel).fit(np.array(X),np.array(y)).fitted_pipeline_\n",
    "        try:\n",
    "            crossValPlot = plot_learning_curve(modelDict[target],target,X,y,cv=ShuffleSplit())\n",
    "        except:\n",
    "            pass\n",
    "    else:\n",
    "        modelDict[target] = clone(mlmodel)\n",
    "        crossValPlot = plot_learning_curve(modelDict[target],target,X,y,cv=ShuffleSplit())\n",
    "    \n",
    "    ax = plt.gca()\n",
    "    ax.set_ylim([-0.1, 1.1])\n",
    "    strip_target = ''.join([char for char in target if char != '/'])\n",
    "    print(strip_target)\n",
    "    crossValPlot.savefig('figures/' + strip_target + 'CrossValPlot.pdf',transparent=False)\n",
    "    plt.show()\n",
    "    \n",
    "    #Save out Cross Validation Plot\n",
    "    \n",
    "    modelDict[target] = modelDict[target].fit(X,y)\n",
    "    average_training_score +=  modelDict[target].score(X,y)\n",
    "    #print(modelDict[target_name].predict([reduced_features[2]]))\n",
    "    \n",
    "average_training_score /= n\n",
    "print('Average Training Score:',average_training_score,'n:',n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Time Series in Data Set:  1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"15\" halign=\"left\">feature</th>\n",
       "      <th colspan=\"6\" halign=\"left\">target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>ATP (uM)</th>\n",
       "      <th>Acetate g/L</th>\n",
       "      <th>Acetyl-CoA (uM)</th>\n",
       "      <th>AtoB</th>\n",
       "      <th>GPP (uM)</th>\n",
       "      <th>HMG-CoA (uM)</th>\n",
       "      <th>HMGR</th>\n",
       "      <th>HMGS</th>\n",
       "      <th>IPP/DMAPP (uM)</th>\n",
       "      <th>Intracellular Mevalonate (uM)</th>\n",
       "      <th>...</th>\n",
       "      <th>PMD</th>\n",
       "      <th>PMK</th>\n",
       "      <th>Pyruvate g/L</th>\n",
       "      <th>citrate (uM)</th>\n",
       "      <th>Acetyl-CoA (uM)</th>\n",
       "      <th>HMG-CoA (uM)</th>\n",
       "      <th>IPP/DMAPP (uM)</th>\n",
       "      <th>Intracellular Mevalonate (uM)</th>\n",
       "      <th>Limonene g/L</th>\n",
       "      <th>Mev-P (uM)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Strain</th>\n",
       "      <th>Time (h)</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"61\" valign=\"top\">L2</th>\n",
       "      <th>0.000000</th>\n",
       "      <td>0.302952</td>\n",
       "      <td>0.303717</td>\n",
       "      <td>0.286359</td>\n",
       "      <td>1.249415e+06</td>\n",
       "      <td>0.008534</td>\n",
       "      <td>0.007628</td>\n",
       "      <td>9.586787e+05</td>\n",
       "      <td>1.806693e+06</td>\n",
       "      <td>0.174187</td>\n",
       "      <td>0.880607</td>\n",
       "      <td>...</td>\n",
       "      <td>2.318537e+05</td>\n",
       "      <td>3.571606e+04</td>\n",
       "      <td>3.834561e-02</td>\n",
       "      <td>0.021561</td>\n",
       "      <td>0.014010</td>\n",
       "      <td>0.008557</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>0.471134</td>\n",
       "      <td>0.000579</td>\n",
       "      <td>0.009681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.727273</th>\n",
       "      <td>0.288104</td>\n",
       "      <td>0.412069</td>\n",
       "      <td>0.293084</td>\n",
       "      <td>3.175036e+06</td>\n",
       "      <td>0.015908</td>\n",
       "      <td>0.011736</td>\n",
       "      <td>1.817455e+06</td>\n",
       "      <td>3.830152e+06</td>\n",
       "      <td>0.174457</td>\n",
       "      <td>1.219823</td>\n",
       "      <td>...</td>\n",
       "      <td>3.794901e+05</td>\n",
       "      <td>1.756340e+05</td>\n",
       "      <td>3.151425e-02</td>\n",
       "      <td>0.021772</td>\n",
       "      <td>0.014010</td>\n",
       "      <td>0.008557</td>\n",
       "      <td>0.000976</td>\n",
       "      <td>0.462692</td>\n",
       "      <td>0.000599</td>\n",
       "      <td>0.009552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.454545</th>\n",
       "      <td>0.273255</td>\n",
       "      <td>0.494135</td>\n",
       "      <td>0.299808</td>\n",
       "      <td>5.082022e+06</td>\n",
       "      <td>0.023576</td>\n",
       "      <td>0.015843</td>\n",
       "      <td>2.659607e+06</td>\n",
       "      <td>5.845767e+06</td>\n",
       "      <td>0.175593</td>\n",
       "      <td>1.546884</td>\n",
       "      <td>...</td>\n",
       "      <td>5.368942e+05</td>\n",
       "      <td>3.123969e+05</td>\n",
       "      <td>2.792344e-02</td>\n",
       "      <td>0.021983</td>\n",
       "      <td>0.014010</td>\n",
       "      <td>0.008557</td>\n",
       "      <td>0.002179</td>\n",
       "      <td>0.445809</td>\n",
       "      <td>0.000640</td>\n",
       "      <td>0.009294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.181818</th>\n",
       "      <td>0.258406</td>\n",
       "      <td>0.549913</td>\n",
       "      <td>0.306533</td>\n",
       "      <td>6.970373e+06</td>\n",
       "      <td>0.031540</td>\n",
       "      <td>0.019950</td>\n",
       "      <td>3.485134e+06</td>\n",
       "      <td>7.853538e+06</td>\n",
       "      <td>0.177594</td>\n",
       "      <td>1.861788</td>\n",
       "      <td>...</td>\n",
       "      <td>7.040660e+05</td>\n",
       "      <td>4.460049e+05</td>\n",
       "      <td>2.757316e-02</td>\n",
       "      <td>0.022194</td>\n",
       "      <td>0.014010</td>\n",
       "      <td>0.008557</td>\n",
       "      <td>0.002299</td>\n",
       "      <td>0.444121</td>\n",
       "      <td>0.000645</td>\n",
       "      <td>0.009268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.909091</th>\n",
       "      <td>0.243558</td>\n",
       "      <td>0.581022</td>\n",
       "      <td>0.313258</td>\n",
       "      <td>8.873632e+06</td>\n",
       "      <td>0.039267</td>\n",
       "      <td>0.024057</td>\n",
       "      <td>4.323961e+06</td>\n",
       "      <td>9.867584e+06</td>\n",
       "      <td>0.178903</td>\n",
       "      <td>2.186418</td>\n",
       "      <td>...</td>\n",
       "      <td>8.634237e+05</td>\n",
       "      <td>5.821369e+05</td>\n",
       "      <td>3.436354e-02</td>\n",
       "      <td>0.022404</td>\n",
       "      <td>0.014010</td>\n",
       "      <td>0.008557</td>\n",
       "      <td>0.003982</td>\n",
       "      <td>0.420485</td>\n",
       "      <td>0.000702</td>\n",
       "      <td>0.008907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.636364</th>\n",
       "      <td>0.228709</td>\n",
       "      <td>0.579706</td>\n",
       "      <td>0.319983</td>\n",
       "      <td>1.070980e+07</td>\n",
       "      <td>0.048057</td>\n",
       "      <td>0.028164</td>\n",
       "      <td>5.102941e+06</td>\n",
       "      <td>1.185339e+07</td>\n",
       "      <td>0.183329</td>\n",
       "      <td>2.467286</td>\n",
       "      <td>...</td>\n",
       "      <td>1.057945e+06</td>\n",
       "      <td>7.069111e+05</td>\n",
       "      <td>3.957085e-02</td>\n",
       "      <td>0.022615</td>\n",
       "      <td>0.014974</td>\n",
       "      <td>0.008782</td>\n",
       "      <td>0.009273</td>\n",
       "      <td>0.346199</td>\n",
       "      <td>0.000884</td>\n",
       "      <td>0.007771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.363636</th>\n",
       "      <td>0.212950</td>\n",
       "      <td>0.555286</td>\n",
       "      <td>0.327633</td>\n",
       "      <td>1.244907e+07</td>\n",
       "      <td>0.058380</td>\n",
       "      <td>0.032488</td>\n",
       "      <td>5.795474e+06</td>\n",
       "      <td>1.379841e+07</td>\n",
       "      <td>0.192256</td>\n",
       "      <td>2.684945</td>\n",
       "      <td>...</td>\n",
       "      <td>1.303258e+06</td>\n",
       "      <td>8.152797e+05</td>\n",
       "      <td>3.623255e-02</td>\n",
       "      <td>0.022696</td>\n",
       "      <td>0.014814</td>\n",
       "      <td>0.008745</td>\n",
       "      <td>0.015525</td>\n",
       "      <td>0.258407</td>\n",
       "      <td>0.001098</td>\n",
       "      <td>0.006429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.090909</th>\n",
       "      <td>0.198253</td>\n",
       "      <td>0.520314</td>\n",
       "      <td>0.334204</td>\n",
       "      <td>1.409144e+07</td>\n",
       "      <td>0.070237</td>\n",
       "      <td>0.036559</td>\n",
       "      <td>6.401561e+06</td>\n",
       "      <td>1.570263e+07</td>\n",
       "      <td>0.205685</td>\n",
       "      <td>2.839393</td>\n",
       "      <td>...</td>\n",
       "      <td>1.599364e+06</td>\n",
       "      <td>9.072426e+05</td>\n",
       "      <td>2.428436e-02</td>\n",
       "      <td>0.022929</td>\n",
       "      <td>0.010312</td>\n",
       "      <td>0.007691</td>\n",
       "      <td>0.020816</td>\n",
       "      <td>0.184122</td>\n",
       "      <td>0.001280</td>\n",
       "      <td>0.005293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.818182</th>\n",
       "      <td>0.186741</td>\n",
       "      <td>0.473192</td>\n",
       "      <td>0.337533</td>\n",
       "      <td>1.566671e+07</td>\n",
       "      <td>0.083156</td>\n",
       "      <td>0.039872</td>\n",
       "      <td>6.947800e+06</td>\n",
       "      <td>1.757862e+07</td>\n",
       "      <td>0.222231</td>\n",
       "      <td>2.950080</td>\n",
       "      <td>...</td>\n",
       "      <td>1.930632e+06</td>\n",
       "      <td>9.878477e+05</td>\n",
       "      <td>1.064928e-02</td>\n",
       "      <td>0.023618</td>\n",
       "      <td>0.002594</td>\n",
       "      <td>0.005886</td>\n",
       "      <td>0.022547</td>\n",
       "      <td>0.148374</td>\n",
       "      <td>0.001332</td>\n",
       "      <td>0.004695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6.545455</th>\n",
       "      <td>0.179324</td>\n",
       "      <td>0.422059</td>\n",
       "      <td>0.336694</td>\n",
       "      <td>1.738527e+07</td>\n",
       "      <td>0.097970</td>\n",
       "      <td>0.042210</td>\n",
       "      <td>7.551284e+06</td>\n",
       "      <td>1.965121e+07</td>\n",
       "      <td>0.238153</td>\n",
       "      <td>3.053051</td>\n",
       "      <td>...</td>\n",
       "      <td>2.295358e+06</td>\n",
       "      <td>1.066046e+06</td>\n",
       "      <td>2.039587e-03</td>\n",
       "      <td>0.024896</td>\n",
       "      <td>-0.005766</td>\n",
       "      <td>0.003930</td>\n",
       "      <td>0.022643</td>\n",
       "      <td>0.152741</td>\n",
       "      <td>0.001339</td>\n",
       "      <td>0.004788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7.272727</th>\n",
       "      <td>0.175698</td>\n",
       "      <td>0.374880</td>\n",
       "      <td>0.331997</td>\n",
       "      <td>1.889264e+07</td>\n",
       "      <td>0.109882</td>\n",
       "      <td>0.043645</td>\n",
       "      <td>8.072226e+06</td>\n",
       "      <td>2.143046e+07</td>\n",
       "      <td>0.254837</td>\n",
       "      <td>3.170027</td>\n",
       "      <td>...</td>\n",
       "      <td>2.607945e+06</td>\n",
       "      <td>1.148486e+06</td>\n",
       "      <td>-5.176000e-05</td>\n",
       "      <td>0.026718</td>\n",
       "      <td>-0.012198</td>\n",
       "      <td>0.002426</td>\n",
       "      <td>0.025193</td>\n",
       "      <td>0.168413</td>\n",
       "      <td>0.001460</td>\n",
       "      <td>0.005261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8.000000</th>\n",
       "      <td>0.174348</td>\n",
       "      <td>0.331686</td>\n",
       "      <td>0.324985</td>\n",
       "      <td>1.994779e+07</td>\n",
       "      <td>0.116462</td>\n",
       "      <td>0.044538</td>\n",
       "      <td>8.401481e+06</td>\n",
       "      <td>2.261051e+07</td>\n",
       "      <td>0.274430</td>\n",
       "      <td>3.295566</td>\n",
       "      <td>...</td>\n",
       "      <td>2.831884e+06</td>\n",
       "      <td>1.234359e+06</td>\n",
       "      <td>8.484848e-04</td>\n",
       "      <td>0.028866</td>\n",
       "      <td>-0.014127</td>\n",
       "      <td>0.001974</td>\n",
       "      <td>0.029570</td>\n",
       "      <td>0.181296</td>\n",
       "      <td>0.001659</td>\n",
       "      <td>0.005796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8.727273</th>\n",
       "      <td>0.172542</td>\n",
       "      <td>0.293871</td>\n",
       "      <td>0.318435</td>\n",
       "      <td>2.047537e+07</td>\n",
       "      <td>0.116822</td>\n",
       "      <td>0.045540</td>\n",
       "      <td>8.507104e+06</td>\n",
       "      <td>2.309147e+07</td>\n",
       "      <td>0.297417</td>\n",
       "      <td>3.431093</td>\n",
       "      <td>...</td>\n",
       "      <td>2.952402e+06</td>\n",
       "      <td>1.324238e+06</td>\n",
       "      <td>9.419966e-04</td>\n",
       "      <td>0.030949</td>\n",
       "      <td>-0.013323</td>\n",
       "      <td>0.002162</td>\n",
       "      <td>0.033947</td>\n",
       "      <td>0.194178</td>\n",
       "      <td>0.001857</td>\n",
       "      <td>0.006330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9.454545</th>\n",
       "      <td>0.170433</td>\n",
       "      <td>0.263639</td>\n",
       "      <td>0.312194</td>\n",
       "      <td>2.055074e+07</td>\n",
       "      <td>0.111849</td>\n",
       "      <td>0.046614</td>\n",
       "      <td>8.421040e+06</td>\n",
       "      <td>2.297323e+07</td>\n",
       "      <td>0.323313</td>\n",
       "      <td>3.575182</td>\n",
       "      <td>...</td>\n",
       "      <td>2.984271e+06</td>\n",
       "      <td>1.417551e+06</td>\n",
       "      <td>5.529986e-04</td>\n",
       "      <td>0.032988</td>\n",
       "      <td>-0.013001</td>\n",
       "      <td>0.002237</td>\n",
       "      <td>0.037609</td>\n",
       "      <td>0.210835</td>\n",
       "      <td>0.001988</td>\n",
       "      <td>0.006689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10.181818</th>\n",
       "      <td>0.168325</td>\n",
       "      <td>0.239559</td>\n",
       "      <td>0.305954</td>\n",
       "      <td>2.041544e+07</td>\n",
       "      <td>0.103509</td>\n",
       "      <td>0.047688</td>\n",
       "      <td>8.239992e+06</td>\n",
       "      <td>2.256417e+07</td>\n",
       "      <td>0.351574</td>\n",
       "      <td>3.734695</td>\n",
       "      <td>...</td>\n",
       "      <td>2.970914e+06</td>\n",
       "      <td>1.516186e+06</td>\n",
       "      <td>2.197742e-04</td>\n",
       "      <td>0.034514</td>\n",
       "      <td>-0.013001</td>\n",
       "      <td>0.002237</td>\n",
       "      <td>0.038262</td>\n",
       "      <td>0.215695</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>0.006724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10.909091</th>\n",
       "      <td>0.166216</td>\n",
       "      <td>0.218725</td>\n",
       "      <td>0.299713</td>\n",
       "      <td>2.042315e+07</td>\n",
       "      <td>0.097298</td>\n",
       "      <td>0.048762</td>\n",
       "      <td>8.122409e+06</td>\n",
       "      <td>2.235045e+07</td>\n",
       "      <td>0.378411</td>\n",
       "      <td>3.885783</td>\n",
       "      <td>...</td>\n",
       "      <td>2.987557e+06</td>\n",
       "      <td>1.611874e+06</td>\n",
       "      <td>7.105052e-05</td>\n",
       "      <td>0.036296</td>\n",
       "      <td>-0.013001</td>\n",
       "      <td>0.002237</td>\n",
       "      <td>0.034427</td>\n",
       "      <td>0.175022</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.006700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11.636364</th>\n",
       "      <td>0.164107</td>\n",
       "      <td>0.204083</td>\n",
       "      <td>0.293472</td>\n",
       "      <td>2.036141e+07</td>\n",
       "      <td>0.094243</td>\n",
       "      <td>0.049836</td>\n",
       "      <td>8.000967e+06</td>\n",
       "      <td>2.209716e+07</td>\n",
       "      <td>0.401150</td>\n",
       "      <td>3.986727</td>\n",
       "      <td>...</td>\n",
       "      <td>3.008266e+06</td>\n",
       "      <td>1.691341e+06</td>\n",
       "      <td>3.509543e-05</td>\n",
       "      <td>0.040390</td>\n",
       "      <td>-0.013001</td>\n",
       "      <td>0.002237</td>\n",
       "      <td>0.027472</td>\n",
       "      <td>0.089902</td>\n",
       "      <td>0.002071</td>\n",
       "      <td>0.006827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12.363636</th>\n",
       "      <td>0.161998</td>\n",
       "      <td>0.194408</td>\n",
       "      <td>0.287232</td>\n",
       "      <td>2.019934e+07</td>\n",
       "      <td>0.095749</td>\n",
       "      <td>0.050910</td>\n",
       "      <td>7.873949e+06</td>\n",
       "      <td>2.178669e+07</td>\n",
       "      <td>0.417970</td>\n",
       "      <td>4.015241</td>\n",
       "      <td>...</td>\n",
       "      <td>3.034849e+06</td>\n",
       "      <td>1.747380e+06</td>\n",
       "      <td>4.345195e-05</td>\n",
       "      <td>0.047821</td>\n",
       "      <td>-0.013001</td>\n",
       "      <td>0.002237</td>\n",
       "      <td>0.019252</td>\n",
       "      <td>-0.010695</td>\n",
       "      <td>0.002153</td>\n",
       "      <td>0.006977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13.090909</th>\n",
       "      <td>0.159889</td>\n",
       "      <td>0.187484</td>\n",
       "      <td>0.280991</td>\n",
       "      <td>1.993694e+07</td>\n",
       "      <td>0.101816</td>\n",
       "      <td>0.051984</td>\n",
       "      <td>7.741356e+06</td>\n",
       "      <td>2.141906e+07</td>\n",
       "      <td>0.428872</td>\n",
       "      <td>3.971326</td>\n",
       "      <td>...</td>\n",
       "      <td>3.067304e+06</td>\n",
       "      <td>1.779989e+06</td>\n",
       "      <td>3.005610e-05</td>\n",
       "      <td>0.058590</td>\n",
       "      <td>-0.013001</td>\n",
       "      <td>0.002237</td>\n",
       "      <td>0.012296</td>\n",
       "      <td>-0.095816</td>\n",
       "      <td>0.002223</td>\n",
       "      <td>0.007104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13.818182</th>\n",
       "      <td>0.157780</td>\n",
       "      <td>0.183172</td>\n",
       "      <td>0.274750</td>\n",
       "      <td>1.960509e+07</td>\n",
       "      <td>0.111040</td>\n",
       "      <td>0.053058</td>\n",
       "      <td>7.604902e+06</td>\n",
       "      <td>2.101185e+07</td>\n",
       "      <td>0.435676</td>\n",
       "      <td>3.877266</td>\n",
       "      <td>...</td>\n",
       "      <td>3.103826e+06</td>\n",
       "      <td>1.796379e+06</td>\n",
       "      <td>2.271013e-05</td>\n",
       "      <td>0.071670</td>\n",
       "      <td>-0.013001</td>\n",
       "      <td>0.002237</td>\n",
       "      <td>0.009134</td>\n",
       "      <td>-0.134507</td>\n",
       "      <td>0.002255</td>\n",
       "      <td>0.007162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14.545455</th>\n",
       "      <td>0.155671</td>\n",
       "      <td>0.178161</td>\n",
       "      <td>0.268510</td>\n",
       "      <td>1.926551e+07</td>\n",
       "      <td>0.120614</td>\n",
       "      <td>0.054132</td>\n",
       "      <td>7.468019e+06</td>\n",
       "      <td>2.060023e+07</td>\n",
       "      <td>0.442026</td>\n",
       "      <td>3.777636</td>\n",
       "      <td>...</td>\n",
       "      <td>3.140799e+06</td>\n",
       "      <td>1.810966e+06</td>\n",
       "      <td>1.527273e-05</td>\n",
       "      <td>0.085007</td>\n",
       "      <td>-0.013001</td>\n",
       "      <td>0.002237</td>\n",
       "      <td>0.009451</td>\n",
       "      <td>-0.130638</td>\n",
       "      <td>0.002252</td>\n",
       "      <td>0.007156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15.272727</th>\n",
       "      <td>0.153562</td>\n",
       "      <td>0.174053</td>\n",
       "      <td>0.262269</td>\n",
       "      <td>1.894138e+07</td>\n",
       "      <td>0.129487</td>\n",
       "      <td>0.055206</td>\n",
       "      <td>7.331995e+06</td>\n",
       "      <td>2.019742e+07</td>\n",
       "      <td>0.449285</td>\n",
       "      <td>3.689148</td>\n",
       "      <td>...</td>\n",
       "      <td>3.176869e+06</td>\n",
       "      <td>1.829157e+06</td>\n",
       "      <td>7.636364e-06</td>\n",
       "      <td>0.097830</td>\n",
       "      <td>-0.013049</td>\n",
       "      <td>0.002303</td>\n",
       "      <td>0.010109</td>\n",
       "      <td>-0.126842</td>\n",
       "      <td>0.002262</td>\n",
       "      <td>0.007043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16.000000</th>\n",
       "      <td>0.151367</td>\n",
       "      <td>0.172980</td>\n",
       "      <td>0.255983</td>\n",
       "      <td>1.860296e+07</td>\n",
       "      <td>0.138674</td>\n",
       "      <td>0.056343</td>\n",
       "      <td>7.193582e+06</td>\n",
       "      <td>1.978461e+07</td>\n",
       "      <td>0.456582</td>\n",
       "      <td>3.594983</td>\n",
       "      <td>...</td>\n",
       "      <td>3.213486e+06</td>\n",
       "      <td>1.846873e+06</td>\n",
       "      <td>2.290909e-06</td>\n",
       "      <td>0.111119</td>\n",
       "      <td>-0.013167</td>\n",
       "      <td>0.002467</td>\n",
       "      <td>0.010173</td>\n",
       "      <td>-0.136698</td>\n",
       "      <td>0.002305</td>\n",
       "      <td>0.006790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16.727273</th>\n",
       "      <td>0.149041</td>\n",
       "      <td>0.175467</td>\n",
       "      <td>0.249629</td>\n",
       "      <td>1.824312e+07</td>\n",
       "      <td>0.148331</td>\n",
       "      <td>0.057574</td>\n",
       "      <td>7.051586e+06</td>\n",
       "      <td>1.935680e+07</td>\n",
       "      <td>0.463935</td>\n",
       "      <td>3.492303</td>\n",
       "      <td>...</td>\n",
       "      <td>3.250926e+06</td>\n",
       "      <td>1.863875e+06</td>\n",
       "      <td>-3.818182e-07</td>\n",
       "      <td>0.125107</td>\n",
       "      <td>-0.012884</td>\n",
       "      <td>0.002073</td>\n",
       "      <td>0.010018</td>\n",
       "      <td>-0.113044</td>\n",
       "      <td>0.002202</td>\n",
       "      <td>0.007398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17.454545</th>\n",
       "      <td>0.147366</td>\n",
       "      <td>0.180439</td>\n",
       "      <td>0.243615</td>\n",
       "      <td>1.799038e+07</td>\n",
       "      <td>0.155636</td>\n",
       "      <td>0.058333</td>\n",
       "      <td>6.927503e+06</td>\n",
       "      <td>1.900397e+07</td>\n",
       "      <td>0.471009</td>\n",
       "      <td>3.432200</td>\n",
       "      <td>...</td>\n",
       "      <td>3.284256e+06</td>\n",
       "      <td>1.884446e+06</td>\n",
       "      <td>-7.636364e-07</td>\n",
       "      <td>0.135601</td>\n",
       "      <td>-0.011940</td>\n",
       "      <td>0.000760</td>\n",
       "      <td>0.009503</td>\n",
       "      <td>-0.034198</td>\n",
       "      <td>0.001859</td>\n",
       "      <td>0.009423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18.181818</th>\n",
       "      <td>0.146774</td>\n",
       "      <td>0.186073</td>\n",
       "      <td>0.238166</td>\n",
       "      <td>1.791616e+07</td>\n",
       "      <td>0.159019</td>\n",
       "      <td>0.058304</td>\n",
       "      <td>6.833276e+06</td>\n",
       "      <td>1.877612e+07</td>\n",
       "      <td>0.477618</td>\n",
       "      <td>3.443058</td>\n",
       "      <td>...</td>\n",
       "      <td>3.310738e+06</td>\n",
       "      <td>1.910968e+06</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.140271</td>\n",
       "      <td>-0.010714</td>\n",
       "      <td>-0.000946</td>\n",
       "      <td>0.008832</td>\n",
       "      <td>0.068301</td>\n",
       "      <td>0.001413</td>\n",
       "      <td>0.012056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18.909091</th>\n",
       "      <td>0.147354</td>\n",
       "      <td>0.189850</td>\n",
       "      <td>0.233329</td>\n",
       "      <td>1.803472e+07</td>\n",
       "      <td>0.158168</td>\n",
       "      <td>0.057424</td>\n",
       "      <td>6.771293e+06</td>\n",
       "      <td>1.868323e+07</td>\n",
       "      <td>0.483727</td>\n",
       "      <td>3.530553</td>\n",
       "      <td>...</td>\n",
       "      <td>3.329823e+06</td>\n",
       "      <td>1.943915e+06</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.138652</td>\n",
       "      <td>-0.009582</td>\n",
       "      <td>-0.002522</td>\n",
       "      <td>0.008213</td>\n",
       "      <td>0.162916</td>\n",
       "      <td>0.001001</td>\n",
       "      <td>0.014487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19.636364</th>\n",
       "      <td>0.148843</td>\n",
       "      <td>0.193006</td>\n",
       "      <td>0.228967</td>\n",
       "      <td>1.830323e+07</td>\n",
       "      <td>0.154022</td>\n",
       "      <td>0.055883</td>\n",
       "      <td>6.734388e+06</td>\n",
       "      <td>1.869533e+07</td>\n",
       "      <td>0.489445</td>\n",
       "      <td>3.677656</td>\n",
       "      <td>...</td>\n",
       "      <td>3.343155e+06</td>\n",
       "      <td>1.981861e+06</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.132141</td>\n",
       "      <td>-0.008922</td>\n",
       "      <td>-0.003441</td>\n",
       "      <td>0.007852</td>\n",
       "      <td>0.218108</td>\n",
       "      <td>0.000760</td>\n",
       "      <td>0.015905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20.363636</th>\n",
       "      <td>0.150636</td>\n",
       "      <td>0.196018</td>\n",
       "      <td>0.224764</td>\n",
       "      <td>1.862173e+07</td>\n",
       "      <td>0.148779</td>\n",
       "      <td>0.054121</td>\n",
       "      <td>6.705842e+06</td>\n",
       "      <td>1.874242e+07</td>\n",
       "      <td>0.495033</td>\n",
       "      <td>3.844628</td>\n",
       "      <td>...</td>\n",
       "      <td>3.354569e+06</td>\n",
       "      <td>2.021472e+06</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.123999</td>\n",
       "      <td>-0.008899</td>\n",
       "      <td>-0.003474</td>\n",
       "      <td>0.007839</td>\n",
       "      <td>0.220079</td>\n",
       "      <td>0.000752</td>\n",
       "      <td>0.015956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21.090909</th>\n",
       "      <td>0.152169</td>\n",
       "      <td>0.199309</td>\n",
       "      <td>0.220424</td>\n",
       "      <td>1.889738e+07</td>\n",
       "      <td>0.144477</td>\n",
       "      <td>0.052548</td>\n",
       "      <td>6.670132e+06</td>\n",
       "      <td>1.875951e+07</td>\n",
       "      <td>0.500733</td>\n",
       "      <td>3.994569</td>\n",
       "      <td>...</td>\n",
       "      <td>3.367628e+06</td>\n",
       "      <td>2.059655e+06</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.117255</td>\n",
       "      <td>-0.009040</td>\n",
       "      <td>-0.003277</td>\n",
       "      <td>0.007916</td>\n",
       "      <td>0.208252</td>\n",
       "      <td>0.000803</td>\n",
       "      <td>0.015652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50.909091</th>\n",
       "      <td>0.129172</td>\n",
       "      <td>0.429059</td>\n",
       "      <td>0.117316</td>\n",
       "      <td>1.685195e+07</td>\n",
       "      <td>0.042913</td>\n",
       "      <td>0.014843</td>\n",
       "      <td>4.588160e+06</td>\n",
       "      <td>1.320788e+07</td>\n",
       "      <td>0.416747</td>\n",
       "      <td>4.503979</td>\n",
       "      <td>...</td>\n",
       "      <td>3.181909e+06</td>\n",
       "      <td>2.362881e+06</td>\n",
       "      <td>1.863882e-02</td>\n",
       "      <td>0.200737</td>\n",
       "      <td>-0.002570</td>\n",
       "      <td>-0.000574</td>\n",
       "      <td>-0.015972</td>\n",
       "      <td>0.045917</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>0.006162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51.636364</th>\n",
       "      <td>0.128200</td>\n",
       "      <td>0.434889</td>\n",
       "      <td>0.116082</td>\n",
       "      <td>1.680055e+07</td>\n",
       "      <td>0.042022</td>\n",
       "      <td>0.014567</td>\n",
       "      <td>4.572880e+06</td>\n",
       "      <td>1.310816e+07</td>\n",
       "      <td>0.405247</td>\n",
       "      <td>4.537039</td>\n",
       "      <td>...</td>\n",
       "      <td>3.169967e+06</td>\n",
       "      <td>2.366027e+06</td>\n",
       "      <td>2.202128e-02</td>\n",
       "      <td>0.205076</td>\n",
       "      <td>-0.002570</td>\n",
       "      <td>-0.000574</td>\n",
       "      <td>-0.015972</td>\n",
       "      <td>0.045917</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>0.006162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52.363636</th>\n",
       "      <td>0.127228</td>\n",
       "      <td>0.440719</td>\n",
       "      <td>0.114849</td>\n",
       "      <td>1.674914e+07</td>\n",
       "      <td>0.041131</td>\n",
       "      <td>0.014292</td>\n",
       "      <td>4.557600e+06</td>\n",
       "      <td>1.300844e+07</td>\n",
       "      <td>0.393748</td>\n",
       "      <td>4.570099</td>\n",
       "      <td>...</td>\n",
       "      <td>3.158024e+06</td>\n",
       "      <td>2.369172e+06</td>\n",
       "      <td>2.540373e-02</td>\n",
       "      <td>0.209416</td>\n",
       "      <td>-0.002570</td>\n",
       "      <td>-0.000574</td>\n",
       "      <td>-0.015972</td>\n",
       "      <td>0.045917</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>0.006162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53.090909</th>\n",
       "      <td>0.126255</td>\n",
       "      <td>0.446549</td>\n",
       "      <td>0.113615</td>\n",
       "      <td>1.669774e+07</td>\n",
       "      <td>0.040240</td>\n",
       "      <td>0.014017</td>\n",
       "      <td>4.542320e+06</td>\n",
       "      <td>1.290872e+07</td>\n",
       "      <td>0.382248</td>\n",
       "      <td>4.603159</td>\n",
       "      <td>...</td>\n",
       "      <td>3.146082e+06</td>\n",
       "      <td>2.372318e+06</td>\n",
       "      <td>2.878619e-02</td>\n",
       "      <td>0.213756</td>\n",
       "      <td>-0.002570</td>\n",
       "      <td>-0.000574</td>\n",
       "      <td>-0.015972</td>\n",
       "      <td>0.045917</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>0.006162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53.818182</th>\n",
       "      <td>0.125283</td>\n",
       "      <td>0.452379</td>\n",
       "      <td>0.112382</td>\n",
       "      <td>1.664633e+07</td>\n",
       "      <td>0.039349</td>\n",
       "      <td>0.013741</td>\n",
       "      <td>4.527040e+06</td>\n",
       "      <td>1.280900e+07</td>\n",
       "      <td>0.370749</td>\n",
       "      <td>4.636219</td>\n",
       "      <td>...</td>\n",
       "      <td>3.134140e+06</td>\n",
       "      <td>2.375463e+06</td>\n",
       "      <td>3.216864e-02</td>\n",
       "      <td>0.218095</td>\n",
       "      <td>-0.002570</td>\n",
       "      <td>-0.000574</td>\n",
       "      <td>-0.015972</td>\n",
       "      <td>0.045917</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>0.006162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54.545455</th>\n",
       "      <td>0.124311</td>\n",
       "      <td>0.458209</td>\n",
       "      <td>0.111148</td>\n",
       "      <td>1.659493e+07</td>\n",
       "      <td>0.038457</td>\n",
       "      <td>0.013466</td>\n",
       "      <td>4.511760e+06</td>\n",
       "      <td>1.270928e+07</td>\n",
       "      <td>0.359249</td>\n",
       "      <td>4.669280</td>\n",
       "      <td>...</td>\n",
       "      <td>3.122198e+06</td>\n",
       "      <td>2.378608e+06</td>\n",
       "      <td>3.555110e-02</td>\n",
       "      <td>0.222435</td>\n",
       "      <td>-0.002570</td>\n",
       "      <td>-0.000574</td>\n",
       "      <td>-0.015972</td>\n",
       "      <td>0.045917</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>0.006162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55.272727</th>\n",
       "      <td>0.123339</td>\n",
       "      <td>0.464039</td>\n",
       "      <td>0.109915</td>\n",
       "      <td>1.654352e+07</td>\n",
       "      <td>0.037566</td>\n",
       "      <td>0.013190</td>\n",
       "      <td>4.496480e+06</td>\n",
       "      <td>1.260956e+07</td>\n",
       "      <td>0.347750</td>\n",
       "      <td>4.702340</td>\n",
       "      <td>...</td>\n",
       "      <td>3.110256e+06</td>\n",
       "      <td>2.381754e+06</td>\n",
       "      <td>3.893355e-02</td>\n",
       "      <td>0.226774</td>\n",
       "      <td>-0.002570</td>\n",
       "      <td>-0.000574</td>\n",
       "      <td>-0.015972</td>\n",
       "      <td>0.045917</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>0.006162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56.000000</th>\n",
       "      <td>0.122367</td>\n",
       "      <td>0.469869</td>\n",
       "      <td>0.108682</td>\n",
       "      <td>1.649212e+07</td>\n",
       "      <td>0.036675</td>\n",
       "      <td>0.012915</td>\n",
       "      <td>4.481200e+06</td>\n",
       "      <td>1.250984e+07</td>\n",
       "      <td>0.336250</td>\n",
       "      <td>4.735400</td>\n",
       "      <td>...</td>\n",
       "      <td>3.098314e+06</td>\n",
       "      <td>2.384899e+06</td>\n",
       "      <td>4.231601e-02</td>\n",
       "      <td>0.231114</td>\n",
       "      <td>-0.002570</td>\n",
       "      <td>-0.000574</td>\n",
       "      <td>-0.015972</td>\n",
       "      <td>0.045917</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>0.006162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56.727273</th>\n",
       "      <td>0.121394</td>\n",
       "      <td>0.475699</td>\n",
       "      <td>0.107448</td>\n",
       "      <td>1.644071e+07</td>\n",
       "      <td>0.035784</td>\n",
       "      <td>0.012639</td>\n",
       "      <td>4.465920e+06</td>\n",
       "      <td>1.241012e+07</td>\n",
       "      <td>0.324751</td>\n",
       "      <td>4.768460</td>\n",
       "      <td>...</td>\n",
       "      <td>3.086372e+06</td>\n",
       "      <td>2.388044e+06</td>\n",
       "      <td>4.569846e-02</td>\n",
       "      <td>0.235454</td>\n",
       "      <td>-0.002570</td>\n",
       "      <td>-0.000574</td>\n",
       "      <td>-0.015972</td>\n",
       "      <td>0.045917</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>0.006162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57.454545</th>\n",
       "      <td>0.120422</td>\n",
       "      <td>0.481529</td>\n",
       "      <td>0.106215</td>\n",
       "      <td>1.638931e+07</td>\n",
       "      <td>0.034893</td>\n",
       "      <td>0.012364</td>\n",
       "      <td>4.450640e+06</td>\n",
       "      <td>1.231040e+07</td>\n",
       "      <td>0.313251</td>\n",
       "      <td>4.801520</td>\n",
       "      <td>...</td>\n",
       "      <td>3.074430e+06</td>\n",
       "      <td>2.391190e+06</td>\n",
       "      <td>4.908091e-02</td>\n",
       "      <td>0.239793</td>\n",
       "      <td>-0.002570</td>\n",
       "      <td>-0.000574</td>\n",
       "      <td>-0.015972</td>\n",
       "      <td>0.045917</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>0.006162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58.181818</th>\n",
       "      <td>0.119450</td>\n",
       "      <td>0.487359</td>\n",
       "      <td>0.104981</td>\n",
       "      <td>1.633790e+07</td>\n",
       "      <td>0.034001</td>\n",
       "      <td>0.012089</td>\n",
       "      <td>4.435360e+06</td>\n",
       "      <td>1.221068e+07</td>\n",
       "      <td>0.301752</td>\n",
       "      <td>4.834580</td>\n",
       "      <td>...</td>\n",
       "      <td>3.062488e+06</td>\n",
       "      <td>2.394335e+06</td>\n",
       "      <td>5.246337e-02</td>\n",
       "      <td>0.244133</td>\n",
       "      <td>-0.002570</td>\n",
       "      <td>-0.000574</td>\n",
       "      <td>-0.015972</td>\n",
       "      <td>0.045917</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>0.006162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58.909091</th>\n",
       "      <td>0.118478</td>\n",
       "      <td>0.493189</td>\n",
       "      <td>0.103748</td>\n",
       "      <td>1.628650e+07</td>\n",
       "      <td>0.033110</td>\n",
       "      <td>0.011813</td>\n",
       "      <td>4.420080e+06</td>\n",
       "      <td>1.211096e+07</td>\n",
       "      <td>0.290252</td>\n",
       "      <td>4.867640</td>\n",
       "      <td>...</td>\n",
       "      <td>3.050546e+06</td>\n",
       "      <td>2.397480e+06</td>\n",
       "      <td>5.584582e-02</td>\n",
       "      <td>0.248473</td>\n",
       "      <td>-0.002570</td>\n",
       "      <td>-0.000574</td>\n",
       "      <td>-0.015972</td>\n",
       "      <td>0.045917</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>0.006162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59.636364</th>\n",
       "      <td>0.117505</td>\n",
       "      <td>0.499019</td>\n",
       "      <td>0.102514</td>\n",
       "      <td>1.623509e+07</td>\n",
       "      <td>0.032219</td>\n",
       "      <td>0.011538</td>\n",
       "      <td>4.404800e+06</td>\n",
       "      <td>1.201124e+07</td>\n",
       "      <td>0.278753</td>\n",
       "      <td>4.900700</td>\n",
       "      <td>...</td>\n",
       "      <td>3.038604e+06</td>\n",
       "      <td>2.400626e+06</td>\n",
       "      <td>5.922828e-02</td>\n",
       "      <td>0.252812</td>\n",
       "      <td>-0.002570</td>\n",
       "      <td>-0.000574</td>\n",
       "      <td>-0.015972</td>\n",
       "      <td>0.045917</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>0.006162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60.363636</th>\n",
       "      <td>0.116533</td>\n",
       "      <td>0.504850</td>\n",
       "      <td>0.101281</td>\n",
       "      <td>1.618369e+07</td>\n",
       "      <td>0.031328</td>\n",
       "      <td>0.011262</td>\n",
       "      <td>4.389521e+06</td>\n",
       "      <td>1.191152e+07</td>\n",
       "      <td>0.267253</td>\n",
       "      <td>4.933760</td>\n",
       "      <td>...</td>\n",
       "      <td>3.026661e+06</td>\n",
       "      <td>2.403771e+06</td>\n",
       "      <td>6.261073e-02</td>\n",
       "      <td>0.257152</td>\n",
       "      <td>-0.002570</td>\n",
       "      <td>-0.000574</td>\n",
       "      <td>-0.015972</td>\n",
       "      <td>0.045917</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>0.006162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61.090909</th>\n",
       "      <td>0.115561</td>\n",
       "      <td>0.510680</td>\n",
       "      <td>0.100047</td>\n",
       "      <td>1.613228e+07</td>\n",
       "      <td>0.030436</td>\n",
       "      <td>0.010987</td>\n",
       "      <td>4.374241e+06</td>\n",
       "      <td>1.181180e+07</td>\n",
       "      <td>0.255754</td>\n",
       "      <td>4.966820</td>\n",
       "      <td>...</td>\n",
       "      <td>3.014719e+06</td>\n",
       "      <td>2.406917e+06</td>\n",
       "      <td>6.599319e-02</td>\n",
       "      <td>0.261491</td>\n",
       "      <td>-0.002570</td>\n",
       "      <td>-0.000574</td>\n",
       "      <td>-0.015972</td>\n",
       "      <td>0.045917</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>0.006162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61.818182</th>\n",
       "      <td>0.114589</td>\n",
       "      <td>0.516510</td>\n",
       "      <td>0.098814</td>\n",
       "      <td>1.608088e+07</td>\n",
       "      <td>0.029545</td>\n",
       "      <td>0.010712</td>\n",
       "      <td>4.358961e+06</td>\n",
       "      <td>1.171208e+07</td>\n",
       "      <td>0.244254</td>\n",
       "      <td>4.999880</td>\n",
       "      <td>...</td>\n",
       "      <td>3.002777e+06</td>\n",
       "      <td>2.410062e+06</td>\n",
       "      <td>6.937564e-02</td>\n",
       "      <td>0.265831</td>\n",
       "      <td>-0.002570</td>\n",
       "      <td>-0.000574</td>\n",
       "      <td>-0.015972</td>\n",
       "      <td>0.045917</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>0.006162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62.545455</th>\n",
       "      <td>0.113617</td>\n",
       "      <td>0.522340</td>\n",
       "      <td>0.097580</td>\n",
       "      <td>1.602947e+07</td>\n",
       "      <td>0.028654</td>\n",
       "      <td>0.010436</td>\n",
       "      <td>4.343681e+06</td>\n",
       "      <td>1.161237e+07</td>\n",
       "      <td>0.232755</td>\n",
       "      <td>5.032940</td>\n",
       "      <td>...</td>\n",
       "      <td>2.990835e+06</td>\n",
       "      <td>2.413207e+06</td>\n",
       "      <td>7.275810e-02</td>\n",
       "      <td>0.270171</td>\n",
       "      <td>-0.002570</td>\n",
       "      <td>-0.000574</td>\n",
       "      <td>-0.015972</td>\n",
       "      <td>0.045917</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>0.006162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63.272727</th>\n",
       "      <td>0.112644</td>\n",
       "      <td>0.528170</td>\n",
       "      <td>0.096347</td>\n",
       "      <td>1.597807e+07</td>\n",
       "      <td>0.027763</td>\n",
       "      <td>0.010161</td>\n",
       "      <td>4.328401e+06</td>\n",
       "      <td>1.151265e+07</td>\n",
       "      <td>0.221255</td>\n",
       "      <td>5.066000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.978893e+06</td>\n",
       "      <td>2.416353e+06</td>\n",
       "      <td>7.614055e-02</td>\n",
       "      <td>0.274510</td>\n",
       "      <td>-0.002570</td>\n",
       "      <td>-0.000574</td>\n",
       "      <td>-0.015972</td>\n",
       "      <td>0.045917</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>0.006162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64.000000</th>\n",
       "      <td>0.111672</td>\n",
       "      <td>0.534000</td>\n",
       "      <td>0.095113</td>\n",
       "      <td>1.592666e+07</td>\n",
       "      <td>0.026872</td>\n",
       "      <td>0.009885</td>\n",
       "      <td>4.313121e+06</td>\n",
       "      <td>1.141293e+07</td>\n",
       "      <td>0.209756</td>\n",
       "      <td>5.099060</td>\n",
       "      <td>...</td>\n",
       "      <td>2.966951e+06</td>\n",
       "      <td>2.419498e+06</td>\n",
       "      <td>7.952301e-02</td>\n",
       "      <td>0.278850</td>\n",
       "      <td>-0.002570</td>\n",
       "      <td>-0.000574</td>\n",
       "      <td>-0.015972</td>\n",
       "      <td>0.045917</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>0.006162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64.727273</th>\n",
       "      <td>0.110700</td>\n",
       "      <td>0.539830</td>\n",
       "      <td>0.093880</td>\n",
       "      <td>1.587526e+07</td>\n",
       "      <td>0.025980</td>\n",
       "      <td>0.009610</td>\n",
       "      <td>4.297841e+06</td>\n",
       "      <td>1.131321e+07</td>\n",
       "      <td>0.198256</td>\n",
       "      <td>5.132120</td>\n",
       "      <td>...</td>\n",
       "      <td>2.955009e+06</td>\n",
       "      <td>2.422643e+06</td>\n",
       "      <td>8.290546e-02</td>\n",
       "      <td>0.283189</td>\n",
       "      <td>-0.002570</td>\n",
       "      <td>-0.000574</td>\n",
       "      <td>-0.015972</td>\n",
       "      <td>0.045917</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>0.006162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65.454545</th>\n",
       "      <td>0.109728</td>\n",
       "      <td>0.545660</td>\n",
       "      <td>0.092647</td>\n",
       "      <td>1.582385e+07</td>\n",
       "      <td>0.025089</td>\n",
       "      <td>0.009334</td>\n",
       "      <td>4.282561e+06</td>\n",
       "      <td>1.121349e+07</td>\n",
       "      <td>0.186757</td>\n",
       "      <td>5.165180</td>\n",
       "      <td>...</td>\n",
       "      <td>2.943067e+06</td>\n",
       "      <td>2.425789e+06</td>\n",
       "      <td>8.628792e-02</td>\n",
       "      <td>0.287529</td>\n",
       "      <td>-0.002570</td>\n",
       "      <td>-0.000574</td>\n",
       "      <td>-0.015972</td>\n",
       "      <td>0.045917</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>0.006162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66.181818</th>\n",
       "      <td>0.108756</td>\n",
       "      <td>0.551490</td>\n",
       "      <td>0.091413</td>\n",
       "      <td>1.577245e+07</td>\n",
       "      <td>0.024198</td>\n",
       "      <td>0.009059</td>\n",
       "      <td>4.267281e+06</td>\n",
       "      <td>1.111377e+07</td>\n",
       "      <td>0.175258</td>\n",
       "      <td>5.198240</td>\n",
       "      <td>...</td>\n",
       "      <td>2.931125e+06</td>\n",
       "      <td>2.428934e+06</td>\n",
       "      <td>8.967037e-02</td>\n",
       "      <td>0.291869</td>\n",
       "      <td>-0.002570</td>\n",
       "      <td>-0.000574</td>\n",
       "      <td>-0.015972</td>\n",
       "      <td>0.045917</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>0.006162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66.909091</th>\n",
       "      <td>0.107783</td>\n",
       "      <td>0.557320</td>\n",
       "      <td>0.090180</td>\n",
       "      <td>1.572104e+07</td>\n",
       "      <td>0.023307</td>\n",
       "      <td>0.008784</td>\n",
       "      <td>4.252001e+06</td>\n",
       "      <td>1.101405e+07</td>\n",
       "      <td>0.163758</td>\n",
       "      <td>5.231300</td>\n",
       "      <td>...</td>\n",
       "      <td>2.919183e+06</td>\n",
       "      <td>2.432079e+06</td>\n",
       "      <td>9.305282e-02</td>\n",
       "      <td>0.296208</td>\n",
       "      <td>-0.002570</td>\n",
       "      <td>-0.000574</td>\n",
       "      <td>-0.015972</td>\n",
       "      <td>0.045917</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>0.006162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67.636364</th>\n",
       "      <td>0.106811</td>\n",
       "      <td>0.563150</td>\n",
       "      <td>0.088946</td>\n",
       "      <td>1.566964e+07</td>\n",
       "      <td>0.022416</td>\n",
       "      <td>0.008508</td>\n",
       "      <td>4.236721e+06</td>\n",
       "      <td>1.091433e+07</td>\n",
       "      <td>0.152259</td>\n",
       "      <td>5.264360</td>\n",
       "      <td>...</td>\n",
       "      <td>2.907241e+06</td>\n",
       "      <td>2.435225e+06</td>\n",
       "      <td>9.643528e-02</td>\n",
       "      <td>0.300548</td>\n",
       "      <td>-0.002570</td>\n",
       "      <td>-0.000574</td>\n",
       "      <td>-0.015972</td>\n",
       "      <td>0.045917</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>0.006162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68.363636</th>\n",
       "      <td>0.105839</td>\n",
       "      <td>0.568980</td>\n",
       "      <td>0.087713</td>\n",
       "      <td>1.561823e+07</td>\n",
       "      <td>0.021524</td>\n",
       "      <td>0.008233</td>\n",
       "      <td>4.221441e+06</td>\n",
       "      <td>1.081461e+07</td>\n",
       "      <td>0.140759</td>\n",
       "      <td>5.297420</td>\n",
       "      <td>...</td>\n",
       "      <td>2.895298e+06</td>\n",
       "      <td>2.438370e+06</td>\n",
       "      <td>9.981773e-02</td>\n",
       "      <td>0.304888</td>\n",
       "      <td>-0.002570</td>\n",
       "      <td>-0.000574</td>\n",
       "      <td>-0.015972</td>\n",
       "      <td>0.045917</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>0.006162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69.090909</th>\n",
       "      <td>0.104867</td>\n",
       "      <td>0.574810</td>\n",
       "      <td>0.086479</td>\n",
       "      <td>1.556683e+07</td>\n",
       "      <td>0.020633</td>\n",
       "      <td>0.007957</td>\n",
       "      <td>4.206161e+06</td>\n",
       "      <td>1.071489e+07</td>\n",
       "      <td>0.129260</td>\n",
       "      <td>5.330480</td>\n",
       "      <td>...</td>\n",
       "      <td>2.883356e+06</td>\n",
       "      <td>2.441516e+06</td>\n",
       "      <td>1.032002e-01</td>\n",
       "      <td>0.309227</td>\n",
       "      <td>-0.002570</td>\n",
       "      <td>-0.000574</td>\n",
       "      <td>-0.015972</td>\n",
       "      <td>0.045917</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>0.006162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69.818182</th>\n",
       "      <td>0.103895</td>\n",
       "      <td>0.580640</td>\n",
       "      <td>0.085246</td>\n",
       "      <td>1.551542e+07</td>\n",
       "      <td>0.019742</td>\n",
       "      <td>0.007682</td>\n",
       "      <td>4.190881e+06</td>\n",
       "      <td>1.061517e+07</td>\n",
       "      <td>0.117760</td>\n",
       "      <td>5.363540</td>\n",
       "      <td>...</td>\n",
       "      <td>2.871414e+06</td>\n",
       "      <td>2.444661e+06</td>\n",
       "      <td>1.065826e-01</td>\n",
       "      <td>0.313567</td>\n",
       "      <td>-0.002570</td>\n",
       "      <td>-0.000574</td>\n",
       "      <td>-0.015972</td>\n",
       "      <td>0.045917</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>0.006162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70.545455</th>\n",
       "      <td>0.102922</td>\n",
       "      <td>0.586470</td>\n",
       "      <td>0.084012</td>\n",
       "      <td>1.546402e+07</td>\n",
       "      <td>0.018851</td>\n",
       "      <td>0.007407</td>\n",
       "      <td>4.175601e+06</td>\n",
       "      <td>1.051545e+07</td>\n",
       "      <td>0.106261</td>\n",
       "      <td>5.396600</td>\n",
       "      <td>...</td>\n",
       "      <td>2.859472e+06</td>\n",
       "      <td>2.447806e+06</td>\n",
       "      <td>1.099651e-01</td>\n",
       "      <td>0.317906</td>\n",
       "      <td>-0.002570</td>\n",
       "      <td>-0.000574</td>\n",
       "      <td>-0.015972</td>\n",
       "      <td>0.045917</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>0.006162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71.272727</th>\n",
       "      <td>0.101950</td>\n",
       "      <td>0.592300</td>\n",
       "      <td>0.082779</td>\n",
       "      <td>1.541261e+07</td>\n",
       "      <td>0.017960</td>\n",
       "      <td>0.007131</td>\n",
       "      <td>4.160321e+06</td>\n",
       "      <td>1.041573e+07</td>\n",
       "      <td>0.094761</td>\n",
       "      <td>5.429661</td>\n",
       "      <td>...</td>\n",
       "      <td>2.847530e+06</td>\n",
       "      <td>2.450952e+06</td>\n",
       "      <td>1.133476e-01</td>\n",
       "      <td>0.322246</td>\n",
       "      <td>-0.002570</td>\n",
       "      <td>-0.000574</td>\n",
       "      <td>-0.015972</td>\n",
       "      <td>0.045917</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>0.006162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72.000000</th>\n",
       "      <td>0.100978</td>\n",
       "      <td>0.598130</td>\n",
       "      <td>0.081545</td>\n",
       "      <td>1.536121e+07</td>\n",
       "      <td>0.017068</td>\n",
       "      <td>0.006856</td>\n",
       "      <td>4.145041e+06</td>\n",
       "      <td>1.031601e+07</td>\n",
       "      <td>0.083262</td>\n",
       "      <td>5.462721</td>\n",
       "      <td>...</td>\n",
       "      <td>2.835588e+06</td>\n",
       "      <td>2.454097e+06</td>\n",
       "      <td>1.167300e-01</td>\n",
       "      <td>0.326586</td>\n",
       "      <td>-0.002570</td>\n",
       "      <td>-0.000574</td>\n",
       "      <td>-0.015972</td>\n",
       "      <td>0.045917</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>0.006162</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   feature                                            \\\n",
       "                  ATP (uM) Acetate g/L Acetyl-CoA (uM)          AtoB   \n",
       "Strain Time (h)                                                        \n",
       "L2     0.000000   0.302952    0.303717        0.286359  1.249415e+06   \n",
       "       0.727273   0.288104    0.412069        0.293084  3.175036e+06   \n",
       "       1.454545   0.273255    0.494135        0.299808  5.082022e+06   \n",
       "       2.181818   0.258406    0.549913        0.306533  6.970373e+06   \n",
       "       2.909091   0.243558    0.581022        0.313258  8.873632e+06   \n",
       "       3.636364   0.228709    0.579706        0.319983  1.070980e+07   \n",
       "       4.363636   0.212950    0.555286        0.327633  1.244907e+07   \n",
       "       5.090909   0.198253    0.520314        0.334204  1.409144e+07   \n",
       "       5.818182   0.186741    0.473192        0.337533  1.566671e+07   \n",
       "       6.545455   0.179324    0.422059        0.336694  1.738527e+07   \n",
       "       7.272727   0.175698    0.374880        0.331997  1.889264e+07   \n",
       "       8.000000   0.174348    0.331686        0.324985  1.994779e+07   \n",
       "       8.727273   0.172542    0.293871        0.318435  2.047537e+07   \n",
       "       9.454545   0.170433    0.263639        0.312194  2.055074e+07   \n",
       "       10.181818  0.168325    0.239559        0.305954  2.041544e+07   \n",
       "       10.909091  0.166216    0.218725        0.299713  2.042315e+07   \n",
       "       11.636364  0.164107    0.204083        0.293472  2.036141e+07   \n",
       "       12.363636  0.161998    0.194408        0.287232  2.019934e+07   \n",
       "       13.090909  0.159889    0.187484        0.280991  1.993694e+07   \n",
       "       13.818182  0.157780    0.183172        0.274750  1.960509e+07   \n",
       "       14.545455  0.155671    0.178161        0.268510  1.926551e+07   \n",
       "       15.272727  0.153562    0.174053        0.262269  1.894138e+07   \n",
       "       16.000000  0.151367    0.172980        0.255983  1.860296e+07   \n",
       "       16.727273  0.149041    0.175467        0.249629  1.824312e+07   \n",
       "       17.454545  0.147366    0.180439        0.243615  1.799038e+07   \n",
       "       18.181818  0.146774    0.186073        0.238166  1.791616e+07   \n",
       "       18.909091  0.147354    0.189850        0.233329  1.803472e+07   \n",
       "       19.636364  0.148843    0.193006        0.228967  1.830323e+07   \n",
       "       20.363636  0.150636    0.196018        0.224764  1.862173e+07   \n",
       "       21.090909  0.152169    0.199309        0.220424  1.889738e+07   \n",
       "...                    ...         ...             ...           ...   \n",
       "       50.909091  0.129172    0.429059        0.117316  1.685195e+07   \n",
       "       51.636364  0.128200    0.434889        0.116082  1.680055e+07   \n",
       "       52.363636  0.127228    0.440719        0.114849  1.674914e+07   \n",
       "       53.090909  0.126255    0.446549        0.113615  1.669774e+07   \n",
       "       53.818182  0.125283    0.452379        0.112382  1.664633e+07   \n",
       "       54.545455  0.124311    0.458209        0.111148  1.659493e+07   \n",
       "       55.272727  0.123339    0.464039        0.109915  1.654352e+07   \n",
       "       56.000000  0.122367    0.469869        0.108682  1.649212e+07   \n",
       "       56.727273  0.121394    0.475699        0.107448  1.644071e+07   \n",
       "       57.454545  0.120422    0.481529        0.106215  1.638931e+07   \n",
       "       58.181818  0.119450    0.487359        0.104981  1.633790e+07   \n",
       "       58.909091  0.118478    0.493189        0.103748  1.628650e+07   \n",
       "       59.636364  0.117505    0.499019        0.102514  1.623509e+07   \n",
       "       60.363636  0.116533    0.504850        0.101281  1.618369e+07   \n",
       "       61.090909  0.115561    0.510680        0.100047  1.613228e+07   \n",
       "       61.818182  0.114589    0.516510        0.098814  1.608088e+07   \n",
       "       62.545455  0.113617    0.522340        0.097580  1.602947e+07   \n",
       "       63.272727  0.112644    0.528170        0.096347  1.597807e+07   \n",
       "       64.000000  0.111672    0.534000        0.095113  1.592666e+07   \n",
       "       64.727273  0.110700    0.539830        0.093880  1.587526e+07   \n",
       "       65.454545  0.109728    0.545660        0.092647  1.582385e+07   \n",
       "       66.181818  0.108756    0.551490        0.091413  1.577245e+07   \n",
       "       66.909091  0.107783    0.557320        0.090180  1.572104e+07   \n",
       "       67.636364  0.106811    0.563150        0.088946  1.566964e+07   \n",
       "       68.363636  0.105839    0.568980        0.087713  1.561823e+07   \n",
       "       69.090909  0.104867    0.574810        0.086479  1.556683e+07   \n",
       "       69.818182  0.103895    0.580640        0.085246  1.551542e+07   \n",
       "       70.545455  0.102922    0.586470        0.084012  1.546402e+07   \n",
       "       71.272727  0.101950    0.592300        0.082779  1.541261e+07   \n",
       "       72.000000  0.100978    0.598130        0.081545  1.536121e+07   \n",
       "\n",
       "                                                                     \\\n",
       "                  GPP (uM) HMG-CoA (uM)          HMGR          HMGS   \n",
       "Strain Time (h)                                                       \n",
       "L2     0.000000   0.008534     0.007628  9.586787e+05  1.806693e+06   \n",
       "       0.727273   0.015908     0.011736  1.817455e+06  3.830152e+06   \n",
       "       1.454545   0.023576     0.015843  2.659607e+06  5.845767e+06   \n",
       "       2.181818   0.031540     0.019950  3.485134e+06  7.853538e+06   \n",
       "       2.909091   0.039267     0.024057  4.323961e+06  9.867584e+06   \n",
       "       3.636364   0.048057     0.028164  5.102941e+06  1.185339e+07   \n",
       "       4.363636   0.058380     0.032488  5.795474e+06  1.379841e+07   \n",
       "       5.090909   0.070237     0.036559  6.401561e+06  1.570263e+07   \n",
       "       5.818182   0.083156     0.039872  6.947800e+06  1.757862e+07   \n",
       "       6.545455   0.097970     0.042210  7.551284e+06  1.965121e+07   \n",
       "       7.272727   0.109882     0.043645  8.072226e+06  2.143046e+07   \n",
       "       8.000000   0.116462     0.044538  8.401481e+06  2.261051e+07   \n",
       "       8.727273   0.116822     0.045540  8.507104e+06  2.309147e+07   \n",
       "       9.454545   0.111849     0.046614  8.421040e+06  2.297323e+07   \n",
       "       10.181818  0.103509     0.047688  8.239992e+06  2.256417e+07   \n",
       "       10.909091  0.097298     0.048762  8.122409e+06  2.235045e+07   \n",
       "       11.636364  0.094243     0.049836  8.000967e+06  2.209716e+07   \n",
       "       12.363636  0.095749     0.050910  7.873949e+06  2.178669e+07   \n",
       "       13.090909  0.101816     0.051984  7.741356e+06  2.141906e+07   \n",
       "       13.818182  0.111040     0.053058  7.604902e+06  2.101185e+07   \n",
       "       14.545455  0.120614     0.054132  7.468019e+06  2.060023e+07   \n",
       "       15.272727  0.129487     0.055206  7.331995e+06  2.019742e+07   \n",
       "       16.000000  0.138674     0.056343  7.193582e+06  1.978461e+07   \n",
       "       16.727273  0.148331     0.057574  7.051586e+06  1.935680e+07   \n",
       "       17.454545  0.155636     0.058333  6.927503e+06  1.900397e+07   \n",
       "       18.181818  0.159019     0.058304  6.833276e+06  1.877612e+07   \n",
       "       18.909091  0.158168     0.057424  6.771293e+06  1.868323e+07   \n",
       "       19.636364  0.154022     0.055883  6.734388e+06  1.869533e+07   \n",
       "       20.363636  0.148779     0.054121  6.705842e+06  1.874242e+07   \n",
       "       21.090909  0.144477     0.052548  6.670132e+06  1.875951e+07   \n",
       "...                    ...          ...           ...           ...   \n",
       "       50.909091  0.042913     0.014843  4.588160e+06  1.320788e+07   \n",
       "       51.636364  0.042022     0.014567  4.572880e+06  1.310816e+07   \n",
       "       52.363636  0.041131     0.014292  4.557600e+06  1.300844e+07   \n",
       "       53.090909  0.040240     0.014017  4.542320e+06  1.290872e+07   \n",
       "       53.818182  0.039349     0.013741  4.527040e+06  1.280900e+07   \n",
       "       54.545455  0.038457     0.013466  4.511760e+06  1.270928e+07   \n",
       "       55.272727  0.037566     0.013190  4.496480e+06  1.260956e+07   \n",
       "       56.000000  0.036675     0.012915  4.481200e+06  1.250984e+07   \n",
       "       56.727273  0.035784     0.012639  4.465920e+06  1.241012e+07   \n",
       "       57.454545  0.034893     0.012364  4.450640e+06  1.231040e+07   \n",
       "       58.181818  0.034001     0.012089  4.435360e+06  1.221068e+07   \n",
       "       58.909091  0.033110     0.011813  4.420080e+06  1.211096e+07   \n",
       "       59.636364  0.032219     0.011538  4.404800e+06  1.201124e+07   \n",
       "       60.363636  0.031328     0.011262  4.389521e+06  1.191152e+07   \n",
       "       61.090909  0.030436     0.010987  4.374241e+06  1.181180e+07   \n",
       "       61.818182  0.029545     0.010712  4.358961e+06  1.171208e+07   \n",
       "       62.545455  0.028654     0.010436  4.343681e+06  1.161237e+07   \n",
       "       63.272727  0.027763     0.010161  4.328401e+06  1.151265e+07   \n",
       "       64.000000  0.026872     0.009885  4.313121e+06  1.141293e+07   \n",
       "       64.727273  0.025980     0.009610  4.297841e+06  1.131321e+07   \n",
       "       65.454545  0.025089     0.009334  4.282561e+06  1.121349e+07   \n",
       "       66.181818  0.024198     0.009059  4.267281e+06  1.111377e+07   \n",
       "       66.909091  0.023307     0.008784  4.252001e+06  1.101405e+07   \n",
       "       67.636364  0.022416     0.008508  4.236721e+06  1.091433e+07   \n",
       "       68.363636  0.021524     0.008233  4.221441e+06  1.081461e+07   \n",
       "       69.090909  0.020633     0.007957  4.206161e+06  1.071489e+07   \n",
       "       69.818182  0.019742     0.007682  4.190881e+06  1.061517e+07   \n",
       "       70.545455  0.018851     0.007407  4.175601e+06  1.051545e+07   \n",
       "       71.272727  0.017960     0.007131  4.160321e+06  1.041573e+07   \n",
       "       72.000000  0.017068     0.006856  4.145041e+06  1.031601e+07   \n",
       "\n",
       "                                                                 ...      \\\n",
       "                 IPP/DMAPP (uM) Intracellular Mevalonate (uM)    ...       \n",
       "Strain Time (h)                                                  ...       \n",
       "L2     0.000000        0.174187                      0.880607    ...       \n",
       "       0.727273        0.174457                      1.219823    ...       \n",
       "       1.454545        0.175593                      1.546884    ...       \n",
       "       2.181818        0.177594                      1.861788    ...       \n",
       "       2.909091        0.178903                      2.186418    ...       \n",
       "       3.636364        0.183329                      2.467286    ...       \n",
       "       4.363636        0.192256                      2.684945    ...       \n",
       "       5.090909        0.205685                      2.839393    ...       \n",
       "       5.818182        0.222231                      2.950080    ...       \n",
       "       6.545455        0.238153                      3.053051    ...       \n",
       "       7.272727        0.254837                      3.170027    ...       \n",
       "       8.000000        0.274430                      3.295566    ...       \n",
       "       8.727273        0.297417                      3.431093    ...       \n",
       "       9.454545        0.323313                      3.575182    ...       \n",
       "       10.181818       0.351574                      3.734695    ...       \n",
       "       10.909091       0.378411                      3.885783    ...       \n",
       "       11.636364       0.401150                      3.986727    ...       \n",
       "       12.363636       0.417970                      4.015241    ...       \n",
       "       13.090909       0.428872                      3.971326    ...       \n",
       "       13.818182       0.435676                      3.877266    ...       \n",
       "       14.545455       0.442026                      3.777636    ...       \n",
       "       15.272727       0.449285                      3.689148    ...       \n",
       "       16.000000       0.456582                      3.594983    ...       \n",
       "       16.727273       0.463935                      3.492303    ...       \n",
       "       17.454545       0.471009                      3.432200    ...       \n",
       "       18.181818       0.477618                      3.443058    ...       \n",
       "       18.909091       0.483727                      3.530553    ...       \n",
       "       19.636364       0.489445                      3.677656    ...       \n",
       "       20.363636       0.495033                      3.844628    ...       \n",
       "       21.090909       0.500733                      3.994569    ...       \n",
       "...                         ...                           ...    ...       \n",
       "       50.909091       0.416747                      4.503979    ...       \n",
       "       51.636364       0.405247                      4.537039    ...       \n",
       "       52.363636       0.393748                      4.570099    ...       \n",
       "       53.090909       0.382248                      4.603159    ...       \n",
       "       53.818182       0.370749                      4.636219    ...       \n",
       "       54.545455       0.359249                      4.669280    ...       \n",
       "       55.272727       0.347750                      4.702340    ...       \n",
       "       56.000000       0.336250                      4.735400    ...       \n",
       "       56.727273       0.324751                      4.768460    ...       \n",
       "       57.454545       0.313251                      4.801520    ...       \n",
       "       58.181818       0.301752                      4.834580    ...       \n",
       "       58.909091       0.290252                      4.867640    ...       \n",
       "       59.636364       0.278753                      4.900700    ...       \n",
       "       60.363636       0.267253                      4.933760    ...       \n",
       "       61.090909       0.255754                      4.966820    ...       \n",
       "       61.818182       0.244254                      4.999880    ...       \n",
       "       62.545455       0.232755                      5.032940    ...       \n",
       "       63.272727       0.221255                      5.066000    ...       \n",
       "       64.000000       0.209756                      5.099060    ...       \n",
       "       64.727273       0.198256                      5.132120    ...       \n",
       "       65.454545       0.186757                      5.165180    ...       \n",
       "       66.181818       0.175258                      5.198240    ...       \n",
       "       66.909091       0.163758                      5.231300    ...       \n",
       "       67.636364       0.152259                      5.264360    ...       \n",
       "       68.363636       0.140759                      5.297420    ...       \n",
       "       69.090909       0.129260                      5.330480    ...       \n",
       "       69.818182       0.117760                      5.363540    ...       \n",
       "       70.545455       0.106261                      5.396600    ...       \n",
       "       71.272727       0.094761                      5.429661    ...       \n",
       "       72.000000       0.083262                      5.462721    ...       \n",
       "\n",
       "                                                                         \\\n",
       "                           PMD           PMK  Pyruvate g/L citrate (uM)   \n",
       "Strain Time (h)                                                           \n",
       "L2     0.000000   2.318537e+05  3.571606e+04  3.834561e-02     0.021561   \n",
       "       0.727273   3.794901e+05  1.756340e+05  3.151425e-02     0.021772   \n",
       "       1.454545   5.368942e+05  3.123969e+05  2.792344e-02     0.021983   \n",
       "       2.181818   7.040660e+05  4.460049e+05  2.757316e-02     0.022194   \n",
       "       2.909091   8.634237e+05  5.821369e+05  3.436354e-02     0.022404   \n",
       "       3.636364   1.057945e+06  7.069111e+05  3.957085e-02     0.022615   \n",
       "       4.363636   1.303258e+06  8.152797e+05  3.623255e-02     0.022696   \n",
       "       5.090909   1.599364e+06  9.072426e+05  2.428436e-02     0.022929   \n",
       "       5.818182   1.930632e+06  9.878477e+05  1.064928e-02     0.023618   \n",
       "       6.545455   2.295358e+06  1.066046e+06  2.039587e-03     0.024896   \n",
       "       7.272727   2.607945e+06  1.148486e+06 -5.176000e-05     0.026718   \n",
       "       8.000000   2.831884e+06  1.234359e+06  8.484848e-04     0.028866   \n",
       "       8.727273   2.952402e+06  1.324238e+06  9.419966e-04     0.030949   \n",
       "       9.454545   2.984271e+06  1.417551e+06  5.529986e-04     0.032988   \n",
       "       10.181818  2.970914e+06  1.516186e+06  2.197742e-04     0.034514   \n",
       "       10.909091  2.987557e+06  1.611874e+06  7.105052e-05     0.036296   \n",
       "       11.636364  3.008266e+06  1.691341e+06  3.509543e-05     0.040390   \n",
       "       12.363636  3.034849e+06  1.747380e+06  4.345195e-05     0.047821   \n",
       "       13.090909  3.067304e+06  1.779989e+06  3.005610e-05     0.058590   \n",
       "       13.818182  3.103826e+06  1.796379e+06  2.271013e-05     0.071670   \n",
       "       14.545455  3.140799e+06  1.810966e+06  1.527273e-05     0.085007   \n",
       "       15.272727  3.176869e+06  1.829157e+06  7.636364e-06     0.097830   \n",
       "       16.000000  3.213486e+06  1.846873e+06  2.290909e-06     0.111119   \n",
       "       16.727273  3.250926e+06  1.863875e+06 -3.818182e-07     0.125107   \n",
       "       17.454545  3.284256e+06  1.884446e+06 -7.636364e-07     0.135601   \n",
       "       18.181818  3.310738e+06  1.910968e+06  0.000000e+00     0.140271   \n",
       "       18.909091  3.329823e+06  1.943915e+06  0.000000e+00     0.138652   \n",
       "       19.636364  3.343155e+06  1.981861e+06  0.000000e+00     0.132141   \n",
       "       20.363636  3.354569e+06  2.021472e+06  0.000000e+00     0.123999   \n",
       "       21.090909  3.367628e+06  2.059655e+06  0.000000e+00     0.117255   \n",
       "...                        ...           ...           ...          ...   \n",
       "       50.909091  3.181909e+06  2.362881e+06  1.863882e-02     0.200737   \n",
       "       51.636364  3.169967e+06  2.366027e+06  2.202128e-02     0.205076   \n",
       "       52.363636  3.158024e+06  2.369172e+06  2.540373e-02     0.209416   \n",
       "       53.090909  3.146082e+06  2.372318e+06  2.878619e-02     0.213756   \n",
       "       53.818182  3.134140e+06  2.375463e+06  3.216864e-02     0.218095   \n",
       "       54.545455  3.122198e+06  2.378608e+06  3.555110e-02     0.222435   \n",
       "       55.272727  3.110256e+06  2.381754e+06  3.893355e-02     0.226774   \n",
       "       56.000000  3.098314e+06  2.384899e+06  4.231601e-02     0.231114   \n",
       "       56.727273  3.086372e+06  2.388044e+06  4.569846e-02     0.235454   \n",
       "       57.454545  3.074430e+06  2.391190e+06  4.908091e-02     0.239793   \n",
       "       58.181818  3.062488e+06  2.394335e+06  5.246337e-02     0.244133   \n",
       "       58.909091  3.050546e+06  2.397480e+06  5.584582e-02     0.248473   \n",
       "       59.636364  3.038604e+06  2.400626e+06  5.922828e-02     0.252812   \n",
       "       60.363636  3.026661e+06  2.403771e+06  6.261073e-02     0.257152   \n",
       "       61.090909  3.014719e+06  2.406917e+06  6.599319e-02     0.261491   \n",
       "       61.818182  3.002777e+06  2.410062e+06  6.937564e-02     0.265831   \n",
       "       62.545455  2.990835e+06  2.413207e+06  7.275810e-02     0.270171   \n",
       "       63.272727  2.978893e+06  2.416353e+06  7.614055e-02     0.274510   \n",
       "       64.000000  2.966951e+06  2.419498e+06  7.952301e-02     0.278850   \n",
       "       64.727273  2.955009e+06  2.422643e+06  8.290546e-02     0.283189   \n",
       "       65.454545  2.943067e+06  2.425789e+06  8.628792e-02     0.287529   \n",
       "       66.181818  2.931125e+06  2.428934e+06  8.967037e-02     0.291869   \n",
       "       66.909091  2.919183e+06  2.432079e+06  9.305282e-02     0.296208   \n",
       "       67.636364  2.907241e+06  2.435225e+06  9.643528e-02     0.300548   \n",
       "       68.363636  2.895298e+06  2.438370e+06  9.981773e-02     0.304888   \n",
       "       69.090909  2.883356e+06  2.441516e+06  1.032002e-01     0.309227   \n",
       "       69.818182  2.871414e+06  2.444661e+06  1.065826e-01     0.313567   \n",
       "       70.545455  2.859472e+06  2.447806e+06  1.099651e-01     0.317906   \n",
       "       71.272727  2.847530e+06  2.450952e+06  1.133476e-01     0.322246   \n",
       "       72.000000  2.835588e+06  2.454097e+06  1.167300e-01     0.326586   \n",
       "\n",
       "                          target                              \\\n",
       "                 Acetyl-CoA (uM) HMG-CoA (uM) IPP/DMAPP (uM)   \n",
       "Strain Time (h)                                                \n",
       "L2     0.000000         0.014010     0.008557       0.000375   \n",
       "       0.727273         0.014010     0.008557       0.000976   \n",
       "       1.454545         0.014010     0.008557       0.002179   \n",
       "       2.181818         0.014010     0.008557       0.002299   \n",
       "       2.909091         0.014010     0.008557       0.003982   \n",
       "       3.636364         0.014974     0.008782       0.009273   \n",
       "       4.363636         0.014814     0.008745       0.015525   \n",
       "       5.090909         0.010312     0.007691       0.020816   \n",
       "       5.818182         0.002594     0.005886       0.022547   \n",
       "       6.545455        -0.005766     0.003930       0.022643   \n",
       "       7.272727        -0.012198     0.002426       0.025193   \n",
       "       8.000000        -0.014127     0.001974       0.029570   \n",
       "       8.727273        -0.013323     0.002162       0.033947   \n",
       "       9.454545        -0.013001     0.002237       0.037609   \n",
       "       10.181818       -0.013001     0.002237       0.038262   \n",
       "       10.909091       -0.013001     0.002237       0.034427   \n",
       "       11.636364       -0.013001     0.002237       0.027472   \n",
       "       12.363636       -0.013001     0.002237       0.019252   \n",
       "       13.090909       -0.013001     0.002237       0.012296   \n",
       "       13.818182       -0.013001     0.002237       0.009134   \n",
       "       14.545455       -0.013001     0.002237       0.009451   \n",
       "       15.272727       -0.013049     0.002303       0.010109   \n",
       "       16.000000       -0.013167     0.002467       0.010173   \n",
       "       16.727273       -0.012884     0.002073       0.010018   \n",
       "       17.454545       -0.011940     0.000760       0.009503   \n",
       "       18.181818       -0.010714    -0.000946       0.008832   \n",
       "       18.909091       -0.009582    -0.002522       0.008213   \n",
       "       19.636364       -0.008922    -0.003441       0.007852   \n",
       "       20.363636       -0.008899    -0.003474       0.007839   \n",
       "       21.090909       -0.009040    -0.003277       0.007916   \n",
       "...                          ...          ...            ...   \n",
       "       50.909091       -0.002570    -0.000574      -0.015972   \n",
       "       51.636364       -0.002570    -0.000574      -0.015972   \n",
       "       52.363636       -0.002570    -0.000574      -0.015972   \n",
       "       53.090909       -0.002570    -0.000574      -0.015972   \n",
       "       53.818182       -0.002570    -0.000574      -0.015972   \n",
       "       54.545455       -0.002570    -0.000574      -0.015972   \n",
       "       55.272727       -0.002570    -0.000574      -0.015972   \n",
       "       56.000000       -0.002570    -0.000574      -0.015972   \n",
       "       56.727273       -0.002570    -0.000574      -0.015972   \n",
       "       57.454545       -0.002570    -0.000574      -0.015972   \n",
       "       58.181818       -0.002570    -0.000574      -0.015972   \n",
       "       58.909091       -0.002570    -0.000574      -0.015972   \n",
       "       59.636364       -0.002570    -0.000574      -0.015972   \n",
       "       60.363636       -0.002570    -0.000574      -0.015972   \n",
       "       61.090909       -0.002570    -0.000574      -0.015972   \n",
       "       61.818182       -0.002570    -0.000574      -0.015972   \n",
       "       62.545455       -0.002570    -0.000574      -0.015972   \n",
       "       63.272727       -0.002570    -0.000574      -0.015972   \n",
       "       64.000000       -0.002570    -0.000574      -0.015972   \n",
       "       64.727273       -0.002570    -0.000574      -0.015972   \n",
       "       65.454545       -0.002570    -0.000574      -0.015972   \n",
       "       66.181818       -0.002570    -0.000574      -0.015972   \n",
       "       66.909091       -0.002570    -0.000574      -0.015972   \n",
       "       67.636364       -0.002570    -0.000574      -0.015972   \n",
       "       68.363636       -0.002570    -0.000574      -0.015972   \n",
       "       69.090909       -0.002570    -0.000574      -0.015972   \n",
       "       69.818182       -0.002570    -0.000574      -0.015972   \n",
       "       70.545455       -0.002570    -0.000574      -0.015972   \n",
       "       71.272727       -0.002570    -0.000574      -0.015972   \n",
       "       72.000000       -0.002570    -0.000574      -0.015972   \n",
       "\n",
       "                                                                        \n",
       "                 Intracellular Mevalonate (uM) Limonene g/L Mev-P (uM)  \n",
       "Strain Time (h)                                                         \n",
       "L2     0.000000                       0.471134     0.000579   0.009681  \n",
       "       0.727273                       0.462692     0.000599   0.009552  \n",
       "       1.454545                       0.445809     0.000640   0.009294  \n",
       "       2.181818                       0.444121     0.000645   0.009268  \n",
       "       2.909091                       0.420485     0.000702   0.008907  \n",
       "       3.636364                       0.346199     0.000884   0.007771  \n",
       "       4.363636                       0.258407     0.001098   0.006429  \n",
       "       5.090909                       0.184122     0.001280   0.005293  \n",
       "       5.818182                       0.148374     0.001332   0.004695  \n",
       "       6.545455                       0.152741     0.001339   0.004788  \n",
       "       7.272727                       0.168413     0.001460   0.005261  \n",
       "       8.000000                       0.181296     0.001659   0.005796  \n",
       "       8.727273                       0.194178     0.001857   0.006330  \n",
       "       9.454545                       0.210835     0.001988   0.006689  \n",
       "       10.181818                      0.215695     0.002000   0.006724  \n",
       "       10.909091                      0.175022     0.002001   0.006700  \n",
       "       11.636364                      0.089902     0.002071   0.006827  \n",
       "       12.363636                     -0.010695     0.002153   0.006977  \n",
       "       13.090909                     -0.095816     0.002223   0.007104  \n",
       "       13.818182                     -0.134507     0.002255   0.007162  \n",
       "       14.545455                     -0.130638     0.002252   0.007156  \n",
       "       15.272727                     -0.126842     0.002262   0.007043  \n",
       "       16.000000                     -0.136698     0.002305   0.006790  \n",
       "       16.727273                     -0.113044     0.002202   0.007398  \n",
       "       17.454545                     -0.034198     0.001859   0.009423  \n",
       "       18.181818                      0.068301     0.001413   0.012056  \n",
       "       18.909091                      0.162916     0.001001   0.014487  \n",
       "       19.636364                      0.218108     0.000760   0.015905  \n",
       "       20.363636                      0.220079     0.000752   0.015956  \n",
       "       21.090909                      0.208252     0.000803   0.015652  \n",
       "...                                        ...          ...        ...  \n",
       "       50.909091                      0.045917    -0.000013   0.006162  \n",
       "       51.636364                      0.045917    -0.000013   0.006162  \n",
       "       52.363636                      0.045917    -0.000013   0.006162  \n",
       "       53.090909                      0.045917    -0.000013   0.006162  \n",
       "       53.818182                      0.045917    -0.000013   0.006162  \n",
       "       54.545455                      0.045917    -0.000013   0.006162  \n",
       "       55.272727                      0.045917    -0.000013   0.006162  \n",
       "       56.000000                      0.045917    -0.000013   0.006162  \n",
       "       56.727273                      0.045917    -0.000013   0.006162  \n",
       "       57.454545                      0.045917    -0.000013   0.006162  \n",
       "       58.181818                      0.045917    -0.000013   0.006162  \n",
       "       58.909091                      0.045917    -0.000013   0.006162  \n",
       "       59.636364                      0.045917    -0.000013   0.006162  \n",
       "       60.363636                      0.045917    -0.000013   0.006162  \n",
       "       61.090909                      0.045917    -0.000013   0.006162  \n",
       "       61.818182                      0.045917    -0.000013   0.006162  \n",
       "       62.545455                      0.045917    -0.000013   0.006162  \n",
       "       63.272727                      0.045917    -0.000013   0.006162  \n",
       "       64.000000                      0.045917    -0.000013   0.006162  \n",
       "       64.727273                      0.045917    -0.000013   0.006162  \n",
       "       65.454545                      0.045917    -0.000013   0.006162  \n",
       "       66.181818                      0.045917    -0.000013   0.006162  \n",
       "       66.909091                      0.045917    -0.000013   0.006162  \n",
       "       67.636364                      0.045917    -0.000013   0.006162  \n",
       "       68.363636                      0.045917    -0.000013   0.006162  \n",
       "       69.090909                      0.045917    -0.000013   0.006162  \n",
       "       69.818182                      0.045917    -0.000013   0.006162  \n",
       "       70.545455                      0.045917    -0.000013   0.006162  \n",
       "       71.272727                      0.045917    -0.000013   0.006162  \n",
       "       72.000000                      0.045917    -0.000013   0.006162  \n",
       "\n",
       "[100 rows x 27 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acetyl-CoA (uM) Mean Error: -0.881491046611 Error Standard Deviation: 0.214991743726\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAwYAAAETCAYAAACMQhHVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11e0d5b70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HMG-CoA (uM) Mean Error: -2.38167878788e-05 Error Standard Deviation: 0.00310934465661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/matplotlib/axes/_base.py:2917: UserWarning: Attempting to set identical left==right results\n",
      "in singular transformations; automatically expanding.\n",
      "left=0.0, right=0.0\n",
      "  'left=%s, right=%s') % (left, right))\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAyEAAAETCAYAAAAhwwl8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRc+yqUp8NsoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11e4b1908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IPP/DMAPP (uM) Mean Error: 0.0856413854016 Error Standard Deviation: 0.0229244033383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/matplotlib/axes/_base.py:2917: UserWarning: Attempting to set identical left==right results\n",
      "in singular transformations; automatically expanding.\n",
      "left=0.0843005579931, right=0.0843005579931\n",
      "  'left=%s, right=%s') % (left, right))\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAwwAAAETCAYAAACbYIEcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/g+orHiFFlTi9AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11e404c18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intracellular Mevalonate (uM) Mean Error: 0.95326624746 Error Standard Deviation: 0.507250965327\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAwwAAAETCAYAAACbYIEcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11e023278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limonene g/L Mean Error: 0.000311072556645 Error Standard Deviation: 0.00126062252418\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxEAAAETCAYAAABJMItEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQgghgiKdCCGEEEIIIURQfgMNOiXXNWQscgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11e0239b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mev-P (uM) Mean Error: -0.0589811166529 Error Standard Deviation: 0.0937493078648\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAwQAAAETCAYAAACIt8HoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzs3XlcVPX++PHXLDCsw6JoKmKKopn5c7fFFSWXVCBU1C5l\neqvbLc28VyOz7Lp381ZmZTfv1xZtwWtmeq9ZKe55cTe1XCsKNUFZh22Gmfn9gTOCDMyADAPM+/l4\n+JCZc+ac92cG5pz3Z1WYzWYzQgghhBBCCLekdHUAQgghhBBCCNeRhEAIIYQQQgg3JgmBEEIIIYQQ\nbkwSAiGEEEIIIdyYJARCCCGEEEK4MUkIhBBCCCGEcGOSEFRTWloaHTt25KGHHqqw7fnnn6djx45k\nZmbWyrk2bNhAz549iY6OJiYmhujoaCZMmMDRo0crfc2yZcvYs2eP3eN27NiR5cuXl3vebDYzZMgQ\nRo0aBcDJkyd58cUXKz1Ox44dGT16NNHR0YwZM4YxY8bwf//3f9Uo4Q0vvPAC3333XY1e+9tvvzFt\n2jQArly5woQJE2p0nJulpaVxxx13EB0dXeGfXq+vlXM4et6oqCgSEhL47bffanzcxx57jPPnz1d4\nfuvWrSQkJNT4uCkpKdbfmZvZ+nvYsGEDTzzxBADLly9n48aNVR7/rbfeYtu2bTWOTwhRNbmu3VD2\nuhYTE8OoUaP45z//WY0S2jZ//nxWrFgBVP5dXNaUKVOq/Z6fOHGCyMjIGsdYHZmZmXTt2pWXXnrJ\nof3z8vJ4+OGHb+mciYmJNb7HEPapXR1AQ6TRaPjll1+4ePEirVq1AqCgoIDDhw/X+rl69epV7sso\nOTmZadOmsXPnTtTq8h/fsWPHOH/+PH/961/tHrdly5Zs3ryZZ555xvrcoUOHKCoqwtvbG4AuXbrw\n8ccfs2PHDgYPHmzzOB9++CHBwcFA6RfEn/70JxQKBVOmTKlWORctWlSt/cu6dOkSP//8MwDNmzfn\ns88+q/Gxbubl5cWXX35Za8er6XnNZjMLFy7k9ddf57XXXqvRMVetWlVb4dWasr9/lUlJSaF9+/Z1\nEI0Q7kuuazeUva7pdDqio6OJiIiodP/qcuS7eN++fbVyLmf5/PPPGTJkCP/973+ZOXMmgYGBVe6f\nk5PDiRMn6ig6URPSQlADKpWKESNGsHnzZutz33zzDUOGDCm3X3JyMuPGjSMmJsZaA2IymRg4cGC5\nP4xnn32WTz75xKFz33PPPWRkZJCbm1th24oVK4iPjwcq1tre/DgiIgIfHx+OHDlife6LL75gzJgx\n5Y4ZHx9focalMsHBwSQmJrJ69Wos692tXLmS2NhYoqOj+fOf/8yVK1cASEhI4Omnn2bkyJGsWbOG\nhIQEtm7dymuvvcb8+fOtx9y9ezfjxo0D4N1332Xs2LGMHj2aoUOH8u2332I0Gpk7dy6//vorU6dO\nJS0tje7du9t9nyuLqzo2bNjApEmTiI2NJSEhocJjgLfffpuRI0cyevRopk+fTkZGhs3y21NcXExG\nRgYBAQEA6PV6Fi9eTGxsLGPGjCExMRGdTgfAJ598wpgxY4iLi2PSpEnWmqjIyEjr+7F8+XKGDh3K\n2LFj+fbbb63nubkGpuzjHTt2MGHCBB588EEGDRrEG2+8Ue337GZlj//mm28yevRoHnzwQaZOnUp6\nejoff/wxJ0+e5O9//zvffvstP//8M48++ijx8fEMHjyYJ598kuLiYgB27dplrdlLTExkwIABpKWl\nVfhcCgoKmD17NuPHj2fYsGE8+OCD/PTTT0Dp57J06VJiYmLo378/q1atYunSpTz44IOMGDGCM2fO\n3HKZhaiP5Lpmm5+fH126dOGnn34iJSWFMWPGMGHCBMaMGYNer7f5fkBpIvHMM88wbNgwEhISrN8x\nUP67eP369TzwwAOMHj2ahx9+mMuXL/P8888D8Mgjj3D58mWuXLnCU089xYMPPsjo0aN59913rcf6\n5JNPGDZsGHFxcZW+35VdV0tKSpg3b571e3f69Onk5+fbfU9MJhNJSUnExsbSq1cvkpKSym3/5z//\nyfDhwxk1ahRPPfUUeXl5PP/88xQVFREdHY3RaKzQ6mR5bDKZWLhwIePGjWPkyJGMGDHCKUmpqEgS\nghqKiYlh06ZN1scbN24kNjbW+viXX37h9ddf57333mPjxo0sWLCAadOmUVRURFxcHF988QVQmjV/\n9913jB492u45zWYzSUlJREREWGsvLHJzczl8+DD33XdftcpgqYUuLCzk8OHD9O/fv9w+3bp148qV\nKw53VenUqRMZGRlkZWWxceNGzp49y7///W++/PJLBg4cyNy5c637arVatmzZUq67yrhx49iyZYu1\nS86GDRsYP348Fy9e5LvvvmPt2rVs3ryZZ599ljfffBOVSsXChQsJCwsrdyOrVCorfZ/txVWW5Qus\n7L+//e1v1u3nz59nzZo11pv6so8///xz9uzZw/r169m8eTMdOnQgMTGxyvLffN7Ro0dz7733Ehsb\nS9u2ba21ZO+99x4qlYoNGzawadMmmjVrxrJlyzAajSxevJh//etffP7554wfP77Cl+m2bdv45ptv\n2LhxI5999pk1kaiK2Wxm9erVLF26lA0bNpCUlMR7773nUJP2I488Uu79e/PNNyvsc/nyZT788EM+\n//xzNmzYwH333cf333/PQw89RJcuXZg9ezZRUVGsW7eOmJgYkpKS+Oabb0hLS2Pnzp1kZWUxe/Zs\nXn31Vb788kv69u1bLskr+7ns3r0brVbLunXr+Prrr601hhYXL15k48aNvPXWWyxbtow+ffqwYcMG\n+vfvz9q1a+2WV4iGSq5rFf30008cPHiQ3r17A3Du3Dn+8Y9/sGnTJi5dumTz/SgoKODNN9/Ey8uL\nrVu3snz5cmsrdlmnT59m2bJl/Otf/2Lz5s1ERkaycuVKlixZApS2VLRo0YJZs2YRFxfHhg0bWL9+\nPd999x1btmzhxx9/5K233mLt2rV8/vnneHh42CxDZdfVY8eOceDAATZt2sSGDRto3bq1Q5Uee/bs\nobCwkHvvvZeYmBg+/vhjDAYDANu3b7deI/7zn/8QGhrK2rVrWbJkibXlW6VSVXrs48ePk56eTlJS\nElu2bCE2NrZetm43RtJlqIa6dOmCUqnk5MmTNGnShPz8fCIiIqzb9+3bR3p6OpMnT7Y+p1Ao+PXX\nX4mLi2Ps2LEkJibyn//8h8GDB+Pv72/zPIcOHSI6OhqFQoFer6ddu3Y2b6hSU1MJCQnB09PT4TJY\nalPnzp3Lt99+S2RkpM0/1NatW/Pzzz/TunVru8dUKBRAafPzjh07OHHiBHFxcUBprUJhYaF13169\netk8V6dOnUhOTuaee+5h//79LFq0CF9fX1555RU2b95Mamoqx48ft1uTUdn7bC+usux1GerYsSN+\nfn42H+/evZsHH3wQHx8fAB5++GHeffdd65eyrfLbOu+ePXuYNWsW9913H76+vgDs3LmTvLw867gL\ng8FAkyZNUKlUDB8+nAkTJjBo0CDuu+++Chfl/fv3ExUVZY0zLi7ObiuFQqHg3XffZefOnfznP//h\nwoULmM3mSt+3sso2v0Ppxejrr78ut0/z5s3p1KkTsbGxDBgwgAEDBnDPPfdUONasWbPYt28fq1at\n4pdffiE9PZ2CggIOHTpEeHg4nTp1AiA2NpaFCxdaX1f2cxk+fDitW7dmzZo1pKamcuDAAbp3727d\nNyoqCsD6+265mQgLC+PAgQN2yytEQyXXtVKPPPIISqUSk8mEt7c3s2fPpmvXrqSkpNCiRQtrl6qq\n3o/9+/czZ84cFAoFwcHB1u+Vsvbv30+/fv1o0aIFQLnjWBQUFHDw4EFycnKsrRoFBQWcPn2a33//\nnfvuu4+QkBCgtOVj7969Nstq67pqNBpRqVSMGzeOfv36MWzYMLp27Wr3Pf70008ZPXo0arWaIUOG\nMG/ePLZu3cro0aPZv38/w4cPt7ZmW1o70tLS7B4XoHv37gQEBPDZZ5/x22+/kZKSYr3uCeeShOAW\njBkzhk2bNhEcHEx0dHS5bSaTiXvuuadct4rLly/TrFkzVCoVnTt3ZufOnWzYsIE5c+YApQON0tPT\nAZg+fTpQsa9lZZRKJUaj0fpYoVBYu+0A1uy9rJCQEDp37syuXbvYuHEjiYmJZGVlVdjP8qXhiBMn\nThAaGoqvry8mk4k//vGPTJo0CSjt5pKTk2Pd13KjfLNx48axceNGrl27RlRUFL6+vpw6dYo///nP\nTJ48mfvuu4/evXuXq6m3pVWrVjbfZ3txVcfNZSj7uOz7bzlvSUlJpa+tTP/+/Xn00UeZOXMmX331\nFf7+/phMJubMmcPAgQMByM/Pt3adWbZsGWfPnuW7775j1apVrF+/npUrV1qPd/PvRtnPtrLfm4KC\nAmJjYxk6dCi9evUiLi6Obdu2VShjTSmVStauXcuJEyfYv38/ixcvpm/fvhVabmbOnInRaGTEiBEM\nGjSIy5cvYzabUalUFWJRKm80gJZ9rz/55BPWrVvHQw89xOjRowkMDCx3sbr55qOyWjchGiO5rlWs\nxCir7HdJVe8HUOn3bNnnLJVoUNoyfPHiRcLDw8udw2w289lnn1nHQWRmZqLRaFi3bp3dc1jYuq4C\nfPnllxw5coT//e9/zJgxg4cffthmYmJx8eJFdu3axalTp/jmm28AKCkp4cMPP2T06NEVypSbm2uz\nK1hZZSfp2LlzJ4sWLeLRRx9lyJAhtGvXrlyrlXAe6TJ0C6Kjo9m6dStbtmypMMvK3Xffzb59+7hw\n4QJQ2r95zJgx1pu28ePHs2rVKoqKiujZsydQOtDoyy+/5Msvv6zQb9Oe1q1bk5mZaT1+cHAwly5d\n4tq1a5jN5kpnaYmJieH9998nLy+vXE2Qhdls5uLFi7Rt29ZuDFeuXGHZsmXWAcX9+vVj/fr11i4p\ny5cvZ/bs2XaPExUVxalTp1i3bh3jx48H4ODBg3Tp0oVHH32UPn36sH37duuFQqVS2bwwgO33uaZx\nVVe/fv3YsGEDBQUFAKxZs4bevXtXq7bLYsqUKWi1WmstWr9+/fj444/R6/WYTCZefPFFXnvtNTIz\nMxk4cCCBgYFMnjyZGTNmVGgC7t+/P1u3biU3NxeTyVSuBSQoKIiTJ08CpRedQ4cOAaU1dTqdjhkz\nZhAZGcmBAwes564Np0+fZtSoUYSHh/PEE08wefJka9wqlcqaSO3du5ennnqKkSNHolAoOH78OEaj\nkR49evDLL79w+vRpAL7++mtyc3PLXZgs9u7dS2xsLOPGjaNt27YkJyeXu+kQwp3Jdc1xVb0f/fv3\nZ/369ZhMJnJycti+fXuF1/ft25f9+/dbE6bPPvuMV199Fbjxvefn50e3bt14//33gdIb7IkTJ7J9\n+3buvfde9u3bx++//w5g7bJli63r6o4dO5g8eTLdu3dn2rRpxMTEWL9DK5OUlETPnj3Zs2cPycnJ\nJCcns2HDBn744QcOHz7Mvffey7fffmu9vq5YsYIPPvgAtVqN0Wi0JjDBwcHWcRRlx7Ht27ePwYMH\nM2nSJO666y62bdsm3891RFoIbkHz5s0JDw/H39+/wgj7Dh06MH/+fGbOnInZbEatVrNy5Upr7UJk\nZCR/+9vfeOyxx2olFq1WS8+ePfnf//7HwIEDad++PRMmTCAuLo6QkBAGDRpk83VDhw5l3rx5PPvs\nsza3nzhxgrCwMFq2bGlzu6Vp1VIzERcXZ526bty4cVy5coXx48ejUCho0aIFS5cutVsWT09PRo4c\nyXfffWdtvhw1ahTffPMNI0eOxMPDg3vuuYecnBx0Oh0dOnRApVIxduxYXn/99XLHsvU+VycuS1/+\nmzlSjrFjx3L58mXGjRuHyWSiTZs2LFu2zO7rbPHw8ODFF1/kj3/8I+PGjePPf/4zr7zyCrGxsRiN\nRu644w4SExPx8/PjySefZPLkyXh5eVnHWJQ1cOBAzpw5Q1xcHFqtlk6dOllr0BISEvjrX//KsGHD\nCA0NpU+fPkBpl5tBgwYxYsQItFotYWFhtG/fntTU1BolODfr1KkTI0aMIC4uDh8fH7y8vKytA4MH\nD+aVV17BYDDw7LPP8tRTTxEQEIC3tze9e/fm119/JTAwkNdee43nnnsOpVJJly5dUKvV1hq1sqZM\nmcJLL73Ehg0bUKlU3HnnnZw9e/aWyyBEYyDXNcdV9X5MmzaNefPmMWLECIKDg20mJh07dmTWrFn8\n8Y9/BEpbNxYvXgyU3sBPmjSJd955h2XLlrFgwQJGjx6NXq9n1KhR1oHSs2bN4pFHHsHX17fK7j62\nrqsDBgxg9+7djBo1Ch8fHwICAliwYAFQOhV4ly5dmDhxovUYer2e9evXW2O0uP3223nggQf48MMP\nefPNNzl//rz1de3bt2fBggV4e3vTuXNnRowYwaeffsrcuXOZP38+Wq2We++919rtacKECfz1r3+1\ntjb06tWLb775ptYqn0TlFObaavMXLnfkyBHeffdd3nvvvVo7ZmJiIsOHD6/0i1eI+kCn0/HOO+8w\nbdo0vL29OXXqFE888QR79uyx2UoghGgY5LomRN2QFoJGpEePHrRt25bdu3czYMCAWz7eiRMnUCgU\n8qUp6j0/Pz88PDwYO3YsarUatVrNG2+8IcmAEA2cXNeEqBvSQiCEEEIIIYQbk0HFQgghhBBCuDGn\nJgTHjx+vsOjS5s2brasOCiGEEEIIIVzLaWMIVq1axaZNm8rN8vHDDz+wfv16h+cuz8jIs7tPUJAP\nWVkFNY6zPmpsZWps5YHGVyYpT/1XWZlCQmwv/uRuHLle1BeN8ffTluqWc+PG9Vy4cI7HHvszWm2A\nEyOrXe7webpDGaHxl7Oq64XTEoKwsDBWrFhhnd89KyuL1157jTlz5vDiiy86dIygIB/UavsLYjXG\nC2JjK1NjKw80vjJJeeq/xlgmd+TIda0xqG45f/nlJ3744SSenhonReQc7vB5ukMZwX3KaYvTEoJh\nw4ZZV/80Go288MILPP/882g0jv+hO5KlhYT4N6iaIUc0tjI1tvJA4yuTlKf+q6xMkiSIxiInJweN\nRoOXl5erQxHC7dTJoOJTp06RmprKyy+/zMyZMzl//jyLFi2qi1MLIYQQogHIyckmICDQ/o5CiFpX\nJ+sQdO3alf/+978ApKWlMXPmTF544YW6OLUQQggh6jmTyUReXi4hIe1dHYoQbkmmHRVCCCGES+Xl\n5WEymQgIaDiDiYVoTJyaEISGhrJu3Tq7zwkhhBDCfZlMRnr06EV4eISrQxHCLdVJlyEhhBBCiMoE\nBQXz1FPPujoMIdyWdBkSQgghhBDCjUlCIIQQQgiXOnHiOBs3rufq1QxXhyKEW5KEQAghhBAudfLk\n92ze/AX5+TpXhyKEW5IxBI3czmMXHdpvULdWTo5ECCGEsC0nJwsArVbWIRDCFaSFQAghhBAulZOT\ng0KhQKvVujoUIdySJARCCCGEcKmcnBz8/f1RqVSuDkUItyQJgRBCCOFixQYj6VkFFBuMrg7FJXJy\nsqS7kBAuJGMIhBBCCBcxmkwkJZ/n6NkMMnOLCdZq6B4RQnxke1RK96izMxgMaDReBAUFuzoUIdyW\nJARCCCGEiyQln2fboTTr42u5xWw7lIbRZGZY79YE+GnQeDTubjQeHh689trbmM1mV4cihNuShEAI\nIcQtM5lMvPzyy5w5cwZPT08WLlxImzZtrNuTk5N5++23UavVxMXFMX78eAwGA3PmzOHixYvo9Xqe\nfPJJhgwZQmpqKomJiSgUCjp06MC8efNQNsLa8mKDkaNnbc+7v+voRXYcuUgTN2oxUCgUrg5BCLfV\nuL9dhBBC1Ilt27ah1+tJSkriL3/5C0uXLrVuMxgMLFmyhNWrV7NmzRqSkpK4evUqmzZtIjAwkE8+\n+YR//etfLFiwAIAlS5YwY8YMPvnkE8xmM9u3b3dVsZwqR1dMZm6xzW2m65XllhaDpOTzdRhZ3bp6\nNYMTJ46Rk5Pj6lCEcFvSQiCEEOKWHT58mP79+wPQrVs3Tp48ad124cIFwsLCCAgIAKBnz54cPHiQ\n4cOHM2zYMADMZrN1hplTp07Rp08fAAYMGMC+ffuIioqq8vxBQT6o1a7pWlOkLyErt5ggrQYvT8cu\nqyEh/vgHeBMS5E16VqHd/b+/cI0n4rwdPn59ERLib3eflJRdvPfee8yaNYv27fvXQVS1z5FyNnTu\nUEZwn3LerGF9swghhKiXdDodfn5+1scqlYqSkhLUajU6nQ5//xsXWV9fX3Q6Hb6+vtbXTp8+nRkz\nZgClyYGl+4ivry95eXl2z5+VVVCbxbErr0BP6pU8Un74nVM/Z5Gt0+PnraZTm2Amj+iIj8aj0teG\nhPiTkVFapq7hTcqNIahMRnYhF365RrMgn1org7OVLWdVLl68AoBCoXFo//rG0XI2ZO5QRmj85awq\n2ZGEQAghxC3z8/MjPz/f+thkMqFWq21uy8/PtyYIly9f5qmnnmLSpEmMHj0aoNx4gfz8/Hq1WJW+\npIRFHx3hYobO2q3HQldYwqHT6Rw6nc6gHi15aGiE3X7/8ZHtATh6NoNrlXQfAjCbYcWGE7z4SE88\n1Y3r0p2bW9pVKCBAph0VwlVkDIEQQohb1qNHD3bv3g3AsWPHiIiIsG4LDw8nNTWV7Oxs9Ho9hw4d\nonv37ly9epUpU6Ywa9Ysxo4da92/c+fOpKSkALB792569epVt4WpwqKPjvBbesVk4GY7j1xyqN+/\nSqlk0tAIuoY3sbvvxYx8Fn10xNFQG4zs7GwAa5cyIUTdk4RACCHELYuKisLT05MJEyawZMkSnn/+\neTZv3kxSUhIeHh4kJiYydepUJkyYQFxcHM2bN+fdd98lNzeXd955h4SEBBISEigqKuK5555jxYoV\nxMfHYzAYrOMMXC2vQM/FDJ3D+x8+ne7QQmPFBiPfX7jm0DEvZujIK9A7HENDkJubjaenBi8vb1eH\nIoTbalztjkIIIVxCqVQyf/78cs+Fh4dbf46MjCQyMrLc9rlz5zJ37twKx2rbti1r1651TqC3IM2B\nloGysnR6cnTFdvv9VzXb0M1M5tI47ri98SzilZ2dTUBAgEw7KoQLSUIghBBCOCC0mR9KBQ4nBUF+\nngT4aezuF+CnIVirqXIMgYVSURpHYzJnzssUFRW5Ogwh3Jp0GRJCCCEc4O/jSasQx2/Ge3Zq5tAq\nwxoPFd0jQhw6ZqsQP/x9PB2OoSFo2jSE0NDWrg5DCLcmCYEQQgjhoBce7kHr6y0FVRnUo6V1BiFH\nxEe2Z2ivUJpovVAqSlsX/LzV1vMoFdC6mR8vPNzjFqKvfwwGAwUF+ZjN1eiLJYSoddJlSAghhHCQ\np1rN36b04VpOIfM/PERegaHCPsH+GuIHd7A75WhZltmG4gaGk6MrJsBPg8ZDRV6BnrR0HaHNGl/L\nAMCZMz/y+uuv8OCD43nggWhXhyOE25IWAiGEEKKajCYzOhvJAEC2rpgcnWODhG+m8VDRLMjH2tXI\n38eTO24PbpTJAEBOTumUo1qtTDkqhCs5NSE4fvw4CQkJAPz4449MmjSJhIQEpk6dytWrV515aiGE\nEMJpLAOBbQny93JoMLGQhECI+sJpCcGqVauYO3cuxcWltSSLFi3ixRdfZM2aNURFRbFq1Spnndrt\npaXr2LjnJ3YcSStdTbM68+QJIYSwq6qBwN0jmjo0mFhATk7pKsWBgbJKsRCu5LQxBGFhYaxYsYLZ\ns2cD8Nprr9GsWTMAjEYjGo392pOgIB/UavtfqiEh/rcWbD10K2V68/MTHDuXYX08oFsr7mrf1Gnn\nc4R8RvWflKf+a4xlasjGDmrHmV+zSytezKUDf1uF+DF2UDtXh9Zg3GghkIRACFdyWkIwbNgw0tLS\nrI8tycCRI0dYu3YtH3/8sd1jZGUV2N0nJMSfjIy8mgdaD91KmUqMJn745RrNgrwZ1rs1a745y6+/\n53L7bVVPlefM91A+o/pPylP/VVYmSRJcZ/3On/gt/cbKxSYz/JauY/3On5g0NMKFkTUcOTnZKBQK\ntFqtq0MRwq3V6aDiLVu2MG/ePN577z2CgxvPKov1SeqVPPQGE51vD2ZQ91Z4eii5lisLvgghRG0q\nNhg5ejbD5rajZ69SbDDWcUQN04gRo3noocmoVNLFSghXqrNpR7/88kuSkpJYs2aN9BV0onO/lfbH\njGhdugx8E60Xl68VUGwwSp9WIYSoJTm6YjIrWVk4K6+IHF0xzYJ8Kn19scFYbnpRd9W1azdXhyCE\noI4SAqPRyKJFi2jRogXTpk0DoHfv3kyfPr0uTu9Wzv5W2h8zIrQ06WoaUJoQXMspomVTX1eGJoQQ\njYZllqFrNpKCqmYZMppMrNp4gn3HL5KZW0ywVkP3iBDiI9tXa90CIYSoTU5NCEJDQ1m3bh0ABw4c\ncOapBGAymzmXlk3TAC+CtV4ANAko/V8SAiGEqD2WWYa2HUqrsK2qWYaSks+Xe8213GLrY3cbd3D1\nagZvvPF37r23PyNHjnF1OEK4NamOaEQuZeSTX1RCROsbXbKsCYGMIxBCiFoVH9meob1CaaLVoFBA\nE62Gob1CiY9sb3N/GXdQXnZ2FpcvX6KgwP4EIkII55KEoBE5m3a9u1CZhMBHo8bLU8XVHEkIhBDC\nGcxmM2Zz6f9VcWTcgTvJzi69ZgUEyKJkQrhanQ0qFs5nHT9QJiFQKBQ0DfAiLSOfwuISvDXykQsh\nxK3IK9CTlq7jwOl0dh27ZH0+M09fZfefmo47aKxycy0JgUw0IoSryd1hA7Tz2MUKz5nNZk78lImX\np4ofUzM5/WuWdVuT6wnBtZwiQptVvR6BEEII2/QlJSz66Ih1IbLKHD17lbiB4RXGEdR03EFjZVml\nWBICIVxPugw1EkV6I4XFJTQN9EahUJTbJuMIhBDi1i366Ai/pVedDEDV3X/iI9szpn87mmi9UCqg\nidarynEHjZlllWLpMiSE60kLQSORV6AHIMDXo8K2JtdnHJJxBEIIUTOWbkKOCPLXVNr9R6VU8ljM\nXYzo09prdHEcAAAgAElEQVTt1yFo2zacwsJCaSEQoh6QhKCRyCswAODv41lhm/f1gcW5+fq6DksI\nIRqFtHQddhoGrLy91HZv8jUeqioXLnMHAwdGMnBgpKvDEEIgXYYaDcvNvtZGQgDg66WmoKjE7iwY\nQgghKmoW5O3wvhlZhW43hagQomGThKCRuNFCULHLEICPlwdGk1kuUkIIUQNGewMHyig2mMjILnRi\nNA2fyWRi9er32Llzu6tDEUIgCUGjkVugR6VU4ONluxeY5fmCopK6DEsI4SZMJhMvvfQS8fHxJCQk\nkJqaWm57cnIycXFxxMfHW1ewtzh+/DgJCQnWxz/88AP9+/cnISGBhIQEtmzZUidlqEqAn4Ym2mpM\nCyqtsVXKz9exb98uTp064epQhBDIGIJGwWw2k5dvwN/Ho8IMQxa+1xOC/KISgrV1GZ0Qwh1s27YN\nvV5PUlISx44dY+nSpaxcuRIAg8HAkiVLWL9+Pd7e3kycOJHIyEiaNm3KqlWr2LRpE97eN7rknDp1\nikcffZQpU6a4qjgVVDVl6M28PFWEuPn4AHtkhiEh6hdpIWgEivRGDEaTzQHFFj5epV2JCooMdRWW\nEMKNHD58mP79+wPQrVs3Tp48ad124cIFwsLCCAgIwNPTk549e3Lw4EEAwsLCWLFiRbljnTx5kp07\nd/LQQw8xZ84cdDrHZvdxtvjI9gztFWqdMlTjYfsSeu9dt6HxUFFsMJKeVSBdNW24sUqxzDAkRH0g\nLQSNgGXKUa2NKUctyrYQCCFEbdPpdPj53Vj4UKVSUVJSglqtRqfT4e/vb93m6+trvckfNmwYaWnl\na927du3KuHHj6NKlCytXruTtt9/mueeeq/L8QUE+qNXOn77zmYk9KdKXkJVbjNbXg0++PsP+E5e4\nml1E00Av7rmrJY+MvIMPt/zI/05eJiO7kJBAb+7u0oIpo+9EpSpNIkJC/O2cqXGorJxmc+k6DaGh\ntzWK96IxlMEedygjuE85byYJQSNQ1ZSjFjKGQAjhTH5+fuTn51sfm0wm1Gq1zW35+fnlEoSbRUVF\nodVqrT8vWLDA7vmzsgpqGnqNqIECXTEx991eYU2BlZ8fL9e1KD2rkE17fqKgUM+koRGEhPiTkZFX\np/G6QlXlTEv7HQClUtPg3wt3+DzdoYzQ+MtZVbIjXYYaAXtTjoIkBEII5+rRowe7d+8G4NixY0RE\nRFi3hYeHk5qaSnZ2Nnq9nkOHDtG9e/dKjzV16lS+//57APbv38+dd97p3OBvkWVNAUs3oaNnM2zu\nd/TsVek+dJ1SqSIoKJjAwCBXhyKEQFoIGoVcO1OOQunqmF6eKvJlDIEQwgmioqLYt28fEyZMwGw2\ns3jxYjZv3kxBQQHx8fEkJiYydepUzGYzcXFxNG/evNJjvfzyyyxYsAAPDw+aNm3qUAtBfZGjKyYz\nt9jmtqy8InJ0xYTWcUz10bBhIxk2bKSrwxBCXCcJQSOQZ2fKUQtfLzXZOj1ms7nS2YiEEKImlEol\n8+fPL/dceHi49efIyEgiI22vShsaGlpuKtI777yTzz77zDmBOlmAn4ZgrYZrNpKCIH8vAvyqMXWp\nEELUEeky1MA5MuWohSxOJoQQzmWZntSW7hFN0Xg4f+BzQ5CSsp+TJ793dRhCiOukhaCBc2TKUYuy\n4wi8POWjF0IIZ4iPbA+UjhnIyisiyN+L7hFNrc8LWLNmNU2aNKFLl66uDkUIgSQEDZ4jU45a+JZJ\nCGRxMiGEcA6VUsmkoRHEDQwvN/uQKKXX6yksLECrbefqUIQQ10lC0MA5MuWohWVxMhlYLIQQzmeZ\nfUiUZ1mlWGYYEqL+kDEEDZwlIfDzrl4LgRBCCOEKOTk5AGi1AS6ORAhhIQlBA5df6HhC4COrFQsh\nhHAxSwtBQECgiyMRQlg4NSE4fvw4CQkJAKSmpjJx4kQmTZrEvHnzMJlMzjy129BdTwh8q5EQSAuB\nEEIIV5GEQIj6x2kJwapVq5g7dy7FxaVzMS9ZsoQZM2bwySefYDab2b59u7NO7VZ0hQZ8vNSolPbX\nFZDFyYQQQrjagAGDefXVN+natZurQxFCXOe0QcVhYWGsWLGC2bNnA3Dq1Cn69OkDwIABA9i3bx9R\nUVFVHiMoyAe12v7MDCEh/rcecD1TVZn8/bwAMJrMFBSVcFtTX+tz9vj7epKZU4Sfr6bcugXOfg/d\n7TNqiKQ89V9jLJNwP2q1muDgJq4OQwhRhtMSgmHDhpGWlmZ9XHZ1XF9fX/Ly8uweIyurwO4+ISH+\nZGTYP1ZDYq9Mebqi0v8L9JgBb0+V9Tl7vDxUGE1mrmYV4OV5I9ly5nvojp9RQyPlqf8qK5MkCfVX\nscEo047acPVqBhqNBj8/f7sLagoh6obdhOBf//oX0dHRhITYXnnRUUrljd5J+fn5aLUyEf6t0lVj\nQLHFjXEEhnIJgRBCiNphNJlISj7P0bMZZOYWE6zV0D0ihPjI9qiUMpfHO+8s59KlNFaufN/VoQgh\nrrObEBQVFfGHP/yBNm3aEBsby9ChQ/HwcPwG1KJz586kpKTQt29fdu/ezd13312jgMUNNUkILElA\nkd7olJiEEA1DZmYmH3/8McnJyaSmpqJUKgkLC2PIkCFMnDiR4OBgV4fYYCUln2fboRst5Ndyi62P\nJw2NcFVY9UZubjYBAYHSOiBEPWK3quLpp5/m66+/5vHHHyclJYXo6Gjmz5/Pjz/+WK0TPffcc6xY\nsYL4+HgMBgPDhg2rcdCilK6wdLag6iUEpTmgJARCuK+PP/6YZ599luDgYJYuXcru3bvZu3cvf//7\n3wkMDOTpp5/mo48+cnWYDVKxwcjRsxk2tx09e5Vig3t/95pMJnJzc2WGISHqGYfGEBQWFpKWlsZv\nv/2GUqlEq9WycOFCevTowV/+8pdKXxcaGsq6desAaNu2LWvXrq2dqAVQvTUILG60EMjUo0K4q+bN\nm/Phhx9WeL59+/a0b9+ehx56iK+//toFkTV8ObpiMnOLbW7LyisiR1dMaB3HVJ/k5+swGo0EBMii\nZELUJ3YTgr/85S+kpKQwYMAAnnzySXr16gWAXq+nX79+VSYEwrnyCgwoFDfGBTjCSyNdhoRwd0OH\nDrW7j7Ti1kyAn4ZgrYZrNpKCIH8vAvw0Loiq/pA1CISon+zeSd5zzz0sWLAAHx8f63N6vR5PT0/+\n+9//OjU4UbX8QgM+GjVKB9YgsPCWLkNCuL0hQ4bYfN4yG5ysE1NzGg8V3SNCyo0hsOge0dTtZxvK\nyckBJCEQor6xmxD8+9//ZuzYsdbHJpOJuLg4Nm/efMszD4maM5pMFBSX0DzYu1qvs3YZKpYuQ0K4\nq44dO/Ljjz8yaNAgRo4cScuWLV0dUqMSH9keKB0zkJVXRJC/F90jmlqfd2ehoWE8+eR0WrRo5epQ\nhBBlVJoQPPzwwxw4cACATp063XiBWk1kZKTzIxNVyq/BgGIAD7USpUJaCIRwZ++88w46nY5t27bx\nf//3f+Tn5zN06FCGDx9O8+bNXR1eg6dSKpk0NIK4geGyDsFNAgIC6NWrr6vDEELcpNKEwDLDxMKF\nC5k7d26dBSQcU5MpRwEUCgVenmpJCIRwc35+fsTExBATE0Nubi7ffvstzzzzDGq1WiaAqCUaDxXN\ngnzs7yiEEC5WaUKwY8cOBg8ezJ133snGjRsrbI+JiXFqYKJqNU0IoHRgcW6+vrZDEkI0QJmZmXzz\nzTds3boVnU5HVFSUq0MSjdgHH6zizJkfmTPnZfz9ZYFSIeqLShOCEydOMHjwYGu3oZtJQuBat5QQ\neKrIzDVTYjShVsmqmUK4m4yMDGsSkJmZyf33309iYmK57qFCOENGRjrp6Vfw9paWEyHqk0oTgunT\npwOwZMkS63M6nY7Lly/ToUMH50cmqqQruJWE4MZMQ37ekhAI4W4GDBjAbbfdxv3330+nTp1QKBSc\nPn2a06dPA1LhI5wnJycbPz8/1GrHp8sWQjifQ7MMHTlyhFmzZhETE4Ovry/3338/zz77bF3EJyqh\nKzSgVIB3NdYgsCi7OFlNEgohRMMWHR2NQqEgNzfXZiuwJATCWXJysgkKCnZ1GEKIm9i9m/z0009Z\nvXo1mzZtYsiQIbzwwguMHz9eEgIXyysw4OftgVLh+BoEFjemHpWBxUK4o6VLl9b6MU0mEy+//DJn\nzpzB09OThQsX0qZNG+v25ORk3n77bdRqNXFxcYwfP9667fjx4yxbtow1a9YAkJqaSmJiIgqFgg4d\nOjBv3jyUSmnNbOgMBj0FBQXcfns7V4cihMsVG4z1ahYyh6qXAwMD2bVrFw8//DBqtZriYtvLsou6\nUWwwUmww0jTQq0av95LFyYQQQEJCAgoblQqWWeaqY9u2bej1epKSkjh27BhLly5l5cqVABgMBpYs\nWcL69evx9vZm4sSJREZG0rRpU1atWsWmTZvw9r6xpsqSJUuYMWMGffv25aWXXmL79u2NbrBzfbsZ\nqAuWRcm02gAXRyKEfc76GzWaTCQln+fo2Qwyc4sJ1mroHhFCfGR7VC6s+LCbELRv354nnniCtLQ0\n7rnnHp555hm6dOlSF7GJSuRdHz/g71Oz7j5luwwJIdzXtGnTrD+XlJSwfft2tNqazfxy+PBh+vfv\nD0C3bt04efKkdduFCxcICwsjIKD0RrBnz54cPHiQESNGEBYWxooVK5g9e7Z1/1OnTtGnTx+gdLzD\nvn377CYEQUE+qNX1/8Y6v1DP658e4cSFq1zNLiQk0Ju7u7Rgyug7UTXCSR5CQvytP6tUBqKjo2nf\nvn255xuDxlYeW1xRxiJ9CVm5xQRpNdbKTGcLDvZl9eZT/O/kZTKc8De6auOJciuZX8stZtuhNHy8\nPXks5q5bPn5N2X13Fy9ezNGjR+nQoQOenp5ER0czcODAuohNVCKvoHTKUH8fzxq93ktjSQikhUAI\nd2a56ba49957GTduHM8880y1j6XT6fDz87M+VqlUlJSUoFar0el0+PvfuJnw9fVFp9MBMGzYMNLS\n0sody2w2W1sufH19ycvLs3v+rKyCasdclyy1gnu/v0SR3mR9Pj2rkE17fqKgUM+koREujLD2hYT4\nk5FR9rPzYMyY0q5i5Z9v2CqWs2GzVTNe12V0VS16SIg/b607Wu6GvTb/RosNRvYdv2hz277jlxjR\np7VTWwyrSursJgQFBQWcPXuWAwcOYDabAfjhhx94+umnay9CUS2WFgJtjVsIpMuQEAIuXbpk/dls\nNnP+/Hmys7NrdCw/Pz/y8/Otj00mk3UmmZu35efnl0sQblZ2vEB+fn6NWy3qk6Tk8+VuMm529OxV\n4gaGu033IVH/VHUTXtdu/nux1KIDTk2ci/QlHD2bYXNbbfyN5uiKycy13e0+K6+IHF2xyxYztJsQ\nPPPMM/j7+9OhQwebfU1F3cvLv8UWAukyJIQA/vCHP6BQKKw18sHBwTVemb5Hjx7s2LGDkSNHcuzY\nMSIibly0w8PDSU1NJTs7Gx8fHw4dOsTUqVMrPVbnzp1JSUmhb9++7N69m7vvvrtGMdUXxQZjpTcZ\nFq6+GagL3323h1OnThAdHUezZs1dHY64SVU34U/E/T/SswrqZMxLVX8vzk6cs3Kde8Me4KchWKvh\nmo1zBPl7EeCnqfGxb5XdhODq1au8//77dRGLcFBugQGFAnxrOGWoWqVErVJIC4EQbi45ObnWjhUV\nFcW+ffuYMGECZrOZxYsXs3nzZgoKCoiPjycxMZGpU6diNpuJi4ujefPKbwife+45XnzxRV577TXa\ntWvHsGHDai1OV6iqVtDC1TcDdeHcuTP873/7eOCBaFeHIm5S1U343u8vc/zCNa5mFdZJ1x1X1qIH\naZ17w67xUNE9IsRma2H3iKYubSG0mxDccccdnD59WlawrEfyCvT4enmgUta8xcbLUy3Tjgrhpp5/\n/nkef/xx2rZta3P7uXPnWL16dbmFKe1RKpXMnz+/3HPh4eHWnyMjI4mMjLT52tDQUNatW2d93LZt\nW9auXevwueu7qmoFLVx9M1AXLLMMBQYGujgScbOqbsKL9EaK9IVA3XTdcWUtupen2uk37JYuWEfP\nXiUrr4ggfy+6RzR1SdessuwmBOfOnSM2NpYmTZqg0WisTcvbt2+vi/jETQqLSyjSG2nR5Nb+ILw8\nVWTmFpUbvCeEcA8zZsxg0aJFZGRk0LNnT2677TZUKhWXLl0iJSWF2267jcTERFeH2WhUVSvo5ami\nX9cWLr8ZqAu5udmo1R54ezfeblENlSNJa1nO7Lrj6lp0Z9+wq5RKJg2NIG5geL2aethuQvDWW2/V\nRRzCQRnZpVm61rdm4wcsvDRqTGYwlJjwrAe/iEKIutO8eXPefPNNfv31V3bs2MFPP/2EUqmkdevW\nLFu2jLCwMFeH2OjcfJMR6KehU5sgJkV1wEfjHivGZ2dnExAQIJVQ9VBVN+G2OLvrjitr0evqhl3j\noapXY4bsJgStWrVi8+bNnD9/nj/96U98/fXXsqy9C6VnlSYENV2DwOLGwGKjJARCuKmwsDAeeeQR\nV4fhFsreZKg8PTDqDfWiVrCumEwmcnNzaNPGdjc14Xq2ktaC670Sbubsrjv1oRa9vt2wO5vdhGDZ\nsmX8/vvvnDp1iscee4zPP/+c06dPS3Oyi1y5Ptd2TWcYsvC+nhAU6ktuubVBCCGEYzQeKkKa+jaq\neesdodfradu2HaGh0vpUX9m6Cf981wWXDoB1t5tyV7I7RHzv3r28+uqraDQa/Pz8eP/999m9e3dd\nxCZsqK0WAo2lhUAGFgshhHAyLy8vnn/+ZRISprg6FGGH5SZc46EiPrI9Q3uFEhLohQII9tcwtFeo\nW4x5cTd2EwLLAjGWPn96vb7cojGiblkTghpOOWohi5MJ4d4yMzM5efIkhYWFrg5FCFHfXb8HlOEf\njZfdLkPDhw9nxowZ5OTk8MEHH7Bp0yZGjRpVo5MZDAYSExO5ePEiSqWSBQsWlJuWTtiXnl2Ir5ca\nlerWkjJZnEwI9/XVV18xZ84cfHx8MJlMLF++nD59+rg6LLdQbDBy+Wo+RoPRrcYQ/PrrL5w/f5a7\n7upGSEgzV4cjHOSqFYNF3bObEDz++OPs2bOHli1bcvnyZaZNm8bgwYNrdLJdu3ZRUlLCZ599xr59\n+3jjjTdYsWJFjY7ljooNRrLyirkt+Nb705UdVCyEcC8rV65k/fr1hIeHs2fPHlasWMGaNWtcHVaj\nZjSZSEo+z9GzGWTmFRPop6FtC3/+cH9HAhv5gmQAP/xwkn//+1OmTWsqCUED4coVg0Xds5sQnD17\nlvz8fPr27Ut4eDitW7eu8cnatm2L0WjEZDKh0+lQq6s+fVCQD2q1/V+2kBD/GsdUX9kq0/nfsgFo\nGuiNv5/XLR1fqSp9X01m8Pfzcvp76C6fUUMm5an/aqtMCoXC2jrbv39//v73v9fKcUXlbq5pzcor\nJiuvmCNnr9K6mR+zJv4/CoqM9WZO8tqWk1N6/QoICHBxJMJRrlwxWNS9Su/Ir127xvTp0zl37hxt\n2rRBoVDw888/0717d5YtW4ZWq632yXx8fLh48SIjRowgKyuLd999t8r9s67PqFOVkBB/l8zWsPPY\nRYf2G9StVbWPXVmZjp+5AoCft5o8XVG1j1uW0WQCSlc9ztMVOfU9dNVn5EyNrUxSnvqvsjLVJEm4\neRyYvcoZUTPFBiM5umK8NepKa1oBfkvX8eyKfZhMEKzV0D0ihPjI9qiqOV7Pcr76mFTcSAhkleKG\nwpUrBou6V+lVYMGCBfTs2ZMPPvgAD4/SAax6vZ4VK1awePFili5dWu2TffDBB/Tr14+//OUvXL58\nmUceeYTNmzej0cgvlSN+vVJ6MxCsvfX3S6VU4qFWUixdhoRwO/n5+Rw6dAiz2QxAQUFBuce9e/d2\nZXgNXkFxCZ9+e5bTv2aRmVtMgJ8n2Tp9la8xltbRVNpHu6qb/XLdkXKLbympcJacnBwAtFppIWgo\nXL1isKhblSYEZ86c4Y033ij3nKenJzNnziQ6OrpGJ9NqtdbkIiAggJKSEoxGuSF11K9XdCgVCoJq\nKSvXeKhkDIEQbqh58+YsX77c+rhZs2bWxwqFgo8++shVoTVolhvzvd9fokhvsj5vLxmwxdJHW61S\n2L3ZbwgDP3NysvHz85PWqAbGMr3o9xeucTW7sE5XDBZ1q9K/zMpq7RUKRY2nHZ08eTJz5sxh0qRJ\nGAwGnn32WXx8pP+ZI0wmM7+l62jR1OeWZxiy8PJUkZlbbK0VFEK4BxlA7Bw335jfCksf7W2H06q8\n2a964GdGvRn4mZeXh1Yr3YUaGpVSSdzAcKIHtScrM5+Q6+sTiMan0oRAUcVks1Vtq4qvr2+5Winh\nuPTsQooNRsKa1d5ASY2nCpPZjMFosr+zEKJRi42N5YsvvnB1GA1WVTfmNRHk71Xl2IO9318mpn9b\ndAUGm328oTR5+HL3OXIKSmh7m5Y72wZz6VoBhhITzYK8CAn0obC4pE7GHLz++jsUFcmaFw3JzTNj\nBfvXv65oovZUmhCcO3eOIUOGVHjebDaTkVF7X3rCMZbxA2HN/WrtmJapR2UcgRBCWgpvTVUzslg0\n0XphMhnJ0hnsHq97RFMKi0sqPWaR3sgn355j7KCq1/LZevASAPtPpVcRl/Nv9JRKJT4+vk45tnCO\nhtAVTdSeShOCr7/+ui7jEHakWhMCf644MPuSI2QtAiGEqB1Vzchi0T2iKcP6hDHrne8q3SfY35Me\nHZsRH9meEqO5ymOeTs1iXfL5W47d2Td6+fn5XL2aTkhIM0kKGghZg8D9VFoV0KpVqyr/ibr16xUd\nULstBJY/ZmkhEMI9bdu2DYOhtLZ648aNLo6mYbPMyGKLl6eKob1CiY9sTxOtF+1a2p62u0WwD4se\nv4dJQyNQKZVoPFR0Cguq9JxZecWc/OlarcQPpTd6xYbavx6cOfMj8+fPZc+eXbV+bOEcjqxBIBoX\n6QTWAJjNZn69kkcTrRe+Xh61dlwvz9IGImkhEMI9bdq0iSFDhvDSSy9x6NAhV4fT4MVHtmdor1Ca\naL1QKiDYX8O9XW5j2VP3Wm/yAV6d1p/WzfxQXh+Op1RA62Z+zJvSq0Kt68SoCLw8bV+qA/w80RWV\n1Fr8zrrRk0XJGh5Li5ctsgZB4yTzfzUA2To9eQUGuneo3S9TjaXLkBNqhIQQ9d+bb76JTqdj27Zt\nrFq1irlz5zJ8+HBmzJjh6tAaJJVSyaShEcQNDK9ygTBPTzV/m9KHvAI9aek6Qpv54e/jafOYPho1\n/bq2tD0XfIemHD6TQW6B/TEJjnDWjZ4lIQgMlFmGGgpZg8D92G0heOyxx/jqq6+szcqi7lkGFLdp\nXnszDIEMKhZCgJ+fHz179qR79+54enpy7NgxV4fU4Gk8VDRzYHpGfx9P7rg9uNJkwOLmlocmWi+G\n9gplUlQEfTo3r7W4b/VGr9hgJD2roEK3o9xcy6JkkhA0JJbfu2B/DQpKW7wsXd9E42O3heDxxx/n\niy++4NVXX2XgwIHExsbStWvXuohNXHfhUi5QOqC4Nlm++Iv0tdfkLIRoOFavXs1///tf9Ho9Y8aM\n4b333uO2225zdVjiJlW1PMRHtqdQX8K+73+v8fHLzjJUE5WtlPz0+O4AZGdbugxJQtAQWWaar+GM\n86KBsJsQ9O7dm969e1NUVMTWrVuZPn06fn5+jB07lkmTJuHpWXXNhrh1x85dRa1S0DGsdr9MpYVA\nCPeWnp7OwoULueOOO1wdinCApeWhLJVSydSRnflDVEcOn77C4TMZnPvtGraGAgT4qIkfUjqLUG2u\nQ1DZ9JQ+3p7E3Hc7ubnZqNUeshBpAyPTjroXhwYVp6SkMH/+fF5//XX69+/PCy+8wNWrV3nyySed\nHZ/by8guJC1DR+fbg/HW1O6QDw+1EqVCBhUL4a4SExM5f/48r7/+OoWFhbc005DJZOKll14iPj6e\nhIQEUlNTy21PTk4mLi6O+Ph41q1bV+VrfvjhB/r3709CQgIJCQls2bKl5oV0ExoPFffe1ZJpY/8f\nd98VanOf3p1v4+47S/+1axmIv4+nQ12bqlLV9JT/O3mZYoORhISpTJs2s1qLmlbW/aiu1Zc46pq9\naUfd7f1wB3bvMAcPHkxoaChxcXG89NJLeHl5AdCnTx/Gjh3r9ADd3dFzVwHo1qFprR9boVCg8VTJ\nH7YQbmrZsmX8/vvvnDp1ij/+8Y98/vnnnD59msTExGofa9u2bej1epKSkjh27BhLly5l5cqVABgM\nBpYsWcL69evx9vZm4sSJREZGcuTIEZuvOXXqFI8++ihTpkyp7SK7BUvXn6Nnr5KVV0SQvxfdI5o6\npe93VdNTXs0uJEdXTJs2tzt8vMq6H9X16rj1JQ5XcWTa0Ztbq0TDZjch+Oc//0lERPmmoWPHjtGt\nWzdZ5t4BO49ddHjfQd0qru9w9GwGCqB7+9pPCKC0Vim/FqetE0I0HHv37uWLL74gNjYWf39/3n//\nfcaMGVOjhODw4cP0798fgG7dunHy5EnrtgsXLhAWFmaddrJnz54cPHiQY8eO2XzNyZMn+fnnn9m+\nfTtt2rRhzpw5+PlVvQZLUJAPanXDmfkkJKR2x4Td7JmJPSnSl5CVW0yQVmOdZrq2+Qd4ExLkTXpW\nYYVtTQO9adcmGC9PtcOtA6s2nqi0+9FjMXfVWty1HYezP8+6Zu9zDb+9idN+p1ytsX2Wjqr00zx8\n+DAmk4m5c+eyaNEi67L2JSUlvPzyy7KScR3QFRo4m5ZNu1Zap8356+WpJlunp8RoQq1q/LUeQogb\nlNdrOi03a3q93vpcdel0unI37SqVipKSEtRqNTqdDn//GxdZX19fdDpdpa/p2rUr48aNo0uXLqxc\nuZK3336b5557rsrzZ9XSCu51ISTEn4yMvDo5lxrIyynEmWfrGt7E5vSUd3dpwZkfzzN//gvcf/8I\n4rjnVhAAACAASURBVOImVHmcYoORfcdtV6LtO36JEX1a18l0l9WNoy4/z7pU2efaNbyJ03+nXKWx\nfpYWVSU7lSYE3333HQcOHCA9PZ3ly5ffeIFaTXx8fO1GKGw6fv4qZjN072B79cvaYFmLQFdoIFAW\nGhHCrVjWHMjJyeGDDz5g06ZNjBo1qkbH8vPzIz8/3/rYZDKhVqttbsvPz8ff37/S10RFRaHVlq7m\nGxUVxYIFC2oUk6gblXVRmjL6TvbuTbmeGNpfVLO+dFOpL3G4Wl12PROuV2lCMG3aNKB0OfuYmJg6\nC0jccPhM6YCe7k4YP2BhmWlIVyAJgRDu5vHHH2fPnj20bNmSy5cvM23aNAYPHlyjY/Xo0YMdO3Yw\ncuRIjh07Vq6raXh4OKmpqWRnZ+Pj48OhQ4eYOnUqCoXC5mumTp3Kiy++SNeuXdm/fz933nlnrZRX\nOEdl06KqVErrGgSOTDlqWR33mo2b8bpcHbe+xOFqZT9XlacHRr1BFiRrxCpNCFasWMG0adNISUkh\nJSWlwvYlS5Y4NTB399OlXI6dv0qb5v60aOLrtPNY/rjzCvROO4cQon65dOmS9efw8HDCw8PLbWvZ\nsmW1jxkVFcW+ffuYMGECZrOZxYsXs3nzZgoKCoiPjycxMZGpU6diNpuJi4ujefPmNl8D8PLLL7Ng\nwQI8PDxo2rSptBA0ELamRb2xBkGAQ6+vD6vj1pc46guNh4qQpr6NuiuNqCIhsNTI9OnTp86CEaVM\nJjMff3sGgAlDnNs0Z2khyCuUlaiFcBeRkZEEBARY++9bxohB6XiC7du3V/uYSqWS+fPnl3uubKIR\nGRlJZGSk3ddA6fXns88+q3YMov7JybEkBEEO7V9fuqnUlziEqCuVJgSdOnXi0qVL9O3bty7jEcC2\ng7/y8+U8+nZuTscwx75Ea8oyhiCvQBICIdxFYmIi27Ztw9fXlxEjRjB06FC7s/gIURM3ugzZbyGA\nqldlrkv1JQ4h6kqlCcEf/vAHFApFuZoji5rWIAn7svKK+WjLD2g8VIwf7PyaCK8yg4qFEO5h8uTJ\nTJ48mUuXLvHVV1/x2GOPERwczAMPPEBkZKR1vRnRsBQbjPXu5rVHj94EBQWj1TqWEFjY6n7kCvUl\nDiGcrdKEIDk5uS7jEMDVnEJe/fQoOTo9EyLbE+Tv/IFL1i5DMoZACLfTsmVLpk6dytSpUzl37hwv\nvvgiL7zwAkePHnV1aKIa6vMiWj179qZnz94ujUEIYZ/dQcXPP/+8ze0yqLh25ebrWfrxETJzi5l0\nf0eGdK/+oL6a0HiU/gpIC4EQ7qeoqIhdu3axdetWvv/+e+677z6eeeYZV4clqikp+bzNRbQAJg2N\nqOxlQghhJYOK64FsXTHfHvyNwmIjYweFM3FYpzobzS9jCIRwP1u2bGHr1q2cPHmSfv36MX78eP7x\nj3/UeFEy4TrFBiNHz2bY3Hb07FXiBoa7rPuQwWDglVcWcOeddzFqlExfLkR9VmlCYJkNIjY2lmvX\nrnH8+HHUajVdu3YlMND+fMLCMVl5RXx7MI0ivZGJQzoQ1bt1nZ5fpVTgoVZKQiCEG5k5cyYtWrSg\nV69eGAwGNm3axKZNm6zbpQW44ajPi2hlZ2dz9uxpgoObuOT8QgjHVZoQWHz11VcsWrSIHj16YDKZ\neOmll5g/fz4DBgyo0Qn/+c9/kpycjMFgYOLEiYwbN65Gx2kM8gr01mSgb+fmdZ4MWHh5qmQMgRBu\nRG74G4/6vIhWZmYmQLUHFAsh6p7dhGDlypVs2LCBZs2aAXDx4kWefPLJGiUEKSkpHD16lE8//ZTC\nwkJWr15d/YgbCb3BSPLhixTpjfTp3IyOYa5rdfHyVJGZW4zJbEapULgsDiFE3YiNjXV1CKKW1OdF\ntLKysgCkV4EQDYDdhECtVhMSEmJ93KpVK9Rquy+zae/evURERPDUU0+h0+mYPXt2jY7T0JlMZnYd\nu0ROvp472gTRyclrDdjj5anGaCqioKgEP28Pl8YihBCieurrIlo3VimWhECI+q7SO/uNGzcCEBoa\nyp/+9CdiYmJQq9X85z//oWPHjjU6WVZWFpcuXeLdd98lLS2NJ598kq1bt6KopFY6KMgHtdp+7UZI\niH+N4rkV/n41m6fbbDaz6+hFLl8r4PYWWgb1am2tlS9bjqrKVNNzV3o8X08A1BoPp72XrviMnK2x\nlUnKU/81xjKJW1dfF9GSLkNCNByVJgQpKSkA+Pr64uvry+7duwHw8an54KTAwEDatWuHp6cn7dq1\nQ6PRkJmZSZMmtgccZWUV2D1mSIh/nc3IU1aerqhGr/vhl0xO/XSNIH8N99zZnPz8G/0+LeWwV6aa\nnrsyKmVpQpKaloWXEyYZcdVn5EyNrUxSnvqvsjJJkiAs6tsiWk2bNiUiohNNmjR1dShCCDsqTQiq\nGnRWVFSzG9KePXvy0Ucf8eijj5Kenk5hYaFb9S389Uoeh05n4K1REdmjFR7q+jHFn2Vxsv/f3n2H\nR1mmjd//TsmkzaSH0FswFJWuIIJKaALSpKOo6APbAGVXHlwXkZ8gLLK4uwq6r2XRBxUCLgIWpAiK\nIkuTIk16IEB6MpmSzGRm7vePkJGEVEhmMpPzcxweZuZu5zUJ9zXnfbU8mWlIiHqhXbt27pbZ0qvR\nq1QqTp486Y2whJ8ZOHAgXbrc5+0whBBVUOlggC1btrBixQqsViuKouByuSgoKGDPnj3Vvljfvn3Z\nv38/Y8aMQVEU5s2bh0bj/WZNT0hONfH9kato1Cr6dm1KaB3qq+9OCCwy05AQ9cGpU6e8HYIQQog6\npNKEYOnSpSxcuJCVK1fy29/+lh9++ME9c8CtqI8Dic9dMfLjsVQ0ahX9ujUlJrxmxwDcrmBd0Z+B\nTD0qRP2SlZXF559/jsVicT/wSUlJ4bXXXvN2aMIPrFu3Dq02mN69H/R2KEKISlTaZyUsLIyePXvS\nqVMnTCYTM2bM4PDhw56IzeeZrHZ2HExh98+paDVqBtzTjLioutO/s1hQoHQZEqI+mj59OidPnmTT\npk3k5+ezY8cOWa1Y1AhFUVi9ejXffvuNt0MRQlRBpXf+oKAgLly4QHx8PPv27cNut2My+ddgvZpm\nthay90QaG3+4SEqGhbjIYAb3bE5sRLC3QytTcZchk3QZEqJeycnJYcmSJSQmJjJw4EBWrVrFmTNn\nvB1WvWArdJKeY8VW6PR2KLXCYrHgcDhkylEhfESlXYaee+45/vGPf7B06VLeeecdkpKSGDNmjCdi\n8zmKonDoTCbHL2SjKBAapKVrQiwtGxnKnVq1LggM0KBSQZ50GRKiXgkPL5oOslWrVpw6dYpOnTrh\ncDi8HJV/c7pcvLvhZ3YfuUJ2no2osEC6JMQyPrENGj9qncnLK16DQKYcFcIXVJoQ3Hvvvdx7770A\n/Oc//8FoNMo/8DK4XAo/Hkvl/NU89MEBdGoTTatGYajVdTcRKKZSqTCE6KTLkBD1TM+ePZk5cyZz\n5szh6aef5vjx4wQGBno7LL+WtONsiVWFs/Js7teT+id4K6waJ4uSCeFbKk0IUlNTWbhwIfv27SMg\nIID77ruPF198kaioKE/E5xMUpWjl4cvpZmLCg0js1tTdDcdXhIUEkJ1nq3xHIYTfmDVrFpcuXaJJ\nkya8/vrr7N+/n+nTp3s7LL9lK3Ry6HRGmdsOnc5k9IPxdWJBsZqQl2cEJCEQwldU2j754osv0qtX\nL3bs2MGWLVu46667+POf/+yJ2HzGpTQzl9PNxEUFM+CeZj6XDAAYQnRYbQ4KHS5vhyKE8JANGzbw\n008/sWHDBs6cOUNERAQ//vijt8PyW0azrdwHLzmmAoxm/3koU1BQgFarlR4FQviISlsIsrOzmTRp\nkvv1U089xWeffVarQfkSRVE4cjYTlQruu7NhnVlsrLrCQnVA0cxIUWF1a1pUIUTtKF6RHqCwsJCD\nBw/SvXt3Ro4c6cWo/Fe4PpCosECyykgKIg1BhOv9p7vWQw/1Y8yYEaSn53k7FCFEFVSaEHTs2JEv\nv/ySoUOHArBz507uuuuuWg/MV1xMNZFrthPfOMz9pdoXGUKKFkozWQslIRCinii9In1ubi6zZs3y\nUjT+LzBAQ5eE2BJjCIp1SYjxm+5CxVQqlUxjK4SPKDchKF7aXlEU1q5dy1/+8hfUajVWq5Xw8HBe\nffVVT8ZZJ7kUhaNns1CpoGObaG+Hc1vCQoqSGZlpSIj6KyQkhCtXrtzSsS6Xi/nz5/PLL7+g0+lY\nuHAhLVq0cG/fsWMHK1asQKvVMnr0aMaNG1fuMcnJybzwwguoVCruuOMOXn75Zb/5Yjk+sQ0hwTp2\nH7lKjqmASEMQXRJiGJ/Yxtuh1aizZ0+Tk2MgMrKRt0MRQlRBuQmBLG1fuctpZowWO/FNwjCE+G7r\nAPzaZShP1iIQot6YPHmye0pkRVFISUnhgQceuKVzbd++HbvdTlJSEocPH+avf/0rb7/9NlDUHWnx\n4sV8+umnBAcHM3HiRBITE/npp5/KPGbx4sU899xz9OjRg3nz5vHNN98wYMCAGiu3N2nUaqaOvJvB\n9zbDaLYRrg/0u5YBgA8/fI+8PCP//Of/5+1QhBBVUGmXofz8fJYvX86ePXtwOp307NmTZ599lpCQ\nurfirqclpxYt0Na+RaSXI7l9N3YZEkLUDzNmzHD/rFKpiIyMpE2bW3tSffDgQfr06QNA586dOXbs\nmHvbuXPnaN68uXuAabdu3di/fz+HDx8u85jjx4+7p7t+4IEH2L17t98kBMUCAzQ0iPTfetRoNBIV\n5ft1oxD1RaUJwSuvvEJwcDCLFi0CYO3atbz88sssXbq01oOry1wuhauZFkKCtEQafH8gmLuFQLoM\nCVFvbNmyhZdeeqnEe3PmzGHJkiXVPpfZbEav17tfazQaHA4HWq0Ws9mMwWBwbwsNDcVsNpd7jKIo\n7paL0NBQTCZTpdePjAxBq/WdJ+2xsYbKd/JRhYWFWCxm4uNb+3U5b1Qfylkfygj1p5ylVZoQHD9+\nnE2bNrlfz5s3jyFDhtRqUL4gIzcfu8NFy0ZhdXoV4qoqHkNgki5DQvi9v/zlL1y+fJljx45x5swZ\n9/sOh6NKX77LotfrsVgs7tculwutVlvmNovFgsFgKPeYG8cLWCwWwsLCKr1+To71luL2hthYAxkZ\nt/Y5+4KsrEwAIiMj/bqcxfz99wn1o4zg/+WsKNmpNCFQFIW8vDz3DTkvLw+NxneewtSWlIyiSqxp\nbKiXI6kZvw4qli5DQvi73/3ud1y5coVXX32VGTNmoCgKUPSEPj4+/pbO2bVrV3bu3MmQIUM4fPgw\nCQm/rrobHx9PcnIyubm5hISEcODAAZ555hlUKlWZx3To0IG9e/fSo0cPdu3aRc+ePW+/0MJjjMai\nRckiI6XLkBC+otKE4KmnnmLs2LH07dsXKJopYtq0abUeWF2XkmFGo1bRMNo/+oAG6jToAtQyqFiI\neqBp06Y0bdqUTz75hI0bN/LYY4+RlpbGmjVr6NChwy2dc8CAAezevZsJEyagKAqLFi3i888/x2q1\nMn78eF544QWeeeYZFEVh9OjRxMXFlXkMFHVbeumll3j99ddp3bo1gwYNqsnii1pmNOYAkhAI4Usq\nTQj69u3L3Xffzf79+3G5XLz55pu0bdvWE7HVWRm5+RjNdprEhqLV+MdUeFDUSiBjCISoP55//nn3\n/Tw0NBSXy8X//u//8uabb1b7XGq1mldeeaXEeze2NiQmJpKYmFjpMQCtWrXio48+qnYM9Z2t0Fkn\nZi5q1+5O5s1bSKtWTXC5vBaGEKIaKk0IHnvsMTZv3lyi+be+O3ouC4CmsfpK9vQthhAdl9NNJQb0\nCSH819WrV/nXv/4FFPXznzVrFiNGjPByVKK6nC4XSTvOcuh0Btl5NqLCAumSEMv4xDZovLB+Q3Bw\nMC1atCI62r/7YwvhTyq9U7Rr144NGzZw/vx5rl696v6vPjtyrmjAVBM/GT9QLCwkAIdTId/m9HYo\nQggPUKlU/PLLL+7X586dcw8EFr4jacdZth9IISvPhgJk5dnYfiCFpB1nvRJPQUEBLmkaEMKnVHrn\nP3LkCEeOHCnxnkql4ptvvqm1oOoyl6Jw7ooRQ0gA+uAAb4dTowzXpx41We2EBMmXAiH83Zw5c3j6\n6aeJi4sDICcnp95PKe1rbIVODp3OKHPbodOZjH4w3uPdh957722OHPmJVatWefS6QohbV+m3vh07\ndngiDp+RmmUl3+akdeOabx349vAVAAz6IEzmgho/f2WKZxoyWuzERfnHYGkhRPl69erFzp07OXXq\nFLt27eL7779n6tSpHDp0yNuhiSoymm1k59nK3JZjKsBotnl8ATSjMQeVSoVer6egwOzRawshbk25\nCUFaWhoLFiwgOTmZrl278qc//alKc0H7u/NX8wCICQ/yciQ1L1xflBDkmsuuXIQQ/uXy5cskJSWx\nfv168vLy+O1vf8vbb7/t7bBENYTrA4kKCySrjKQg0hBEuN7zC2cajUbCwyNkLJoQPqTcMQQvvvgi\nrVu3Zvbs2djtdhYvXuzJuOqsC9euJwQR/pcQRF6vOHJNkhAI4c+2bdvGM888w9ixYzEajSxdupQG\nDRowffp0oqKivB2eqIbAAA1dEmLL3NYlIcbj3YUURbmeEIR79LpCiNtTYQvB+++/D8B9993HyJEj\nPRZUXXb+ah5ajYpIgx8mBIbrCYFZph4Vwp/NmDGDhx9+mKSkJFq0aAEgT3N92PjENkDRmIEcUwGR\nhiC6JMS43/ckq9WKw1FIWFiEx68thLh15SYEAQEBJX6+8fXtysrK4tFHH+Xf//73La+K6Q32Qicp\nGWZaNDSgUftf5VmcEORIlyEh/NqmTZv47LPPmDRpEk2aNGHo0KE4nTK7mK/SqNVM6p/A6Afjvb4O\ngdGYC0BEhCQEQviSKk9QXFNPjwoLC5k3bx5BQb73hD05zYTTpdC6kX+OpQgL1aECcqTLkBB+LSEh\ngTlz5rBr1y6mTZvGvn37yMzMZNq0aXz33XfeDk/cosAADQ0iQ7y6KJnBYOCxx56ke/ceXotBCFF9\n5bYQnDlzhn79+rlfp6Wl0a9fP/eiVbc67eiSJUuYMGEC77zzzi0d703FA4pbNw6joND/nqZpNWrC\nQnUyhkCIekKj0dC/f3/69+9PdnY2GzduZNmyZTz44IPeDk34KIMhjMTEgd4OQwhRTeUmBFu2bKnx\ni61fv56oqCj69OlTpYQgMjIErbbyJx2xsYaaCK9SV7KsAHS/qzFHzpQ973NNMeg924JS/BnGRAZz\nOdVETIy+RvsUe+p35En+ViYpT91Xm2WKiopiypQpTJkypdauIYQQom4qNyFo0qRJjV/sP//5DyqV\nij179nDy5EnmzJnD22+/TWxs2TMk5ORYKz1nbKznlkY/dTEbfXAAGpezVtcJ8MY6BMWfoSEoALvD\nxcXLOTW28Jonf0ee4m9lkvLUfeWVyR8TH+G7Nm/+nMOHDzJlym/kb1MIH+LR5Wg//vhj98+TJ09m\n/vz55SYDdU2exU6msYCO8dF+PRtHhHumIZvfrcQshBCidqWkXObs2TNotbLavRC+pMqDiuu7S2lF\nT+ZaxPn3E4/I4sXJZByBEEKIasrLMwLIOgRC+BivpfCrVq3y1qVvyaX0ouXXm8fpvRxJ7SpuIZCZ\nhoQQQlRXbm4uISEhBATovB2KEKIapIWgii5fTwia+XsLgaxFIIQQ4hbl5eUSHi5rEAjhayQhqKJL\naSaCdBpiwn1v/YTqiNBfH0MgLQRCCCGqweFwYDabJSEQwgfJqJ8qsBc6Sc220qZJOGo/HlAMN7QQ\nSEIghBCiGgoL7fTseT+NG9f8LIVCiNolCUEVXMm0oCjQrIF/jx8ACAnUotOqpcuQEEKIagkODmHq\n1N97OwwhxC2QLkNVcNk9oNi/xw8AqFQqIgyB5Jrt3g5FCCGEEEJ4gCQEVVA85Wh9aCEAiNQHYrLY\ncThd3g5FCCGEjzh16jhJSR9z9WqKt0MRQlSTJARVcDndjEoFTWJCvR2KR0QYAlEAo7QSCCGEqKLT\np39h69avyM3N9XYoQohqkoSgEi5F4XK6mYZRIegCNN4OxyMi9TL1qBBCiOoxGosSgbAwWZRMCF8j\ng4orkWksoMDurBfjB4oVL04mU48KIaqqoKCA2bNnk5WVRWhoKEuWLCEqKqrEPmvXrmXNmjVotVp+\n97vf0bdv33KP27ZtG0uWLKFRo0YAzJgxg3vvvdcbRRNVZDQWrVIcESHTjgrha6SFoBKX69n4AZCp\nR4UQ1bd69WoSEhL45JNPGDlyJG+99VaJ7RkZGaxatYo1a9bw/vvv8/rrr2O328s97tixY8yePZtV\nq1axatUqSQZ8gNGYi0ajISSkfnSvFcKfSAtBJdwzDNWnhKB4cTLpMiSEqKKDBw/yP//zPwA88MAD\nNyUER48epUuXLuh0OnQ6Hc2bN+fUqVPlHnf8+HFOnjzJhx9+SMeOHXn++efRasuvsiIjQ9Bqfadb\nZ2ys/7U6m815REZGEhf3a5chfyxnWepDOetDGaH+lLM0SQgqcSmtKCGoTy0EUWFFCUFWXoGXIxFC\n1EXr1q3jww8/LPFedHQ0BkNRRRoaGorJZCqx3Ww2u7cX72M2m0u8f+Nx999/P/3796dp06a8/PLL\nrFmzhscff7zcmHJyrDVSNk+IjTWQkWGqfMdqshU6MZpthOsDCfTCmDetNgC9/tey1VY565r6UM76\nUEbw/3JWlOxIQlCJy+lmwkJ1hF9/al4fRBgC0WrUpOXkezsUIUQdNHbsWMaOHVvivenTp2OxWACw\nWCyEhYWV2K7X693bi/cxGAwl3r/xuNGjR7t/7tevH1u2bKm18vg6p8tF0o6zHDqdQXaejaiwQLok\nxDI+sQ0ated6Bi9Y8JrHriWEqFkyhqACloJCsvIK6lXrAIBapaJBZDDpOfkoiuLtcIQQPqBr1658\n9913AOzatYtu3bqV2N6xY0cOHjyIzWbDZDJx7tw5EhISyjxOURSGDx9OamoqAHv27OHOO+/0bIF8\nSNKOs2w/kEJWng0FyMqzsf1ACkk7zno7NCGEj5AWggqk1MPxA8UaRARzNdOCOb8QQ4jO2+EIIeq4\niRMnMmfOHCZOnEhAQADLli0DYOXKlTRv3px+/foxefJkJk2ahKIozJo1i8DAwDKPU6lULFy4kOnT\npxMUFER8fDzjxo3zcglrVk1177EVOjl0OqPMbYdOZzL6wXiPdB/Kzs7i/PlztGzZipiY2Fq/nhCi\nZklCUIFL6fVv/ECxuKhgANJy8iUhEEJUKjg4mDfeeOOm96dMmeL+edy4cTd9sS/vuN69e9O7d++a\nD9TLnC4X7274md1HrtRI9x6j2UZ2XtkTQOSYCjCabTSIDLndsCt15swvvPPOCh5/fAp9+/av9esJ\nIWqWdBmqwOXiAcX1aA2CYsUVSLoPDdQTQoi6LmnHWTZ9f77GuveE6wPdE0GUFmkI8tj4t+LVicPD\nZVEyIXyRJAQVuJxuRqtR0/D60/L6pEFkUZnTZWCxEELUiMq699gKndU+Z2CAhi4JZXfR6ZIQ47HZ\nhvLyihMCWZRMCF8kXYbK4XC6uJJppmms3qOzNNQVcRG/dhkSvunbw1eqtN9DnZvUciRCCKi97j3j\nE9sARUlFjqmASEMQXRJi3O97wq8tBJIQCOGLJCEoR2q2FYdTqZfjBwCiwoLQalTSZUgIIWpIcfee\nrDKSgtvp3qNRq5nUP4HRD8Z7bR2CvDwjIF2GhPBV9e/RdxUVjx9oXg/HDwCo1SpiI4Kly5AQQtSQ\n2u7eExigoUFkiFcWJTMacwkJCSEgQCahEMIXSQtBOS7X4xmGijWICOZalhVzfiH64ABvhyOEED5v\nfGIbQoJ17D5y1Wvde2rD7Nl/wWw2ezsMIcQtkoSgHJfSi5aubhpbfxOCuKgQOJdFWo4VfbA0Awsh\nxO3SqNVMHXk3g+9t5rXuPbXBYAjDYAirfEchRJ3k0YSgsLCQF198kStXrmC32/nd735Hv379PBlC\nlSiKwqU0M7ERQYQE1d+c6caZhuIbS0IghBA1pbh7jz9wOByYTHmEhYWj0fh+ciNEfeTRMQSbNm0i\nIiKCTz75hPfee48FCxZ48vJVlpGbjzm/kFaN6vfTjuKEIC1bBhYLIYQoW0rKJZ5/fgbr1n3i7VCE\nELfIo4+/H374YQYNGgQUPYWvq08Szl/LA6B1vU8Iri9OlisDi4UQQpTNaJQpR4XwdR5NCEJDQwEw\nm83MnDmT5557rsL9IyND0GorTxpiY2t2JqDUnIsAdO3QqNxzG/RBNXpNT5+/tLLKGRUVilajIsdk\nv+3PuKZ/R3VBXS9TVf+GistR18tTXf5WHvDPMgnfJ2sQCOH7PN5B/tq1a/zhD39g0qRJDBs2rMJ9\nc6owB35srIGMDFNNhQfA8fOZaNQqwgLV5Z7bZC6o0WveyKAPqtXzl6W8ckaHB5OSbiI9PQ+VSnVL\n566N35G3+UKZqvo3lJFh8onyVIe/lQfKL5MkCcLbitcgCAuTsWZC+CqPjiHIzMzk6aefZvbs2YwZ\nM8aTl64yh9NFcmrRCsU6P5j54XY1jQ3FUuAgK8+zCYoQQgjfUNxlKCJCWgiE8FUeTQj+9a9/kZeX\nx1tvvcXkyZOZPHkyBQV164tmSoYZh9NFq8b1e/xAseJxFOev5nk5EiGEqD/OpuTw4eaTHDyVxo8/\nXyPLWHfHcskYAiF8n0e7DM2dO5e5c+d68pLVduH6F99WjaQZHqD19cTowrU87m0f5+VohBDCv2Wb\nC3h++Y/u198dueb+WR+sZcnv7iNYV7cWihwwYDB33nk3oaH1d90eIXydR1sIfEHxk/DWMu8+AC0a\nGlCppIVACCE84cZkoDRzvoM5b+8p8Z6t0El6jhVbobO2QytXQkI7HnqoP2q1fKUQwlfV31W33fcI\nNQAAIABJREFUynH+Wh5BOg2NovxjwZjbFaTT0iRGT3KqCYfThVYjN3whhKgNZ1NyKt3HnO8gy5hP\nhCGQpB1nOXQ6g+w8G1FhgXRJiGV8Yhs08sVcCFFNcte4gbXAQWqWlVaNwlCrb21GHX/UurEBu8PF\n1UyLt0MRQgi/tfvn1Crtd/xCNh98dYrtB1LIyrOhAFl5NrYfSCFpx9naDbIUq9XK7NkzSUr6yKPX\nFULULEkIbnDuqhEF6v0KxaUVd5+SbkNCCFF77r+7YZX22/D9BXYfKzt5OHQ606Pdh4zGXLKzs8jP\nr7uDnoUQlZOE4AbHL2QD0KFlpJcjqVtkpiEhhKhZZfX9b9O0anVPrsVe7rYcUwFGs+2246sqmWFI\nCP8gYwhucPxiNjqtmjuayoDiGzWOCSUwQMOFa5IQ+LLULCuHz2ZS6HChC1ATHRZEpzYx3g5LiHrF\n6XLx7oaf2X3kSpl9//82vVeFA4srE2kIIlwfWIMRV8xoLFqUTBICIXybtBBcl2OycSXDQkLzCAK0\nsiDZjdRqFS0bGriaaSHf5vB2OKKabHYn3x+5ytb9l0nPycdsLSQtO58TF3P48seLJKf614q+QtRl\nSTvOsun78+X2/Y/SB/HX3/S85fN3SYgh0IOLakoLgRD+QRKC64q7C93VMsrLkdRNrRuHoVA0C5Pw\nHQ6ni28OpnDhmono8CCG3NeCiQPu4LEBd9ChZSR51kJeXXWAAyfTvB2q8HEFBQXMmDGDSZMmMXXq\nVLKzs2/aZ+3atTz66KOMGzeOnTt3lti2bds2/vSnP7lfHz58mLFjxzJhwgSWL19e6/F7gq3QyaHT\nGWVuu7Hvf7g+kOiw6j3ljw4LpH/3poxPbHPbcVbHrwmBtKwL4cskIbju+MWiyuvO1tFejqRuan99\nXMVPv5RdmYm6x6Uo7DpyjUxjAa0bhzG4Z3NiwoMA0GjUdG/XgMRuTQAVSz86wLUsmUVK3LrVq1eT\nkJDAJ598wsiRI3nrrbdKbM/IyGDVqlWsWbOG999/n9dffx27vagv/MKFC1m2bBkul8u9/8svv8yy\nZctYvXo1R44c4cSJEx4tT20wmm1k55Xdv//Gvv+BARq6JMRW+by97mrIwqk9mdQ/weNTjrZs2Zpe\nvfoQHV31eIUQdY8kBBR9cTp+IZtIQyCNo2X9gbK0bxFJeKiOfSfTcDhdlR8gvO7AqXRS0s00ig7h\nvrsaolbdPJVu01g9U4a0w1rg4I1Pj2IpKPRCpMIfHDx4kD59+gDwwAMPsGdPyQW0jh49SpcuXdDp\ndBgMBpo3b86pU6cA6Nq1K/Pnz3fvazabsdvtNG/eHJVKRe/evfnxx1vvV19XhOsDiSrnyX/pvv/j\nE9vQv3tTosOCUKsgOiyIxG5N6NetSYn3+ndvypQh7TzaTehG99zTg2ee+S2RkTIZhxC+TAYVA5fS\nTJjzC+l9dyNUZXxpEqBRq+nRIY6t+y/z8/ksutwhT4PqskNnMjiVnEuEXseDXRqjqWBdjfvubEi2\n2c5/dp7lnU0neHZsxzKTByGKrVu3jg8//LDEe9HR0RgMBgBCQ0MxmUqOTTGbze7txfuYzWYAhgwZ\nwt69e0vsq9frS+x7+fLlCmOKjAxB6wPjv+7v1IRN358v4/3GNG1csh/+sxO7UWB3kJNnIzIskCBd\nUZVd1nt1VWysofKd/EB9KGd9KCPUn3KWVrfvJB5y7Pz17kKtZPxARe67syFb919mz/E0SQjqMKPZ\nxsqvTqFWq+jTqTG6KnxJmjykA6cuZvPz+Sy2H0hh4D3NPBCp8FVjx45l7NixJd6bPn06FktRtzOL\nxUJYWMn1XPR6vXt78T43JgiV7Vv6fKXl5FirVQZvGXZfcwB2H7lKjqmASEMQXRJiGHZfczIyyh7g\nrwVMxnxMlbznDR988C6xsQ0YOnTETdtiYw3llsmf1Idy1ocygv+Xs6Jkp953GVIUhT3HU9FqVJIQ\nVKJ5nJ5G0SEcPpOJtUBmG6qLFEVh5eZTmPML6ZYQS6ShagMTNWoV/zO0PWEhAXz67VmZeUhUW9eu\nXfnuu+8A2LVrF926dSuxvWPHjhw8eBCbzYbJZOLcuXMkJCSUeS69Xk9AQACXLl1CURR++OEHunfv\nXutl8ASNWs3UkXezcGoPFk3rycKpPbzS978mOBwOvv/+W44f/9nboQghbpPv3YFq2NkrRq5lWema\nEIs+OMDb4dRpKpWK++5siMPp4uAv6d4OR5Th20NXOHouiztbRtKuRfWmAQzXB/L00A44nArvfH4c\nm91zq50K3zdx4kTOnDnDxIkTSUpKYvr06QCsXLmSb775htjYWCZPnsykSZN48sknmTVrFoGB5Ses\n/+///T+ef/55xowZQ4cOHejUqZOniuIRgQEaGkSGeK3vf00wmYpmnZMpR4XwffW+y9Cuw1cBeKBT\nYy9H4ht6dojjs13n2bz3Ej06xKHz4crM31zLspC04yyhQVqeHtqBI+cyq32OjvHRDOjejG0HLrP6\nmzM8NbhdLUQq/FFwcDBvvPHGTe9PmTLF/fO4ceMYN25cmcf36NGDHj16uF937tyZtWvX1nygosbI\nlKNC+I963UJgLShk/6l0YiOCaNdCZkioipiIYPp1a0pqtpX1u24eGCe8w+F08c6mE9gdLp4a3K7K\nXYXKMuaheJo10LPryFUOnJKWICFE2YoTgrAwaSEQwtfV64TgvyfSsDtcPNCpscyqUg2jH4onLjKY\nbfsvc/pyrrfDEcD6786TnGai992N6Na2wW2dK0Cr5jfD70SnVfPB5lNk5ubXUJRCCH9iNBoBiIiQ\nhEAIX1dvEwKXovDtoSto1Cp6393I2+H4lMAADc8M7QAqePfz41xKkwGo3rT3RBpf77tEXFQIE/vf\nUSPnbBwTyqQBCVhtDpYlHXYvmCSEEMW0Wi1xcY2IipIJOYTwdfV2DMF3h66QkmGhZ4e4EovBiMp9\ne/gKAJ3bxHDoTCavfLCfznfE0L5FJBpNyRzzoc5NqnXOqqjqOaujqtevjWvfjktpJlZ+dZIgnYaZ\no+8mOLDm/kk/0Kkx6Tn5fPXfZJYlHeZ/J3Wt8sD72vh9VuecYwdUbeyDt//uhPBlvXr1oVevPt4O\nQwhRA+plQpBjsvHpd+cIDtQyLrGNt8PxWXfHRxMVFsSPx67x0+lMjp7LoklMKHFRIYTrdRhCdNjs\nThRF8Xaofiktx8ob/zmK3eFixui7aRQdWuPXGP1ga2x2J9/8lMLijw7yP490oFWjiueDF0IIIYRv\nqZcJwSfbTpNvc/LEw22JkNaB29IkNpRh97fk+IUcLqWZSE4zk5xmdm9f/915tBo1oUFaQoK06LQa\nFBS4niMogKKApaAQjVpFoE5DsE5LhF5HZFggMeHBBGhrr2eboijkWezkmGzYHU5UqAgO1BASqL2p\ntaMuuZppYemaQxjNdsb2ja+1heJUKhUTB9yBRqNi6/7LvPp/Bxl4TzPu79iIxtEhN63s7XC6yDHZ\nSMu2Uuh04XQqqNUqggO1hAZpa7QF40YFdgdmayEFdid2h4s9P1/DXmAn0nBrf0OKomApcGDJL6TQ\n4aLQ6SLKEIg+WEdMRBBhIbpaKYcQvmTv3j2o1WruuadH5TsLIeq0epUQKIrCV/9N5uDpDO5oGi5T\njdaQIJ2Wbm1j6ZoQg9FiJzuvAKPZjjm/EH2wDrvDRa6pAJO1kEJnUV90FfDrd0kVTqcLh1PBVao1\nQa2CuKgQmsSG0jRWT1jo7X0RUxSFa1lWjp7L4mRyDslpJvIs9pv2UwERhkAaRAbTMCqExjGhtZqY\nVMfxC9m88/lxTNZCJvS7o9ZXFVarVEzodwed2sSw8quTfL3vEl/vu0RMeBBRhkC0WjUFdidZeQXk\nme1U1B4UEqglJiKIuKgQGkeHEhZ6a2t/mK2FXM20cC3bSmZuPpZSC+X9cPSa+2cVEBUWRIPIYGIj\ngom7/v/gIC1XMy04XQr5BQ4sNgd5Frv7P6erZEm+P/LrOaPDgohvEkaHllHc3Tr6tmZ1EsJXrV+f\nhNPpkIRACD9QbxICl0vhk+2n2fHTFSINgTw9tL3MLFTDVCoVEfrAEq0uD3VuUqWlwL89fAVFUSh0\nurAWOMgx2cjOs3Ety8K1LCvXsqwcOJWBISSA1CwrneKjuaNZBNoqPMXPtzn45XIuP5/P4udzWWQa\nC9zbosMC6XJHDFabA51WjUsp2t+cX0iWsYAck41fLuWiVqloGB2M06nQqU10hct/1xZrgYNPvz3L\nt4evolapeGJQWx7q4rl+7e1bRPLKM/dy8JcMjpzN5MTFHPdnqVGriDQEktAsgqiwIEz5dnRaNRq1\nGqfLRb7NiSm/kMzcfC6lmbl0vRUpNEhLcqqJDi2jaNkojNjwoJtaHRRFITvPRnKqidRsK1czLZis\nhe7tQToNTWJDCQ/VEaTTEKDVcFebWDKzLWTlFZCRk096bj4nk3M4mZxTaTm1GhVhoTrCQ3XoQwLQ\nadVoNWqaNdBjshaSmm3l/NU89p1MZ9/JomlZm8bquTs+io6to4lvEl6lv0shfJmiKOTlGWnUSMbW\nCOEPPJoQuFwu5s+fzy+//IJOp2PhwoW0aNGiVq/pdLk4+EsGX++9xMVUE01jQ5k1rrM80auDVCoV\nOq0GnV5DhD6QVo0AYrEWFHIlw0JKhoVrWRa27r/M1v2X0WrUtGxooEVDA9FhQUTodaACp1PBaLGT\nmm3lcrqZS2kmihseggO13NOuAR3jo7mrVZR7QHlZg0udLoVsYwFXMi1cTjdzNdPKx9tO8/E2aNko\njDtbRhZ9kW1oqLWuMC5FISXdzK4jV9l9LBWb3UmT2FCeGdqelg0935c/SKfl/rsbcf/1mbkURcHh\nVNBoVCUS7PIG6yqKgjm/kNQsK1ezrFzLsrDryDV2XX/6HhyoIcoQRHCgFrVahTm/EKPZVqIFQKtR\n0bSBnsYxRa0MhpCAm5KIkQ/G35SE2gudZOQWJQcZOfnYHC4uXMtDrYKQ692ZwkJ0hARpbzoflBxU\nrCgKqdlWfj6fzc/ns/jlUi4pGWY2//cSwYEaOrSI4u7rf2NRYUHV/JSFqPvy8/Ox2+2yKJkQfsKj\nCcH27dux2+0kJSVx+PBh/vrXv/L222/X+HVsdifbD17m3JU8zl81kmctRAV0b9eApx5uR0hQvWkY\n8QshQQHc0SyCO5pF4HS5aBgZypGzmZxJMXL+ah5nrxjLPVajVtGmSThtm0dwZ8uoaj291ahVxEYG\nExsZTOc7YrDkFxISqOXw2SxOXcrh4rU8vtyTjApoGB1CXGQIDSKDMYQEEBIU4B43ERyoRa1SoVKB\niuv/v/4aBWwOJ3a7E1uhC1uhE3N+IZnGfFKzrJy9YnR/GY40BDKsV0sGdG9WZ7ovqVQqArRVb2lT\nqVQYQooGnN/RLAKXotAyLoxTl4rGoFxON5NrtnE1y4KiFLUgGEJ0tG8RiUtRiI0o6u6jVle/dU8X\noKFJrJ4msXr3e9WZZah0ORpFh9IoOpSB9zTDVujkVHIOx64nCAdPZ3DwdAZQ9HtrHB1Cg6gQIkJ1\nhIXqCNJpCdRpridRClFhQTS9IS4h6rq8vOJVimUNAiH8gUe/GR88eJA+fYqmKOvcuTPHjh2rleuc\nTsnlP98VraIbaQikb5cmDLynGXFRIbVyPeE5GrWaO1tFcWeronmvbXYnV7Ms5JhsRXPlq1Ro1CoM\nwQHERYUQG1Fzg5JDgwN4qHMT+nZtij4smO8PXOLMFSMXruZxKd3MtSxrjVznRjHhQXRqE0OXO2Lo\nfEcMGnXdSARqilqlonXjMFo3LtnaoShF40luLO+tfnn3hMAADZ3axNCpTQwAadlWjp7P4viFbC6n\nmzl+MYfjF8vvrqRRq3jrjw8QoNV4KmQhbkvxomRhYdJCIIQ/UCkenBPyL3/5CwMHDuTBBx8E4KGH\nHmL79u1otfLEXgghhBBCCG/w6ONGvV6PxWJxv3a5XJIMCCGEEEII4UUeTQi6du3Krl27ADh8+DAJ\nCQmevLwQQgghhBCiFI92GSqeZej06dMoisKiRYuIj4/31OWFEEIIIYQQpXg0IRBCCCGEEELULf41\nZYkQQgghhBCiWiQhEEIIIYQQoh6ThEAIIYQQQoh6zOcSgoKCAmbMmMGkSZOYOnUq2dnZN+3zwQcf\nMHbsWMaOHcvy5cu9EGXVVaU8ANnZ2QwaNAibzebhCKvO5XIxb948xo8fz+TJk0lOTi6xfceOHYwe\nPZrx48ezdu1aL0VZdZWVByA/P58JEyZw7tw5L0RYPZWV54svvmDs2LFMmDCBefPm4XK5vBRp1VVW\npi1btjB69GjGjBnDhx9+6KUoq64qf3MAL730En/72988HJ24Vf50ny/N3+775fG3+qA8/lhPlOZv\n9UaNUXzMv//9b+WNN95QFEVRvvjiC2XBggUltl+6dEkZNWqU4nA4FJfLpYwfP145efKkN0KtksrK\noyiKsmvXLmXEiBFKly5dlIKCAk+HWGVbtmxR5syZoyiKohw6dEj57W9/695mt9uV/v37K7m5uYrN\nZlMeffRRJSMjw1uhVklF5VEURTl69KgyatQopVevXsrZs2e9EWK1VFSe/Px8pV+/forValUURVFm\nzZqlbN++3StxVkdFZXI4HMqAAQOUvLw8xeFwKAMHDlSysrK8FWqVVPY3pyiKsnr1amXcuHHK0qVL\nPR2euEX+dJ8vzd/u++Xxt/qgPP5YT5Tmb/VGTfG5FoKDBw/Sp08fAB544AH27NlTYnvDhg157733\n0Gg0qFQqHA4HgYGB3gi1SiorD4BarWblypVERER4OrxqubEsnTt35tixY+5t586do3nz5oSHh6PT\n6ejWrRv79+/3VqhVUlF5AOx2OytWrKB169beCK/aKiqPTqdjzZo1BAcHA9T5fzfFKiqTRqPhq6++\nwmAwkJubi8vlQqfTeSvUKqnsb+6nn37iyJEjjB8/3hvhiVvkT/f50vztvl8ef6sPyuOP9URp/lZv\n1JQ6vUzwunXrbmquiY6OxmAwABAaGorJZCqxPSAggKioKBRF4bXXXqNDhw60atXKYzFX5FbKA3D/\n/fd7JL7bZTab0ev17tcajQaHw4FWq8VsNrvLCUVlNZvN3gizyioqD0C3bt28Fdotqag8arWamJgY\nAFatWoXVavWJv7vKfkdarZatW7fyyiuv8OCDD7orsrqqovKkp6ezYsUKli9fzubNm70YpaiIv9/n\nS/O3+355/K0+KI8/1hOl+Vu9UVPqdEJQPA7gRtOnT8disQBgsVgICwu76TibzcaLL75IaGgoL7/8\nskdirYpbLY+v0Ov17rJAUT+94n9gpbdZLJYSFUVdVFF5fFFl5XG5XCxdupQLFy7w5ptvolKpvBFm\ntVTldzRw4ED69+/PCy+8wIYNGxg9erSnw6yyisrz9ddfk5OTw7Rp08jIyKCgoIDWrVvz6KOPeitc\nUQZ/v8+X5m/3/fL4W31QHn+sJ0rzt3qjpvhcl6GuXbvy3XffAbBr166bsnJFUfj9739P27ZteeWV\nV9BoNN4Is8oqK48v6dq1K7t27QLg8OHDJCQkuLfFx8eTnJxMbm4udrudAwcO0KVLF2+FWiUVlccX\nVVaeefPmYbPZeOutt3zmiUhFZTKbzTz++OPY7XbUajXBwcGo1XX7lldReZ544gnWr1/PqlWrmDZt\nGo888ogkAz7Cn+7zpfnbfb88/lYflMcf64nS/K3eqCk+t1Jxfn4+c+bMISMjg4CAAJYtW0ZsbCwr\nV66kefPmuFwu/vjHP9K5c2f3MX/84x/r7E2osvL069fPvW9iYiKbN2+us332XC4X8+fP5/Tp0yiK\nwqJFizhx4gRWq5Xx48ezY8cOVqxYgaIojB49mscee8zbIVeosvIUmzx5MvPnzyc+Pt6L0VauovLc\nddddjB49mu7du7uf+DzxxBMMGDDAy1FXrLLfUVJSEp9++ilarZa2bdvy0ksv1emHBFX9m1u/fj3n\nz5/n+eef92K0oqr86T5fmr/d98vjb/VBefyxnijN3+qNmuJzCYEQQgghhBCi5tSPdhAhhBBCCCFE\nmSQhEEIIIYQQoh6ThEAIIYQQQoh6TBICIYQQQggh6jFJCIQQQgghhKjHJCEQtywlJYW77rqLESNG\nMGLECAYNGsTMmTPJzMys1nm++eYb/vnPf1b7+iaTid///vcApKWlMXXq1Gqfo7TExESGDBniLlNi\nYiIzZ87EarVW+1yrV69m9erVN72/fv16XnjhhVuK74UXXmD9+vU3vd+2bVt3zMX//f3vf7+lawgh\nhCfdWJeMHDmSoUOHMmXKFFJTU2/5nDfeZ6dOnUpaWlq5+77xxhscOHCgWudv27Ztmdds27YtX3zx\nRYn3P/jgA9q2bUtKSkq1rlGWyZMnM2DAAEaMGMHw4cMZNmwYX331VZn7pqam8uc//7nC873wwgu0\na9fups/n97//PYmJiQBs27aNjz766LZjF3Wb/y2zJzyqQYMGbNy4EShaFO71119n5syZfPLJJ1U+\nR79+/UrMw11VRqORU6dOARAXF8e7775b7XOU5Z133qFp06YA2O12Jk2axIYNG5g0aVK1zjNx4sQa\niaeqin8PQgjha26sSwCWLVvGggULWLFixW2fu7K6Yf/+/fTo0eO2rwPQsGFDtmzZwiOPPOJ+b9u2\nbTW6OvXChQvd8Z49e5YxY8bQo0cPoqOjS+y3aNEinn322UrPFxcXx9atW5k8eTJQtDjXiRMn3Aty\nDRgwgCeeeILBgwffdA3hP6SFQNQYlUrFjBkzOHPmjPuL+jvvvMOoUaMYPnw4r732GoqikJKSwsMP\nP8zEiRN56qmn3E9yvvnmG37zm9+4z/fRRx+xcOFCzGYzM2fOZPz48fTt25fZs2ejKAoLFy4kPT2d\nP/zhD6SkpJCYmEhOTg73338/hYWFAJw+fZphw4YBsGHDBkaNGsWIESN48cUXsdlslZbJZDJhMpmI\niIgAilYZHTNmDCNHjmT69Onk5OQAsGTJEoYPH86oUaNYvnw5AG+++SZvvvmm+9qDBg1i9OjRfPvt\nt+7zJyYmup8a7d27131D3rdvHxMnTmTUqFHuhYpuVWJiIs899xyDBg3i6NGjJT57l8vFwoULGTp0\nKI888gjvvPOOO5YxY8bw6KOPMmfOnFu+thBC3Iru3btz8eJFoOQ9LCsrq9x7eWX3WZvNxosvvsig\nQYN45JFH+Oqrr9iwYQPHjh1j7ty5/PLLLyQnJzNlyhRGjRrFxIkTOXHiBFDUijFx4kRGjBjBvHnz\nyo37nnvu4dixY+5W5StXrhAaGorBYHDvU1a9uHjxYt5//333PjNnzmTr1q2Vfk5t2rQhJCSEK1eu\nlHg/OTmZ9PR09wJp5dU1AAMHDmTLli3u19u3b+ehhx4qcb6BAwfy8ccfVxqP8F2SEIgapdPpaNGi\nBefPn2fXrl0cO3aMTz/9lA0bNpCWlsamTZsAuHDhAkuXLuWDDz5wH/vAAw9w/PhxjEYjAF988QXD\nhw/n22+/pX379iQlJbFlyxYOHz7M8ePHmTt3Lg0aNCjxBCkyMpKOHTvyww8/APDll18yfPhwzpw5\nw9q1a1mzZg0bN24kOjq6xM33RtOmTWPYsGH06tWLqVOn8vjjjzN48GCys7NZtmwZ77//Phs2bKB3\n79787W9/48qVK+zatYtNmzaxZs0aLl68WCLZSEtL429/+xsff/wxSUlJWCyWSj/H4mTos88+49VX\nX+Wtt96q9JjSXYa+//77Ep/tli1biIqKKvHZr169mmvXrrFp0ybWrVvH1q1b3RXpxYsX+fDDD1my\nZEml1xZCiJpSWFjI5s2b6dq1q/u94ntYdnZ2mffyqtxnV61ahdVqZfPmzaxcuZIVK1YwZMgQ7rrr\nLhYuXEjbtm2ZM2cOs2fP5rPPPmPBggXMmjULgAULFvDoo4+ycePGEnGVptVq6d27N9999x0Amzdv\nZvDgwe7t5dWLI0aM4MsvvwSKntD/9NNPN30pL8v333+P0+m8aWXknTt3Vhjnjdq3b09WVpa7u2/p\nmKEoQduxY0eVzid8k3QZEjVOpVIRFBTEnj17OHr0KI8++igABQUFNG7cmG7duhEdHe3ullMsICCA\ngQMHsnXrVnr16kVubi4dO3akY8eOHD16lA8++IDz58+Tm5uL1Wp1P7UvrfjG2rdvXzZv3sz//d//\nsX37dpKTkxk3bhxQVOF06NChzOOLuwxt2bKFxYsXk5iYiEql4siRI1y7do0nnngCKFr+PDw8nLi4\nOAIDA5kwYQJ9+/blueeeIzAw0H2+Q4cO0aVLF2JiYgAYNmwY//3vfyv8DJcuXcrOnTv5+uuvOXLk\nSJWSiIq6DHXq1Mn9842f/d69exk1ahQajYbg4GCGDRvGnj17SExMpFWrViWeagkhRG1JT09nxIgR\nQFFXzY4dO/KnP/3Jvb34HrZ3794y7+VVuc/u37+fcePGoVariY2NdX8BL2axWDh27FiJfvdWq5Wc\nnBz27dvHsmXLABg+fDhz584ttyyDBw9m7dq1DB48mO3bt/Puu++6W4vLqxdHjBiB3W4nOTmZQ4cO\n0bdvX3Q6XZnnnzt3LiEhITidTsLDw/nHP/5BaGhoiX2Sk5Np1apVuTGWVlz3Dh06FLPZTJMmTUps\nb9KkCcnJyVU+n/A9khCIGmW327lw4QJt2rThv//9L08++SRTpkwBIC8vD41GQ05ODkFBQWUeP3z4\ncP75z39iNBrdfTBXrVrFli1bGDduHL169eL06dMoilJuDImJiSxevJj9+/fTsGFDGjZsiNPpZPDg\nwe6buMViwel0VliWQYMGsXv3bubNm8f777+P0+mka9eu/Otf/wLAZrNhsVjQarWsW7eOffv2sWvX\nLiZMmMCqVavc51GpVLhcLvdrrbbkP7visjgcDvd7kyZNokePHvTo0YP77ruP559/vsK11tEVAAAE\nTElEQVRYK3NjgnLjZ39jXMWxFH8u5f2OhBCippUeQ1Ba8T2svHv5nj17KrzPlvVecnIyjRo1cr92\nuVzodLoScaSmprofPhXfq1UqFSqVqtxYe/Towdy5czl9+jSRkZElHqw4nc4y60Uoqv+++uorDh06\n5J4kozhJgl8f+tw4hqA8arXafd5iZdU1xQYPHszixYvR6XQMGDDgpu1arbbCMgvfJ12GRI1xuVy8\n+eabdOrUiebNm9OzZ082btyIxWLB4XDwhz/8oUQ/xbJ07tyZ9PR0Nm7c6L4R7t69m/HjxzN8+HBU\nKhWnTp3C5XKh1WrLvLHpdDr69OnDokWLGD58OFB0g962bRtZWVkoisL8+fP58MMPKy3Ts88+y6FD\nh9i5cyedOnXi8OHDXLhwAYC33nqL1157jRMnTvD4449zzz33MGfOHOLj4937AHTr1o0jR46QlpaG\ny+UqMSNEZGQkZ8+eBYpmWwLIzc3l4sWLPPvsszz44IPs3r270uTlVvXs2ZMNGzbgdDrJz8/n888/\nr7HBdUIIUdPKu5dXdJ8tds8997B582YURSErK4vHH38cu92ORqPB6XRiMBho2bKl+4v37t27eeyx\nxwDo1auXu8vr1q1bsdvt5cao0Wjo3bs38+bNY8iQISW2VVQvFs8YlJycTPfu3YGiJKD4v+po1qwZ\nV69edb8uq665Ubt27cjMzGTdunU8/PDDN21PSUmhRYsW1YpB+BZpIRC35cZmXpfLRfv27d3NqomJ\niZw6dYpx48bhdDrp06cPo0aNumnwU2mDBw/mhx9+oFmzZgA8+eSTzJ8/n3//+9+EhobSpUsXUlJS\n6N69O40bN2by5MksXry4xDlGjBjBpk2b3De2du3aMX36dJ588kl3nNOmTau0fNHR0UydOpXXXnuN\nzz//nEWLFvHcc8/hcrmIi4tj6dKlREZG0rlzZx555BGCg4Np3769ezwEQExMDHPnzuWpp54iODiY\nNm3auM8/c+ZMFixYwPLly+nduzcAERERjB07lqFDh6LX6+ncuTMFBQWVTn1645MkgBYtWvDGG29U\neMz48eO5ePEiI0aMoLCwkOHDhzNgwAD27t1b6WcjhBCeVt69PDAwsNz7bLFJkyaxcOFC94Oil156\nCb1eT58+fXj55ZdZsmQJS5cuZf78+bz33nsEBATw97//HZVKxbx585g9ezZr1qzh7rvvvqmLTmmD\nBw9m48aN7qk7i5VXLwI0atTIXZ/c7tP4vn37lmhZLquuKW3AgAHs27ePhg0b3jRF6t69e29pNkDh\nO1RKRX0vhBBCCCGEz5k+fTozZ84kISHhts81ceJEli9fLtOO+jHpMiSEEEII4Wf+/Oc/lzubXnV8\n/fXXDBo0SJIBPyctBEIIIYQQQtRj0kIghBBCCCFEPSYJgRBCCCGEEPWYJARCCCGEEELUY5IQCCGE\nEEIIUY9JQiCEEEIIIUQ99v8DY/3J6tAJhYIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11e2e1e48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Derivative Mean Error: 1.37999900233 Error Standard Deviation: 0.324052131009\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeEAAAFXCAYAAACV2fZmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11e9f7cf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#2. Calculate Error Distribution and Plot + Report Moments\n",
    "ts_test_df = generateTSDataSet(test_df,features,targets)\n",
    "display(ts_test_df)\n",
    "errors = []\n",
    "for target in modelDict:\n",
    "    times,y_test = remove_NaN(ts_test_df.reset_index()['Time (h)'].values,ts_test_df[('target',target)].values)\n",
    "    \n",
    "    feature_list = [('feature',feature) for feature in specific_features[target]]\n",
    "    target_df = ts_test_df[feature_list]\n",
    "    target_df = target_df[ts_test_df.index.get_level_values('Time (h)').isin(times)]\n",
    "    \n",
    "    #Check to make sure there are no NaNs in each feature\n",
    "    for feature in target_df.columns:\n",
    "        if any([math.isnan(val) for val in target_df[feature].values]):\n",
    "            X,y = remove_NaN(target_df.reset_index()['Time'].values,target_df[feature].values)\n",
    "            fnc = interp1d(X,y)\n",
    "        index = 0\n",
    "        for time,val in zip(times,target_df[feature].values):\n",
    "            if math.isnan(val):\n",
    "                #print(feature,time,fnc(time))\n",
    "                target_df[feature].iloc[index] = fnc(time)\n",
    "            index += 1\n",
    "    #display(target_df)\n",
    "    \n",
    "    y_prediction = modelDict[target].predict(target_df.values)\n",
    "    \n",
    "    #print(y_prediction)\n",
    "    #print(y_test)\n",
    "    log_error = [math.log(max(y_p,0.0001)) - math.log(max(y_t,0.0001)) for y_p,y_t in zip(y_prediction,y_test)]\n",
    "    error = [y_p - y_t for y_p,y_t in zip(y_prediction,y_test)]\n",
    "    errors.append(error)\n",
    "    \n",
    "    mu = np.mean(error)\n",
    "    sigma = np.std(error)\n",
    "    print(target,'Mean Error:',mu,'Error Standard Deviation:',sigma)\n",
    "    \n",
    "    plt.figure(figsize=(13,4))\n",
    "    plt.subplot(121)\n",
    "    sns.distplot(error)\n",
    "    plt.title(target + ' Derivative '+ 'Error Residual Histagram')\n",
    "    plt.xlabel('Derivative Residual Error')\n",
    "    plt.ylabel('Probability Density')\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    plot_model_fit(target,y_prediction,y_test)\n",
    "    \n",
    "    strip_target = ''.join([char for char in target if char != '/'])\n",
    "    plt.savefig('figures/' + strip_target + 'ErrorResiduals.pdf')\n",
    "    plt.show()\n",
    "    \n",
    "    #modelDict[target].predict()\n",
    "\n",
    "#Compute Net Error Magnitude\n",
    "error_magnitude = [0]*len(errors[0])\n",
    "for error in errors:\n",
    "    error_magnitude = [em + e**2 for em,e in zip(error_magnitude,error)]\n",
    "error_magnitude = [math.sqrt(e) for e in error_magnitude]\n",
    "mu = np.mean(error_magnitude)\n",
    "sigma = np.std(error_magnitude)\n",
    "print('Total Derivative','Mean Error:',mu,'Error Standard Deviation:',sigma)\n",
    "    \n",
    "sns.distplot(error_magnitude)\n",
    "plt.title('Total Derivative Error Risidual Histagram')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0633897204865148e+16\n",
      "differential_evolution step 1: f(x)= 7.84561e+27\n",
      "differential_evolution step 2: f(x)= 7.84561e+27\n",
      "differential_evolution step 3: f(x)= 7.84561e+27\n",
      "differential_evolution step 4: f(x)= 2.7018e+27\n",
      "differential_evolution step 5: f(x)= 2.7018e+27\n",
      "differential_evolution step 6: f(x)= 1.81261e+26\n",
      "differential_evolution step 7: f(x)= 1.81261e+26\n",
      "differential_evolution step 8: f(x)= 1.81261e+26\n",
      "differential_evolution step 9: f(x)= 1.81261e+26\n",
      "differential_evolution step 10: f(x)= 1.81261e+26\n",
      "differential_evolution step 11: f(x)= 1.81261e+26\n",
      "differential_evolution step 12: f(x)= 1.81261e+26\n",
      "differential_evolution step 13: f(x)= 1.81261e+26\n",
      "differential_evolution step 14: f(x)= 1.81261e+26\n",
      "differential_evolution step 15: f(x)= 7.71253e+25\n",
      "differential_evolution step 16: f(x)= 1.88109e+25\n",
      "differential_evolution step 17: f(x)= 1.88109e+25\n",
      "differential_evolution step 18: f(x)= 6.75556e+24\n",
      "differential_evolution step 19: f(x)= 1.29253e+24\n",
      "differential_evolution step 20: f(x)= 1.29253e+24\n",
      "differential_evolution step 21: f(x)= 1.29253e+24\n",
      "differential_evolution step 22: f(x)= 1.29253e+24\n",
      "differential_evolution step 23: f(x)= 1.29253e+24\n",
      "differential_evolution step 24: f(x)= 1.29253e+24\n",
      "differential_evolution step 25: f(x)= 1.29253e+24\n",
      "differential_evolution step 26: f(x)= 6.23532e+22\n",
      "differential_evolution step 27: f(x)= 6.23532e+22\n",
      "differential_evolution step 28: f(x)= 6.23532e+22\n",
      "differential_evolution step 29: f(x)= 6.23532e+22\n",
      "differential_evolution step 30: f(x)= 6.23532e+22\n",
      "differential_evolution step 31: f(x)= 6.23532e+22\n",
      "differential_evolution step 32: f(x)= 6.23532e+22\n",
      "differential_evolution step 33: f(x)= 6.23532e+22\n",
      "differential_evolution step 34: f(x)= 6.23532e+22\n",
      "differential_evolution step 35: f(x)= 6.23532e+22\n",
      "differential_evolution step 36: f(x)= 6.23532e+22\n",
      "differential_evolution step 37: f(x)= 6.23532e+22\n",
      "differential_evolution step 38: f(x)= 3.11255e+22\n",
      "differential_evolution step 39: f(x)= 3.11255e+22\n",
      "differential_evolution step 40: f(x)= 3.11255e+22\n",
      "differential_evolution step 41: f(x)= 3.11255e+22\n",
      "differential_evolution step 42: f(x)= 3.11255e+22\n",
      "differential_evolution step 43: f(x)= 3.11255e+22\n",
      "differential_evolution step 44: f(x)= 3.11255e+22\n",
      "differential_evolution step 45: f(x)= 1.04983e+22\n",
      "differential_evolution step 46: f(x)= 1.04983e+22\n",
      "differential_evolution step 47: f(x)= 1.04983e+22\n",
      "differential_evolution step 48: f(x)= 1.04983e+22\n",
      "differential_evolution step 49: f(x)= 1.04403e+22\n",
      "differential_evolution step 50: f(x)= 1.04403e+22\n",
      "differential_evolution step 51: f(x)= 8.35895e+21\n",
      "differential_evolution step 52: f(x)= 5.80318e+21\n",
      "differential_evolution step 53: f(x)= 5.80318e+21\n",
      "differential_evolution step 54: f(x)= 5.80318e+21\n",
      "differential_evolution step 55: f(x)= 6.77415e+20\n",
      "differential_evolution step 56: f(x)= 6.77415e+20\n",
      "differential_evolution step 57: f(x)= 2.52962e+20\n",
      "differential_evolution step 58: f(x)= 2.52962e+20\n",
      "differential_evolution step 59: f(x)= 2.52962e+20\n",
      "differential_evolution step 60: f(x)= 2.52962e+20\n",
      "differential_evolution step 61: f(x)= 1.5216e+20\n",
      "differential_evolution step 62: f(x)= 1.5216e+20\n",
      "differential_evolution step 63: f(x)= 2.28896e+19\n",
      "differential_evolution step 64: f(x)= 2.28896e+19\n",
      "differential_evolution step 65: f(x)= 2.28896e+19\n",
      "differential_evolution step 66: f(x)= 2.08708e+19\n",
      "differential_evolution step 67: f(x)= 2.08708e+19\n",
      "differential_evolution step 68: f(x)= 2.08708e+19\n",
      "differential_evolution step 69: f(x)= 2.08708e+19\n",
      "differential_evolution step 70: f(x)= 2.08708e+19\n",
      "differential_evolution step 71: f(x)= 2.08708e+19\n",
      "differential_evolution step 72: f(x)= 2.08708e+19\n",
      "differential_evolution step 73: f(x)= 1.73168e+19\n",
      "differential_evolution step 74: f(x)= 1.73168e+19\n",
      "differential_evolution step 75: f(x)= 1.73168e+19\n",
      "differential_evolution step 76: f(x)= 1.73168e+19\n",
      "differential_evolution step 77: f(x)= 1.73168e+19\n",
      "differential_evolution step 78: f(x)= 1.73168e+19\n",
      "differential_evolution step 79: f(x)= 1.73168e+19\n",
      "differential_evolution step 80: f(x)= 1.73168e+19\n",
      "differential_evolution step 81: f(x)= 1.73168e+19\n",
      "differential_evolution step 82: f(x)= 1.35511e+19\n",
      "differential_evolution step 83: f(x)= 8.98245e+18\n",
      "differential_evolution step 84: f(x)= 3.51418e+18\n",
      "differential_evolution step 85: f(x)= 3.51418e+18\n",
      "differential_evolution step 86: f(x)= 3.12756e+18\n",
      "differential_evolution step 87: f(x)= 3.12756e+18\n",
      "differential_evolution step 88: f(x)= 3.12756e+18\n",
      "differential_evolution step 89: f(x)= 3.12756e+18\n",
      "differential_evolution step 90: f(x)= 3.12756e+18\n",
      "differential_evolution step 91: f(x)= 3.12756e+18\n",
      "differential_evolution step 92: f(x)= 2.59142e+18\n",
      "differential_evolution step 93: f(x)= 2.59142e+18\n",
      "differential_evolution step 94: f(x)= 4.03225e+17\n",
      "differential_evolution step 95: f(x)= 4.03225e+17\n",
      "differential_evolution step 96: f(x)= 4.03225e+17\n",
      "differential_evolution step 97: f(x)= 4.03225e+17\n",
      "differential_evolution step 98: f(x)= 2.37352e+17\n",
      "differential_evolution step 99: f(x)= 2.37352e+17\n",
      "differential_evolution step 100: f(x)= 2.37352e+17\n",
      "differential_evolution step 101: f(x)= 2.37352e+17\n",
      "differential_evolution step 102: f(x)= 2.37352e+17\n",
      "differential_evolution step 103: f(x)= 2.37352e+17\n",
      "differential_evolution step 104: f(x)= 2.37352e+17\n",
      "differential_evolution step 105: f(x)= 9.38534e+16\n",
      "differential_evolution step 106: f(x)= 9.38534e+16\n",
      "differential_evolution step 107: f(x)= 9.38534e+16\n",
      "differential_evolution step 108: f(x)= 9.38534e+16\n",
      "differential_evolution step 109: f(x)= 9.38534e+16\n",
      "differential_evolution step 110: f(x)= 9.38534e+16\n",
      "differential_evolution step 111: f(x)= 9.38534e+16\n",
      "differential_evolution step 112: f(x)= 9.38534e+16\n",
      "differential_evolution step 113: f(x)= 9.38534e+16\n",
      "differential_evolution step 114: f(x)= 9.38534e+16\n",
      "differential_evolution step 115: f(x)= 9.38534e+16\n",
      "differential_evolution step 116: f(x)= 9.38534e+16\n",
      "differential_evolution step 117: f(x)= 9.38534e+16\n",
      "differential_evolution step 118: f(x)= 9.38534e+16\n",
      "differential_evolution step 119: f(x)= 9.38534e+16\n",
      "differential_evolution step 120: f(x)= 9.38534e+16\n",
      "differential_evolution step 121: f(x)= 9.38534e+16\n",
      "differential_evolution step 122: f(x)= 9.38534e+16\n",
      "differential_evolution step 123: f(x)= 9.38534e+16\n",
      "differential_evolution step 124: f(x)= 9.38534e+16\n",
      "differential_evolution step 125: f(x)= 9.38534e+16\n",
      "differential_evolution step 126: f(x)= 8.94317e+16\n",
      "differential_evolution step 127: f(x)= 8.94317e+16\n",
      "differential_evolution step 128: f(x)= 8.94317e+16\n",
      "differential_evolution step 129: f(x)= 8.94317e+16\n",
      "differential_evolution step 130: f(x)= 8.94317e+16\n",
      "differential_evolution step 131: f(x)= 8.94317e+16\n",
      "differential_evolution step 132: f(x)= 8.94317e+16\n",
      "differential_evolution step 133: f(x)= 7.72974e+16\n",
      "differential_evolution step 134: f(x)= 4.09256e+16\n",
      "differential_evolution step 135: f(x)= 4.09256e+16\n",
      "differential_evolution step 136: f(x)= 2.8018e+16\n",
      "differential_evolution step 137: f(x)= 2.46217e+16\n",
      "differential_evolution step 138: f(x)= 2.46217e+16\n",
      "differential_evolution step 139: f(x)= 2.46217e+16\n",
      "differential_evolution step 140: f(x)= 2.46217e+16\n",
      "differential_evolution step 141: f(x)= 2.46217e+16\n",
      "differential_evolution step 142: f(x)= 1.68331e+16\n",
      "differential_evolution step 143: f(x)= 1.68331e+16\n",
      "differential_evolution step 144: f(x)= 1.68331e+16\n",
      "differential_evolution step 145: f(x)= 1.68331e+16\n",
      "differential_evolution step 146: f(x)= 1.46183e+16\n",
      "differential_evolution step 147: f(x)= 1.46183e+16\n",
      "differential_evolution step 148: f(x)= 1.46183e+16\n",
      "differential_evolution step 149: f(x)= 1.46183e+16\n",
      "differential_evolution step 150: f(x)= 1.46183e+16\n",
      "differential_evolution step 151: f(x)= 1.46183e+16\n",
      "differential_evolution step 152: f(x)= 1.46183e+16\n",
      "differential_evolution step 153: f(x)= 1.46183e+16\n",
      "differential_evolution step 154: f(x)= 1.20548e+16\n",
      "differential_evolution step 155: f(x)= 1.01409e+16\n",
      "differential_evolution step 156: f(x)= 6.41504e+15\n",
      "differential_evolution step 157: f(x)= 6.41504e+15\n",
      "differential_evolution step 158: f(x)= 5.90745e+15\n",
      "differential_evolution step 159: f(x)= 5.90745e+15\n",
      "differential_evolution step 160: f(x)= 4.84381e+15\n",
      "differential_evolution step 161: f(x)= 4.84381e+15\n",
      "differential_evolution step 162: f(x)= 1.26025e+15\n",
      "differential_evolution step 163: f(x)= 1.26025e+15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "differential_evolution step 164: f(x)= 1.26025e+15\n",
      "differential_evolution step 165: f(x)= 1.26025e+15\n",
      "differential_evolution step 166: f(x)= 1.26025e+15\n",
      "differential_evolution step 167: f(x)= 1.26025e+15\n",
      "differential_evolution step 168: f(x)= 1.26025e+15\n",
      "differential_evolution step 169: f(x)= 1.26025e+15\n",
      "differential_evolution step 170: f(x)= 1.26025e+15\n",
      "differential_evolution step 171: f(x)= 1.26025e+15\n",
      "differential_evolution step 172: f(x)= 1.26025e+15\n",
      "differential_evolution step 173: f(x)= 1.10944e+15\n",
      "differential_evolution step 174: f(x)= 1.10944e+15\n",
      "differential_evolution step 175: f(x)= 1.10944e+15\n",
      "differential_evolution step 176: f(x)= 1.10944e+15\n",
      "differential_evolution step 177: f(x)= 1.10944e+15\n",
      "differential_evolution step 178: f(x)= 1.10944e+15\n",
      "differential_evolution step 179: f(x)= 1.10944e+15\n",
      "differential_evolution step 180: f(x)= 1.10944e+15\n",
      "differential_evolution step 181: f(x)= 1.10944e+15\n",
      "differential_evolution step 182: f(x)= 1.10944e+15\n",
      "differential_evolution step 183: f(x)= 1.10944e+15\n",
      "differential_evolution step 184: f(x)= 1.10944e+15\n",
      "differential_evolution step 185: f(x)= 1.10944e+15\n",
      "differential_evolution step 186: f(x)= 1.10944e+15\n",
      "differential_evolution step 187: f(x)= 1.10944e+15\n",
      "differential_evolution step 188: f(x)= 1.10944e+15\n",
      "differential_evolution step 189: f(x)= 1.10944e+15\n",
      "differential_evolution step 190: f(x)= 1.10944e+15\n",
      "differential_evolution step 191: f(x)= 2.73559e+14\n",
      "differential_evolution step 192: f(x)= 2.73559e+14\n",
      "differential_evolution step 193: f(x)= 2.73559e+14\n",
      "differential_evolution step 194: f(x)= 2.73559e+14\n",
      "differential_evolution step 195: f(x)= 2.73559e+14\n",
      "differential_evolution step 196: f(x)= 2.73559e+14\n",
      "differential_evolution step 197: f(x)= 2.73559e+14\n",
      "differential_evolution step 198: f(x)= 1.57129e+14\n",
      "differential_evolution step 199: f(x)= 1.57129e+14\n",
      "differential_evolution step 200: f(x)= 1.57129e+14\n",
      "differential_evolution step 201: f(x)= 1.57129e+14\n",
      "differential_evolution step 202: f(x)= 1.57129e+14\n",
      "differential_evolution step 203: f(x)= 1.57129e+14\n",
      "differential_evolution step 204: f(x)= 1.57129e+14\n",
      "differential_evolution step 205: f(x)= 1.57129e+14\n",
      "differential_evolution step 206: f(x)= 1.57129e+14\n",
      "differential_evolution step 207: f(x)= 1.57129e+14\n",
      "differential_evolution step 208: f(x)= 1.57129e+14\n",
      "differential_evolution step 209: f(x)= 1.01812e+14\n",
      "differential_evolution step 210: f(x)= 1.01812e+14\n",
      "differential_evolution step 211: f(x)= 1.01812e+14\n",
      "differential_evolution step 212: f(x)= 1.01812e+14\n",
      "differential_evolution step 213: f(x)= 4.48009e+13\n",
      "differential_evolution step 214: f(x)= 4.48009e+13\n",
      "differential_evolution step 215: f(x)= 4.48009e+13\n",
      "differential_evolution step 216: f(x)= 4.48009e+13\n",
      "differential_evolution step 217: f(x)= 4.48009e+13\n",
      "differential_evolution step 218: f(x)= 4.48009e+13\n",
      "differential_evolution step 219: f(x)= 4.48009e+13\n",
      "differential_evolution step 220: f(x)= 4.48009e+13\n",
      "differential_evolution step 221: f(x)= 4.48009e+13\n",
      "differential_evolution step 222: f(x)= 4.48009e+13\n",
      "differential_evolution step 223: f(x)= 4.48009e+13\n",
      "differential_evolution step 224: f(x)= 4.48009e+13\n",
      "differential_evolution step 225: f(x)= 4.48009e+13\n",
      "differential_evolution step 226: f(x)= 4.48009e+13\n",
      "differential_evolution step 227: f(x)= 4.48009e+13\n",
      "differential_evolution step 228: f(x)= 2.27447e+13\n",
      "differential_evolution step 229: f(x)= 2.27447e+13\n",
      "differential_evolution step 230: f(x)= 2.27447e+13\n",
      "differential_evolution step 231: f(x)= 2.27447e+13\n",
      "differential_evolution step 232: f(x)= 2.27447e+13\n",
      "differential_evolution step 233: f(x)= 2.27447e+13\n",
      "differential_evolution step 234: f(x)= 2.27447e+13\n",
      "differential_evolution step 235: f(x)= 2.27447e+13\n",
      "differential_evolution step 236: f(x)= 2.27447e+13\n",
      "differential_evolution step 237: f(x)= 2.27447e+13\n",
      "differential_evolution step 238: f(x)= 1.43107e+13\n",
      "differential_evolution step 239: f(x)= 1.43107e+13\n",
      "differential_evolution step 240: f(x)= 1.43107e+13\n",
      "differential_evolution step 241: f(x)= 1.43107e+13\n",
      "differential_evolution step 242: f(x)= 1.43107e+13\n",
      "differential_evolution step 243: f(x)= 1.43107e+13\n",
      "differential_evolution step 244: f(x)= 1.43107e+13\n",
      "differential_evolution step 245: f(x)= 1.43107e+13\n",
      "differential_evolution step 246: f(x)= 1.43107e+13\n",
      "differential_evolution step 247: f(x)= 1.43107e+13\n",
      "differential_evolution step 248: f(x)= 1.43107e+13\n",
      "differential_evolution step 249: f(x)= 1.43107e+13\n",
      "differential_evolution step 250: f(x)= 1.43107e+13\n",
      "differential_evolution step 251: f(x)= 1.43107e+13\n",
      "differential_evolution step 252: f(x)= 1.43107e+13\n",
      "differential_evolution step 253: f(x)= 1.43107e+13\n",
      "differential_evolution step 254: f(x)= 1.43107e+13\n",
      "differential_evolution step 255: f(x)= 5.41566e+12\n",
      "differential_evolution step 256: f(x)= 5.41566e+12\n",
      "differential_evolution step 257: f(x)= 5.41566e+12\n",
      "differential_evolution step 258: f(x)= 5.41566e+12\n",
      "differential_evolution step 259: f(x)= 5.41566e+12\n",
      "differential_evolution step 260: f(x)= 5.41566e+12\n",
      "differential_evolution step 261: f(x)= 5.41566e+12\n",
      "differential_evolution step 262: f(x)= 5.41566e+12\n",
      "differential_evolution step 263: f(x)= 5.41566e+12\n",
      "differential_evolution step 264: f(x)= 5.41566e+12\n",
      "differential_evolution step 265: f(x)= 5.41566e+12\n",
      "differential_evolution step 266: f(x)= 5.41566e+12\n",
      "differential_evolution step 267: f(x)= 5.41566e+12\n",
      "differential_evolution step 268: f(x)= 5.41566e+12\n",
      "differential_evolution step 269: f(x)= 5.41566e+12\n",
      "differential_evolution step 270: f(x)= 5.41566e+12\n",
      "differential_evolution step 271: f(x)= 5.41566e+12\n",
      "differential_evolution step 272: f(x)= 5.41566e+12\n",
      "differential_evolution step 273: f(x)= 5.41566e+12\n",
      "differential_evolution step 274: f(x)= 5.41566e+12\n",
      "differential_evolution step 275: f(x)= 5.41566e+12\n",
      "differential_evolution step 276: f(x)= 5.41566e+12\n",
      "differential_evolution step 277: f(x)= 4.10179e+12\n",
      "differential_evolution step 278: f(x)= 4.10179e+12\n",
      "differential_evolution step 279: f(x)= 3.21075e+12\n",
      "differential_evolution step 280: f(x)= 3.21075e+12\n",
      "differential_evolution step 281: f(x)= 3.21075e+12\n",
      "differential_evolution step 282: f(x)= 1.57444e+12\n",
      "differential_evolution step 283: f(x)= 1.57444e+12\n",
      "differential_evolution step 284: f(x)= 1.57444e+12\n",
      "differential_evolution step 285: f(x)= 1.57444e+12\n",
      "differential_evolution step 286: f(x)= 1.57444e+12\n",
      "differential_evolution step 287: f(x)= 4.16838e+11\n",
      "differential_evolution step 288: f(x)= 4.16838e+11\n",
      "differential_evolution step 289: f(x)= 4.16838e+11\n",
      "differential_evolution step 290: f(x)= 4.16838e+11\n",
      "differential_evolution step 291: f(x)= 4.16838e+11\n",
      "differential_evolution step 292: f(x)= 4.16838e+11\n",
      "differential_evolution step 293: f(x)= 4.16838e+11\n",
      "differential_evolution step 294: f(x)= 4.16838e+11\n",
      "differential_evolution step 295: f(x)= 4.16838e+11\n",
      "differential_evolution step 296: f(x)= 4.16838e+11\n",
      "differential_evolution step 297: f(x)= 4.16838e+11\n",
      "differential_evolution step 298: f(x)= 4.16838e+11\n",
      "differential_evolution step 299: f(x)= 4.16838e+11\n",
      "differential_evolution step 300: f(x)= 4.16838e+11\n",
      "differential_evolution step 301: f(x)= 4.16838e+11\n",
      "differential_evolution step 302: f(x)= 4.16838e+11\n",
      "differential_evolution step 303: f(x)= 4.16838e+11\n",
      "differential_evolution step 304: f(x)= 4.16838e+11\n",
      "differential_evolution step 305: f(x)= 4.16838e+11\n",
      "differential_evolution step 306: f(x)= 4.16838e+11\n",
      "differential_evolution step 307: f(x)= 4.16838e+11\n",
      "differential_evolution step 308: f(x)= 4.16838e+11\n",
      "differential_evolution step 309: f(x)= 4.16838e+11\n",
      "differential_evolution step 310: f(x)= 4.16838e+11\n",
      "differential_evolution step 311: f(x)= 4.16838e+11\n",
      "differential_evolution step 312: f(x)= 4.16838e+11\n",
      "differential_evolution step 313: f(x)= 4.16838e+11\n",
      "differential_evolution step 314: f(x)= 4.16838e+11\n",
      "differential_evolution step 315: f(x)= 4.16838e+11\n",
      "differential_evolution step 316: f(x)= 4.16838e+11\n",
      "differential_evolution step 317: f(x)= 4.16838e+11\n",
      "differential_evolution step 318: f(x)= 4.16838e+11\n",
      "differential_evolution step 319: f(x)= 4.16838e+11\n",
      "differential_evolution step 320: f(x)= 4.16838e+11\n",
      "differential_evolution step 321: f(x)= 4.16838e+11\n",
      "differential_evolution step 322: f(x)= 4.16838e+11\n",
      "differential_evolution step 323: f(x)= 4.16838e+11\n",
      "differential_evolution step 324: f(x)= 4.16838e+11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "differential_evolution step 325: f(x)= 4.16838e+11\n",
      "differential_evolution step 326: f(x)= 4.16838e+11\n",
      "differential_evolution step 327: f(x)= 4.16838e+11\n",
      "differential_evolution step 328: f(x)= 4.16838e+11\n",
      "differential_evolution step 329: f(x)= 4.16838e+11\n",
      "differential_evolution step 330: f(x)= 4.16838e+11\n",
      "differential_evolution step 331: f(x)= 4.16838e+11\n",
      "differential_evolution step 332: f(x)= 3.31538e+11\n",
      "differential_evolution step 333: f(x)= 3.31538e+11\n",
      "differential_evolution step 334: f(x)= 3.31538e+11\n",
      "differential_evolution step 335: f(x)= 3.31538e+11\n",
      "differential_evolution step 336: f(x)= 3.31538e+11\n",
      "differential_evolution step 337: f(x)= 3.31538e+11\n",
      "differential_evolution step 338: f(x)= 3.31538e+11\n",
      "differential_evolution step 339: f(x)= 3.31538e+11\n",
      "differential_evolution step 340: f(x)= 3.06859e+11\n",
      "differential_evolution step 341: f(x)= 3.06859e+11\n",
      "differential_evolution step 342: f(x)= 3.06859e+11\n",
      "differential_evolution step 343: f(x)= 3.06859e+11\n",
      "differential_evolution step 344: f(x)= 3.06859e+11\n",
      "differential_evolution step 345: f(x)= 2.98742e+11\n",
      "differential_evolution step 346: f(x)= 2.98742e+11\n",
      "differential_evolution step 347: f(x)= 2.98742e+11\n",
      "differential_evolution step 348: f(x)= 2.98742e+11\n",
      "differential_evolution step 349: f(x)= 2.98742e+11\n",
      "differential_evolution step 350: f(x)= 2.98742e+11\n",
      "differential_evolution step 351: f(x)= 2.88426e+11\n",
      "differential_evolution step 352: f(x)= 2.88426e+11\n",
      "differential_evolution step 353: f(x)= 2.88426e+11\n",
      "differential_evolution step 354: f(x)= 2.88426e+11\n",
      "differential_evolution step 355: f(x)= 2.88426e+11\n",
      "differential_evolution step 356: f(x)= 2.88426e+11\n",
      "differential_evolution step 357: f(x)= 2.88426e+11\n",
      "differential_evolution step 358: f(x)= 1.69069e+11\n",
      "differential_evolution step 359: f(x)= 1.69069e+11\n",
      "differential_evolution step 360: f(x)= 1.69069e+11\n",
      "differential_evolution step 361: f(x)= 1.18203e+11\n",
      "differential_evolution step 362: f(x)= 1.18203e+11\n",
      "differential_evolution step 363: f(x)= 1.18203e+11\n",
      "differential_evolution step 364: f(x)= 1.18203e+11\n",
      "differential_evolution step 365: f(x)= 1.18203e+11\n",
      "differential_evolution step 366: f(x)= 1.18203e+11\n",
      "differential_evolution step 367: f(x)= 7.39718e+10\n",
      "differential_evolution step 368: f(x)= 7.39718e+10\n",
      "differential_evolution step 369: f(x)= 7.39718e+10\n",
      "differential_evolution step 370: f(x)= 7.39718e+10\n",
      "differential_evolution step 371: f(x)= 7.39718e+10\n",
      "differential_evolution step 372: f(x)= 7.39718e+10\n",
      "differential_evolution step 373: f(x)= 6.51448e+10\n",
      "differential_evolution step 374: f(x)= 6.51448e+10\n",
      "differential_evolution step 375: f(x)= 6.51448e+10\n",
      "differential_evolution step 376: f(x)= 6.51448e+10\n",
      "differential_evolution step 377: f(x)= 6.51448e+10\n",
      "differential_evolution step 378: f(x)= 6.51448e+10\n",
      "differential_evolution step 379: f(x)= 6.51448e+10\n",
      "differential_evolution step 380: f(x)= 6.51448e+10\n",
      "differential_evolution step 381: f(x)= 6.51448e+10\n",
      "differential_evolution step 382: f(x)= 6.51448e+10\n",
      "differential_evolution step 383: f(x)= 6.51448e+10\n",
      "differential_evolution step 384: f(x)= 6.51448e+10\n",
      "differential_evolution step 385: f(x)= 6.51448e+10\n",
      "differential_evolution step 386: f(x)= 6.51448e+10\n",
      "differential_evolution step 387: f(x)= 6.51448e+10\n",
      "differential_evolution step 388: f(x)= 5.74939e+10\n",
      "differential_evolution step 389: f(x)= 5.74939e+10\n",
      "differential_evolution step 390: f(x)= 5.74939e+10\n",
      "differential_evolution step 391: f(x)= 5.74939e+10\n",
      "differential_evolution step 392: f(x)= 5.74939e+10\n",
      "differential_evolution step 393: f(x)= 5.74939e+10\n",
      "differential_evolution step 394: f(x)= 5.74939e+10\n",
      "differential_evolution step 395: f(x)= 5.74939e+10\n",
      "differential_evolution step 396: f(x)= 5.74939e+10\n",
      "differential_evolution step 397: f(x)= 5.74939e+10\n",
      "differential_evolution step 398: f(x)= 5.74939e+10\n",
      "differential_evolution step 399: f(x)= 4.3628e+10\n",
      "differential_evolution step 400: f(x)= 4.3628e+10\n",
      "differential_evolution step 401: f(x)= 4.3628e+10\n",
      "differential_evolution step 402: f(x)= 4.3628e+10\n",
      "differential_evolution step 403: f(x)= 3.51859e+10\n",
      "differential_evolution step 404: f(x)= 3.17464e+10\n",
      "differential_evolution step 405: f(x)= 3.17464e+10\n",
      "differential_evolution step 406: f(x)= 3.17464e+10\n",
      "differential_evolution step 407: f(x)= 3.17464e+10\n",
      "differential_evolution step 408: f(x)= 3.17464e+10\n",
      "differential_evolution step 409: f(x)= 3.17464e+10\n",
      "differential_evolution step 410: f(x)= 3.17464e+10\n",
      "differential_evolution step 411: f(x)= 3.10734e+10\n",
      "differential_evolution step 412: f(x)= 3.10734e+10\n",
      "differential_evolution step 413: f(x)= 2.55503e+10\n",
      "differential_evolution step 414: f(x)= 2.55503e+10\n",
      "differential_evolution step 415: f(x)= 2.00594e+10\n",
      "differential_evolution step 416: f(x)= 2.00594e+10\n",
      "differential_evolution step 417: f(x)= 2.00594e+10\n",
      "differential_evolution step 418: f(x)= 2.00594e+10\n",
      "differential_evolution step 419: f(x)= 2.00594e+10\n",
      "differential_evolution step 420: f(x)= 1.6942e+10\n",
      "differential_evolution step 421: f(x)= 1.6942e+10\n",
      "differential_evolution step 422: f(x)= 1.6942e+10\n",
      "differential_evolution step 423: f(x)= 1.6942e+10\n",
      "differential_evolution step 424: f(x)= 1.6942e+10\n",
      "differential_evolution step 425: f(x)= 1.6942e+10\n",
      "differential_evolution step 426: f(x)= 1.16562e+10\n",
      "differential_evolution step 427: f(x)= 1.16562e+10\n",
      "differential_evolution step 428: f(x)= 1.16562e+10\n",
      "differential_evolution step 429: f(x)= 1.16562e+10\n",
      "differential_evolution step 430: f(x)= 1.16562e+10\n",
      "differential_evolution step 431: f(x)= 1.16562e+10\n",
      "differential_evolution step 432: f(x)= 1.16562e+10\n",
      "differential_evolution step 433: f(x)= 1.04707e+10\n",
      "differential_evolution step 434: f(x)= 8.3677e+09\n",
      "differential_evolution step 435: f(x)= 8.3677e+09\n",
      "differential_evolution step 436: f(x)= 4.68682e+09\n",
      "differential_evolution step 437: f(x)= 4.68682e+09\n",
      "differential_evolution step 438: f(x)= 4.68682e+09\n",
      "differential_evolution step 439: f(x)= 4.68682e+09\n",
      "differential_evolution step 440: f(x)= 4.68682e+09\n",
      "differential_evolution step 441: f(x)= 4.68682e+09\n",
      "differential_evolution step 442: f(x)= 4.68682e+09\n",
      "differential_evolution step 443: f(x)= 4.68682e+09\n",
      "differential_evolution step 444: f(x)= 4.68682e+09\n",
      "differential_evolution step 445: f(x)= 4.68682e+09\n",
      "differential_evolution step 446: f(x)= 4.68682e+09\n",
      "differential_evolution step 447: f(x)= 4.68682e+09\n",
      "differential_evolution step 448: f(x)= 4.68682e+09\n",
      "differential_evolution step 449: f(x)= 4.68682e+09\n",
      "differential_evolution step 450: f(x)= 4.68682e+09\n",
      "differential_evolution step 451: f(x)= 4.68682e+09\n",
      "differential_evolution step 452: f(x)= 4.31476e+09\n",
      "differential_evolution step 453: f(x)= 4.31476e+09\n",
      "differential_evolution step 454: f(x)= 4.31476e+09\n",
      "differential_evolution step 455: f(x)= 3.62413e+09\n",
      "differential_evolution step 456: f(x)= 3.62413e+09\n",
      "differential_evolution step 457: f(x)= 3.62413e+09\n",
      "differential_evolution step 458: f(x)= 3.62413e+09\n",
      "differential_evolution step 459: f(x)= 3.00832e+09\n",
      "differential_evolution step 460: f(x)= 3.00832e+09\n",
      "differential_evolution step 461: f(x)= 3.00832e+09\n",
      "differential_evolution step 462: f(x)= 1.95172e+09\n",
      "differential_evolution step 463: f(x)= 1.95172e+09\n",
      "differential_evolution step 464: f(x)= 1.95172e+09\n",
      "differential_evolution step 465: f(x)= 1.95172e+09\n",
      "differential_evolution step 466: f(x)= 1.95172e+09\n",
      "differential_evolution step 467: f(x)= 1.95172e+09\n",
      "differential_evolution step 468: f(x)= 1.95172e+09\n",
      "differential_evolution step 469: f(x)= 1.95172e+09\n",
      "differential_evolution step 470: f(x)= 1.95172e+09\n",
      "differential_evolution step 471: f(x)= 1.95172e+09\n",
      "differential_evolution step 472: f(x)= 1.95172e+09\n",
      "differential_evolution step 473: f(x)= 1.95172e+09\n",
      "differential_evolution step 474: f(x)= 1.95172e+09\n",
      "differential_evolution step 475: f(x)= 1.95172e+09\n",
      "differential_evolution step 476: f(x)= 1.95172e+09\n",
      "differential_evolution step 477: f(x)= 1.95172e+09\n",
      "differential_evolution step 478: f(x)= 1.95172e+09\n",
      "differential_evolution step 479: f(x)= 1.95172e+09\n",
      "differential_evolution step 480: f(x)= 1.95172e+09\n",
      "differential_evolution step 481: f(x)= 1.95172e+09\n",
      "differential_evolution step 482: f(x)= 1.95172e+09\n",
      "differential_evolution step 483: f(x)= 1.95172e+09\n",
      "differential_evolution step 484: f(x)= 1.95172e+09\n",
      "differential_evolution step 485: f(x)= 1.95172e+09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "differential_evolution step 486: f(x)= 1.95172e+09\n",
      "differential_evolution step 487: f(x)= 1.95172e+09\n",
      "differential_evolution step 488: f(x)= 1.95172e+09\n",
      "differential_evolution step 489: f(x)= 1.95172e+09\n",
      "differential_evolution step 490: f(x)= 4.62951e+08\n",
      "differential_evolution step 491: f(x)= 4.62951e+08\n",
      "differential_evolution step 492: f(x)= 4.62951e+08\n",
      "differential_evolution step 493: f(x)= 4.62951e+08\n",
      "differential_evolution step 494: f(x)= 4.62951e+08\n",
      "differential_evolution step 495: f(x)= 4.62951e+08\n",
      "differential_evolution step 496: f(x)= 4.62951e+08\n",
      "differential_evolution step 497: f(x)= 4.62951e+08\n",
      "differential_evolution step 498: f(x)= 4.62951e+08\n",
      "differential_evolution step 499: f(x)= 4.62951e+08\n",
      "differential_evolution step 500: f(x)= 4.62951e+08\n",
      "differential_evolution step 501: f(x)= 4.62951e+08\n",
      "differential_evolution step 502: f(x)= 4.62951e+08\n",
      "differential_evolution step 503: f(x)= 4.62951e+08\n",
      "differential_evolution step 504: f(x)= 4.62951e+08\n",
      "differential_evolution step 505: f(x)= 4.62951e+08\n",
      "differential_evolution step 506: f(x)= 4.62951e+08\n",
      "differential_evolution step 507: f(x)= 4.62951e+08\n",
      "differential_evolution step 508: f(x)= 4.62951e+08\n",
      "differential_evolution step 509: f(x)= 4.62951e+08\n",
      "differential_evolution step 510: f(x)= 4.62951e+08\n",
      "differential_evolution step 511: f(x)= 4.62951e+08\n",
      "differential_evolution step 512: f(x)= 4.62951e+08\n",
      "differential_evolution step 513: f(x)= 4.62951e+08\n",
      "differential_evolution step 514: f(x)= 4.62951e+08\n",
      "differential_evolution step 515: f(x)= 4.62951e+08\n",
      "differential_evolution step 516: f(x)= 4.62951e+08\n",
      "differential_evolution step 517: f(x)= 4.62951e+08\n",
      "differential_evolution step 518: f(x)= 4.62951e+08\n",
      "differential_evolution step 519: f(x)= 4.62951e+08\n",
      "differential_evolution step 520: f(x)= 4.62951e+08\n",
      "differential_evolution step 521: f(x)= 4.62951e+08\n",
      "differential_evolution step 522: f(x)= 4.62951e+08\n",
      "differential_evolution step 523: f(x)= 4.62951e+08\n",
      "differential_evolution step 524: f(x)= 4.62951e+08\n",
      "differential_evolution step 525: f(x)= 4.62951e+08\n",
      "differential_evolution step 526: f(x)= 4.62951e+08\n",
      "differential_evolution step 527: f(x)= 4.62951e+08\n",
      "differential_evolution step 528: f(x)= 4.62951e+08\n",
      "differential_evolution step 529: f(x)= 2.24953e+08\n",
      "differential_evolution step 530: f(x)= 2.24953e+08\n",
      "differential_evolution step 531: f(x)= 2.24953e+08\n",
      "differential_evolution step 532: f(x)= 2.24953e+08\n",
      "differential_evolution step 533: f(x)= 2.24953e+08\n",
      "differential_evolution step 534: f(x)= 2.24953e+08\n",
      "differential_evolution step 535: f(x)= 2.24953e+08\n",
      "differential_evolution step 536: f(x)= 2.24953e+08\n",
      "differential_evolution step 537: f(x)= 2.24953e+08\n",
      "differential_evolution step 538: f(x)= 2.24953e+08\n",
      "differential_evolution step 539: f(x)= 2.24953e+08\n",
      "differential_evolution step 540: f(x)= 2.24953e+08\n",
      "differential_evolution step 541: f(x)= 2.24953e+08\n",
      "differential_evolution step 542: f(x)= 2.24953e+08\n",
      "differential_evolution step 543: f(x)= 2.24953e+08\n",
      "differential_evolution step 544: f(x)= 1.37592e+08\n",
      "differential_evolution step 545: f(x)= 1.37592e+08\n",
      "differential_evolution step 546: f(x)= 1.37592e+08\n",
      "differential_evolution step 547: f(x)= 1.37592e+08\n",
      "differential_evolution step 548: f(x)= 1.37041e+08\n",
      "differential_evolution step 549: f(x)= 1.37041e+08\n",
      "differential_evolution step 550: f(x)= 1.37041e+08\n",
      "differential_evolution step 551: f(x)= 1.37041e+08\n",
      "differential_evolution step 552: f(x)= 1.37041e+08\n",
      "differential_evolution step 553: f(x)= 1.37041e+08\n",
      "differential_evolution step 554: f(x)= 1.37041e+08\n",
      "differential_evolution step 555: f(x)= 1.2426e+08\n",
      "differential_evolution step 556: f(x)= 1.2426e+08\n",
      "differential_evolution step 557: f(x)= 1.2426e+08\n",
      "differential_evolution step 558: f(x)= 8.85432e+07\n",
      "differential_evolution step 559: f(x)= 8.85432e+07\n",
      "differential_evolution step 560: f(x)= 8.85432e+07\n",
      "differential_evolution step 561: f(x)= 8.67354e+07\n",
      "differential_evolution step 562: f(x)= 8.67354e+07\n",
      "differential_evolution step 563: f(x)= 8.67354e+07\n",
      "differential_evolution step 564: f(x)= 7.01427e+07\n",
      "differential_evolution step 565: f(x)= 7.01427e+07\n",
      "differential_evolution step 566: f(x)= 7.01427e+07\n",
      "differential_evolution step 567: f(x)= 7.01427e+07\n",
      "differential_evolution step 568: f(x)= 7.01427e+07\n",
      "differential_evolution step 569: f(x)= 7.01427e+07\n",
      "differential_evolution step 570: f(x)= 7.01427e+07\n",
      "differential_evolution step 571: f(x)= 6.21287e+07\n",
      "differential_evolution step 572: f(x)= 4.51703e+07\n",
      "differential_evolution step 573: f(x)= 4.51703e+07\n",
      "differential_evolution step 574: f(x)= 4.51703e+07\n",
      "differential_evolution step 575: f(x)= 4.51703e+07\n",
      "differential_evolution step 576: f(x)= 4.51703e+07\n",
      "differential_evolution step 577: f(x)= 4.51703e+07\n",
      "differential_evolution step 578: f(x)= 4.51703e+07\n",
      "differential_evolution step 579: f(x)= 4.51703e+07\n",
      "differential_evolution step 580: f(x)= 4.51703e+07\n",
      "differential_evolution step 581: f(x)= 4.51703e+07\n",
      "differential_evolution step 582: f(x)= 4.51703e+07\n",
      "differential_evolution step 583: f(x)= 3.80403e+07\n",
      "differential_evolution step 584: f(x)= 3.80403e+07\n",
      "differential_evolution step 585: f(x)= 3.80403e+07\n",
      "differential_evolution step 586: f(x)= 3.80403e+07\n",
      "differential_evolution step 587: f(x)= 3.80403e+07\n",
      "differential_evolution step 588: f(x)= 3.80403e+07\n",
      "differential_evolution step 589: f(x)= 3.80403e+07\n",
      "differential_evolution step 590: f(x)= 3.80403e+07\n",
      "differential_evolution step 591: f(x)= 2.45465e+07\n",
      "differential_evolution step 592: f(x)= 2.45465e+07\n",
      "differential_evolution step 593: f(x)= 2.45465e+07\n",
      "differential_evolution step 594: f(x)= 2.45465e+07\n",
      "differential_evolution step 595: f(x)= 2.45465e+07\n",
      "differential_evolution step 596: f(x)= 2.45465e+07\n",
      "differential_evolution step 597: f(x)= 2.45465e+07\n",
      "differential_evolution step 598: f(x)= 2.45465e+07\n",
      "differential_evolution step 599: f(x)= 2.45465e+07\n",
      "differential_evolution step 600: f(x)= 2.45465e+07\n",
      "differential_evolution step 601: f(x)= 2.45465e+07\n",
      "differential_evolution step 602: f(x)= 2.45465e+07\n",
      "differential_evolution step 603: f(x)= 2.45465e+07\n",
      "differential_evolution step 604: f(x)= 2.45465e+07\n",
      "differential_evolution step 605: f(x)= 1.89481e+07\n",
      "differential_evolution step 606: f(x)= 1.89481e+07\n",
      "differential_evolution step 607: f(x)= 1.89481e+07\n",
      "differential_evolution step 608: f(x)= 1.89481e+07\n",
      "differential_evolution step 609: f(x)= 1.89481e+07\n",
      "differential_evolution step 610: f(x)= 1.89481e+07\n",
      "differential_evolution step 611: f(x)= 1.89481e+07\n",
      "differential_evolution step 612: f(x)= 1.89481e+07\n",
      "differential_evolution step 613: f(x)= 1.89481e+07\n",
      "differential_evolution step 614: f(x)= 1.89481e+07\n",
      "differential_evolution step 615: f(x)= 1.89481e+07\n",
      "differential_evolution step 616: f(x)= 8.68124e+06\n",
      "differential_evolution step 617: f(x)= 8.68124e+06\n",
      "differential_evolution step 618: f(x)= 8.68124e+06\n",
      "differential_evolution step 619: f(x)= 8.68124e+06\n",
      "differential_evolution step 620: f(x)= 8.68124e+06\n",
      "differential_evolution step 621: f(x)= 8.68124e+06\n",
      "differential_evolution step 622: f(x)= 8.68124e+06\n",
      "differential_evolution step 623: f(x)= 8.68124e+06\n",
      "differential_evolution step 624: f(x)= 8.68124e+06\n",
      "differential_evolution step 625: f(x)= 8.68124e+06\n",
      "differential_evolution step 626: f(x)= 8.68124e+06\n",
      "differential_evolution step 627: f(x)= 8.68124e+06\n",
      "differential_evolution step 628: f(x)= 5.22449e+06\n",
      "differential_evolution step 629: f(x)= 5.22449e+06\n",
      "differential_evolution step 630: f(x)= 5.22449e+06\n",
      "differential_evolution step 631: f(x)= 5.22449e+06\n",
      "differential_evolution step 632: f(x)= 5.22449e+06\n",
      "differential_evolution step 633: f(x)= 3.75892e+06\n",
      "differential_evolution step 634: f(x)= 3.75892e+06\n",
      "differential_evolution step 635: f(x)= 3.75892e+06\n",
      "differential_evolution step 636: f(x)= 3.75892e+06\n",
      "differential_evolution step 637: f(x)= 3.75892e+06\n",
      "differential_evolution step 638: f(x)= 3.75892e+06\n",
      "differential_evolution step 639: f(x)= 3.75892e+06\n",
      "differential_evolution step 640: f(x)= 3.75892e+06\n",
      "differential_evolution step 641: f(x)= 3.75892e+06\n",
      "differential_evolution step 642: f(x)= 3.75892e+06\n",
      "differential_evolution step 643: f(x)= 3.75892e+06\n",
      "differential_evolution step 644: f(x)= 3.75892e+06\n",
      "differential_evolution step 645: f(x)= 3.75892e+06\n",
      "differential_evolution step 646: f(x)= 3.75892e+06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "differential_evolution step 647: f(x)= 3.75892e+06\n",
      "differential_evolution step 648: f(x)= 3.75892e+06\n",
      "differential_evolution step 649: f(x)= 3.75892e+06\n",
      "differential_evolution step 650: f(x)= 3.75892e+06\n",
      "differential_evolution step 651: f(x)= 2.80949e+06\n",
      "differential_evolution step 652: f(x)= 2.80949e+06\n",
      "differential_evolution step 653: f(x)= 2.80949e+06\n",
      "differential_evolution step 654: f(x)= 2.80949e+06\n",
      "differential_evolution step 655: f(x)= 2.80949e+06\n",
      "differential_evolution step 656: f(x)= 2.80949e+06\n",
      "differential_evolution step 657: f(x)= 2.80949e+06\n",
      "differential_evolution step 658: f(x)= 2.80949e+06\n",
      "differential_evolution step 659: f(x)= 2.80949e+06\n",
      "differential_evolution step 660: f(x)= 2.80949e+06\n",
      "differential_evolution step 661: f(x)= 2.80949e+06\n",
      "differential_evolution step 662: f(x)= 2.80949e+06\n",
      "differential_evolution step 663: f(x)= 2.80949e+06\n",
      "differential_evolution step 664: f(x)= 2.80949e+06\n",
      "differential_evolution step 665: f(x)= 2.80949e+06\n",
      "differential_evolution step 666: f(x)= 2.80949e+06\n",
      "differential_evolution step 667: f(x)= 2.80949e+06\n",
      "differential_evolution step 668: f(x)= 2.80949e+06\n",
      "differential_evolution step 669: f(x)= 2.80949e+06\n",
      "differential_evolution step 670: f(x)= 2.80949e+06\n",
      "differential_evolution step 671: f(x)= 2.80949e+06\n",
      "differential_evolution step 672: f(x)= 2.80949e+06\n",
      "differential_evolution step 673: f(x)= 2.80949e+06\n",
      "differential_evolution step 674: f(x)= 2.80949e+06\n",
      "differential_evolution step 675: f(x)= 2.80949e+06\n",
      "differential_evolution step 676: f(x)= 2.80949e+06\n",
      "differential_evolution step 677: f(x)= 2.80949e+06\n",
      "differential_evolution step 678: f(x)= 2.80949e+06\n",
      "differential_evolution step 679: f(x)= 2.80949e+06\n",
      "differential_evolution step 680: f(x)= 2.80949e+06\n",
      "differential_evolution step 681: f(x)= 563877\n",
      "differential_evolution step 682: f(x)= 563877\n",
      "differential_evolution step 683: f(x)= 563877\n",
      "differential_evolution step 684: f(x)= 563877\n",
      "differential_evolution step 685: f(x)= 563877\n",
      "differential_evolution step 686: f(x)= 563877\n",
      "differential_evolution step 687: f(x)= 563877\n",
      "differential_evolution step 688: f(x)= 563877\n",
      "differential_evolution step 689: f(x)= 563513\n",
      "differential_evolution step 690: f(x)= 563513\n",
      "differential_evolution step 691: f(x)= 563513\n",
      "differential_evolution step 692: f(x)= 563513\n",
      "differential_evolution step 693: f(x)= 563513\n",
      "differential_evolution step 694: f(x)= 563513\n",
      "differential_evolution step 695: f(x)= 563513\n",
      "differential_evolution step 696: f(x)= 563513\n",
      "differential_evolution step 697: f(x)= 563513\n",
      "differential_evolution step 698: f(x)= 563513\n",
      "differential_evolution step 699: f(x)= 563513\n",
      "differential_evolution step 700: f(x)= 563513\n",
      "differential_evolution step 701: f(x)= 563513\n",
      "differential_evolution step 702: f(x)= 563513\n",
      "differential_evolution step 703: f(x)= 563513\n",
      "differential_evolution step 704: f(x)= 563513\n",
      "differential_evolution step 705: f(x)= 563513\n",
      "differential_evolution step 706: f(x)= 563513\n",
      "differential_evolution step 707: f(x)= 563513\n",
      "differential_evolution step 708: f(x)= 563513\n",
      "differential_evolution step 709: f(x)= 563513\n",
      "differential_evolution step 710: f(x)= 563513\n",
      "differential_evolution step 711: f(x)= 563513\n",
      "differential_evolution step 712: f(x)= 563513\n",
      "differential_evolution step 713: f(x)= 563513\n",
      "differential_evolution step 714: f(x)= 563513\n",
      "differential_evolution step 715: f(x)= 563513\n",
      "differential_evolution step 716: f(x)= 563513\n",
      "differential_evolution step 717: f(x)= 563513\n",
      "differential_evolution step 718: f(x)= 563513\n",
      "differential_evolution step 719: f(x)= 563513\n",
      "differential_evolution step 720: f(x)= 563513\n",
      "differential_evolution step 721: f(x)= 563513\n",
      "differential_evolution step 722: f(x)= 563513\n",
      "differential_evolution step 723: f(x)= 563513\n",
      "differential_evolution step 724: f(x)= 563513\n",
      "differential_evolution step 725: f(x)= 563513\n",
      "differential_evolution step 726: f(x)= 563513\n",
      "differential_evolution step 727: f(x)= 563513\n",
      "differential_evolution step 728: f(x)= 563513\n",
      "differential_evolution step 729: f(x)= 563513\n",
      "differential_evolution step 730: f(x)= 563513\n",
      "differential_evolution step 731: f(x)= 563513\n",
      "differential_evolution step 732: f(x)= 563513\n",
      "differential_evolution step 733: f(x)= 497487\n",
      "differential_evolution step 734: f(x)= 497487\n",
      "differential_evolution step 735: f(x)= 497487\n",
      "differential_evolution step 736: f(x)= 497487\n",
      "differential_evolution step 737: f(x)= 497487\n",
      "differential_evolution step 738: f(x)= 476973\n",
      "differential_evolution step 739: f(x)= 476973\n",
      "differential_evolution step 740: f(x)= 476973\n",
      "differential_evolution step 741: f(x)= 379095\n",
      "differential_evolution step 742: f(x)= 337319\n",
      "differential_evolution step 743: f(x)= 337319\n",
      "differential_evolution step 744: f(x)= 337319\n",
      "differential_evolution step 745: f(x)= 337319\n",
      "differential_evolution step 746: f(x)= 219214\n",
      "differential_evolution step 747: f(x)= 219214\n",
      "differential_evolution step 748: f(x)= 219214\n",
      "differential_evolution step 749: f(x)= 219214\n",
      "differential_evolution step 750: f(x)= 219214\n",
      "differential_evolution step 751: f(x)= 219214\n",
      "differential_evolution step 752: f(x)= 219214\n",
      "differential_evolution step 753: f(x)= 219214\n",
      "differential_evolution step 754: f(x)= 219214\n",
      "differential_evolution step 755: f(x)= 201779\n",
      "differential_evolution step 756: f(x)= 201779\n",
      "differential_evolution step 757: f(x)= 180703\n",
      "differential_evolution step 758: f(x)= 180703\n",
      "differential_evolution step 759: f(x)= 180703\n",
      "differential_evolution step 760: f(x)= 180703\n",
      "differential_evolution step 761: f(x)= 180703\n",
      "differential_evolution step 762: f(x)= 180703\n",
      "differential_evolution step 763: f(x)= 138135\n",
      "differential_evolution step 764: f(x)= 138135\n",
      "differential_evolution step 765: f(x)= 72271.4\n",
      "differential_evolution step 766: f(x)= 72271.4\n",
      "differential_evolution step 767: f(x)= 72271.4\n",
      "differential_evolution step 768: f(x)= 72271.4\n",
      "differential_evolution step 769: f(x)= 72271.4\n",
      "differential_evolution step 770: f(x)= 72271.4\n",
      "differential_evolution step 771: f(x)= 72271.4\n",
      "differential_evolution step 772: f(x)= 72271.4\n",
      "differential_evolution step 773: f(x)= 72271.4\n",
      "differential_evolution step 774: f(x)= 72271.4\n",
      "differential_evolution step 775: f(x)= 72271.4\n",
      "differential_evolution step 776: f(x)= 72271.4\n",
      "differential_evolution step 777: f(x)= 72271.4\n",
      "differential_evolution step 778: f(x)= 72271.4\n",
      "differential_evolution step 779: f(x)= 72271.4\n",
      "differential_evolution step 780: f(x)= 72271.4\n",
      "differential_evolution step 781: f(x)= 72271.4\n",
      "differential_evolution step 782: f(x)= 72271.4\n",
      "differential_evolution step 783: f(x)= 72271.4\n",
      "differential_evolution step 784: f(x)= 72271.4\n",
      "differential_evolution step 785: f(x)= 72271.4\n",
      "differential_evolution step 786: f(x)= 72271.4\n",
      "differential_evolution step 787: f(x)= 72271.4\n",
      "differential_evolution step 788: f(x)= 72271.4\n",
      "differential_evolution step 789: f(x)= 72271.4\n",
      "differential_evolution step 790: f(x)= 72271.4\n",
      "differential_evolution step 791: f(x)= 72271.4\n",
      "differential_evolution step 792: f(x)= 72271.4\n",
      "differential_evolution step 793: f(x)= 72271.4\n",
      "differential_evolution step 794: f(x)= 72271.4\n",
      "differential_evolution step 795: f(x)= 72271.4\n",
      "differential_evolution step 796: f(x)= 43744.5\n",
      "differential_evolution step 797: f(x)= 43744.5\n",
      "differential_evolution step 798: f(x)= 43744.5\n",
      "differential_evolution step 799: f(x)= 43744.5\n",
      "differential_evolution step 800: f(x)= 43744.5\n",
      "differential_evolution step 801: f(x)= 43744.5\n",
      "differential_evolution step 802: f(x)= 43744.5\n",
      "differential_evolution step 803: f(x)= 43744.5\n",
      "differential_evolution step 804: f(x)= 43744.5\n",
      "differential_evolution step 805: f(x)= 43744.5\n",
      "differential_evolution step 806: f(x)= 43744.5\n",
      "differential_evolution step 807: f(x)= 43744.5\n",
      "differential_evolution step 808: f(x)= 43744.5\n",
      "differential_evolution step 809: f(x)= 43744.5\n",
      "differential_evolution step 810: f(x)= 43744.5\n",
      "differential_evolution step 811: f(x)= 43744.5\n",
      "differential_evolution step 812: f(x)= 43744.5\n",
      "differential_evolution step 813: f(x)= 43744.5\n",
      "differential_evolution step 814: f(x)= 43744.5\n",
      "differential_evolution step 815: f(x)= 43744.5\n",
      "differential_evolution step 816: f(x)= 43744.5\n",
      "differential_evolution step 817: f(x)= 43744.5\n",
      "differential_evolution step 818: f(x)= 43744.5\n",
      "differential_evolution step 819: f(x)= 43744.5\n",
      "differential_evolution step 820: f(x)= 28338.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "differential_evolution step 821: f(x)= 28338.3\n",
      "differential_evolution step 822: f(x)= 28338.3\n",
      "differential_evolution step 823: f(x)= 28338.3\n",
      "differential_evolution step 824: f(x)= 28338.3\n",
      "differential_evolution step 825: f(x)= 28338.3\n",
      "differential_evolution step 826: f(x)= 28338.3\n",
      "differential_evolution step 827: f(x)= 28338.3\n",
      "differential_evolution step 828: f(x)= 28338.3\n",
      "differential_evolution step 829: f(x)= 28338.3\n",
      "differential_evolution step 830: f(x)= 28338.3\n",
      "differential_evolution step 831: f(x)= 28338.3\n",
      "differential_evolution step 832: f(x)= 28338.3\n",
      "differential_evolution step 833: f(x)= 28338.3\n",
      "differential_evolution step 834: f(x)= 28338.3\n",
      "differential_evolution step 835: f(x)= 28338.3\n",
      "differential_evolution step 836: f(x)= 24332.4\n",
      "differential_evolution step 837: f(x)= 24332.4\n",
      "differential_evolution step 838: f(x)= 24332.4\n",
      "differential_evolution step 839: f(x)= 24319.9\n",
      "differential_evolution step 840: f(x)= 24319.9\n",
      "differential_evolution step 841: f(x)= 24319.9\n",
      "differential_evolution step 842: f(x)= 16919.7\n",
      "differential_evolution step 843: f(x)= 15466\n",
      "differential_evolution step 844: f(x)= 15466\n",
      "differential_evolution step 845: f(x)= 15466\n",
      "differential_evolution step 846: f(x)= 14948.3\n",
      "differential_evolution step 847: f(x)= 14948.3\n",
      "differential_evolution step 848: f(x)= 14948.3\n",
      "differential_evolution step 849: f(x)= 14948.3\n",
      "differential_evolution step 850: f(x)= 14948.3\n",
      "differential_evolution step 851: f(x)= 14948.3\n",
      "differential_evolution step 852: f(x)= 14948.3\n",
      "differential_evolution step 853: f(x)= 14398.1\n",
      "differential_evolution step 854: f(x)= 14398.1\n",
      "differential_evolution step 855: f(x)= 7906.58\n",
      "differential_evolution step 856: f(x)= 7906.58\n",
      "differential_evolution step 857: f(x)= 7906.58\n",
      "differential_evolution step 858: f(x)= 7906.58\n",
      "differential_evolution step 859: f(x)= 7906.58\n",
      "differential_evolution step 860: f(x)= 7906.58\n",
      "differential_evolution step 861: f(x)= 7906.58\n",
      "differential_evolution step 862: f(x)= 7503.6\n",
      "differential_evolution step 863: f(x)= 4561.11\n",
      "differential_evolution step 864: f(x)= 4561.11\n",
      "differential_evolution step 865: f(x)= 4561.11\n",
      "differential_evolution step 866: f(x)= 4561.11\n",
      "differential_evolution step 867: f(x)= 4561.11\n",
      "differential_evolution step 868: f(x)= 4561.11\n",
      "differential_evolution step 869: f(x)= 4561.11\n",
      "differential_evolution step 870: f(x)= 4561.11\n",
      "differential_evolution step 871: f(x)= 4561.11\n",
      "differential_evolution step 872: f(x)= 4561.11\n",
      "differential_evolution step 873: f(x)= 2538.73\n",
      "differential_evolution step 874: f(x)= 2538.73\n",
      "differential_evolution step 875: f(x)= 1767.9\n",
      "differential_evolution step 876: f(x)= 1767.9\n",
      "differential_evolution step 877: f(x)= 1767.9\n",
      "differential_evolution step 878: f(x)= 1767.9\n",
      "differential_evolution step 879: f(x)= 1767.9\n",
      "differential_evolution step 880: f(x)= 1767.9\n",
      "differential_evolution step 881: f(x)= 1767.9\n",
      "differential_evolution step 882: f(x)= 1767.9\n",
      "differential_evolution step 883: f(x)= 1767.9\n",
      "differential_evolution step 884: f(x)= 1767.9\n",
      "differential_evolution step 885: f(x)= 1767.9\n",
      "differential_evolution step 886: f(x)= 1767.9\n",
      "differential_evolution step 887: f(x)= 1767.9\n",
      "differential_evolution step 888: f(x)= 1767.9\n",
      "differential_evolution step 889: f(x)= 1767.9\n",
      "differential_evolution step 890: f(x)= 1767.9\n",
      "differential_evolution step 891: f(x)= 1767.9\n",
      "differential_evolution step 892: f(x)= 1767.9\n",
      "differential_evolution step 893: f(x)= 1767.9\n",
      "differential_evolution step 894: f(x)= 1767.9\n",
      "differential_evolution step 895: f(x)= 1767.9\n",
      "differential_evolution step 896: f(x)= 1767.9\n",
      "differential_evolution step 897: f(x)= 1767.9\n",
      "differential_evolution step 898: f(x)= 1767.9\n",
      "differential_evolution step 899: f(x)= 1767.9\n",
      "differential_evolution step 900: f(x)= 1627.6\n",
      "differential_evolution step 901: f(x)= 1627.6\n",
      "differential_evolution step 902: f(x)= 1627.6\n",
      "differential_evolution step 903: f(x)= 1627.6\n",
      "differential_evolution step 904: f(x)= 1627.6\n",
      "differential_evolution step 905: f(x)= 1627.6\n",
      "differential_evolution step 906: f(x)= 1627.6\n",
      "differential_evolution step 907: f(x)= 1627.6\n",
      "differential_evolution step 908: f(x)= 1627.6\n",
      "differential_evolution step 909: f(x)= 1627.6\n",
      "differential_evolution step 910: f(x)= 1627.6\n",
      "differential_evolution step 911: f(x)= 855.739\n",
      "differential_evolution step 912: f(x)= 855.739\n",
      "differential_evolution step 913: f(x)= 855.739\n",
      "differential_evolution step 914: f(x)= 855.739\n",
      "differential_evolution step 915: f(x)= 855.739\n",
      "differential_evolution step 916: f(x)= 855.739\n",
      "differential_evolution step 917: f(x)= 855.739\n",
      "differential_evolution step 918: f(x)= 855.739\n",
      "differential_evolution step 919: f(x)= 855.739\n",
      "differential_evolution step 920: f(x)= 855.739\n",
      "differential_evolution step 921: f(x)= 855.739\n",
      "differential_evolution step 922: f(x)= 855.739\n",
      "differential_evolution step 923: f(x)= 855.739\n",
      "differential_evolution step 924: f(x)= 855.739\n",
      "differential_evolution step 925: f(x)= 855.739\n",
      "differential_evolution step 926: f(x)= 855.739\n",
      "differential_evolution step 927: f(x)= 855.739\n",
      "differential_evolution step 928: f(x)= 855.739\n",
      "differential_evolution step 929: f(x)= 855.739\n",
      "differential_evolution step 930: f(x)= 855.739\n",
      "differential_evolution step 931: f(x)= 855.739\n",
      "differential_evolution step 932: f(x)= 855.739\n",
      "differential_evolution step 933: f(x)= 855.739\n",
      "differential_evolution step 934: f(x)= 855.739\n",
      "differential_evolution step 935: f(x)= 855.739\n",
      "differential_evolution step 936: f(x)= 855.739\n",
      "differential_evolution step 937: f(x)= 855.739\n",
      "differential_evolution step 938: f(x)= 855.739\n",
      "differential_evolution step 939: f(x)= 855.739\n",
      "differential_evolution step 940: f(x)= 855.739\n",
      "differential_evolution step 941: f(x)= 855.739\n",
      "differential_evolution step 942: f(x)= 855.739\n",
      "differential_evolution step 943: f(x)= 855.739\n",
      "differential_evolution step 944: f(x)= 855.739\n",
      "differential_evolution step 945: f(x)= 855.739\n",
      "differential_evolution step 946: f(x)= 855.739\n",
      "differential_evolution step 947: f(x)= 855.739\n",
      "differential_evolution step 948: f(x)= 846.387\n",
      "differential_evolution step 949: f(x)= 846.387\n",
      "differential_evolution step 950: f(x)= 846.387\n",
      "differential_evolution step 951: f(x)= 846.387\n",
      "differential_evolution step 952: f(x)= 846.387\n",
      "differential_evolution step 953: f(x)= 846.387\n",
      "differential_evolution step 954: f(x)= 846.387\n",
      "differential_evolution step 955: f(x)= 712.661\n",
      "differential_evolution step 956: f(x)= 712.661\n",
      "differential_evolution step 957: f(x)= 529.715\n",
      "differential_evolution step 958: f(x)= 529.715\n",
      "differential_evolution step 959: f(x)= 529.715\n",
      "differential_evolution step 960: f(x)= 529.715\n",
      "differential_evolution step 961: f(x)= 529.715\n",
      "differential_evolution step 962: f(x)= 470.1\n",
      "differential_evolution step 963: f(x)= 470.1\n",
      "differential_evolution step 964: f(x)= 470.1\n",
      "differential_evolution step 965: f(x)= 470.1\n",
      "differential_evolution step 966: f(x)= 314.29\n",
      "differential_evolution step 967: f(x)= 314.29\n",
      "differential_evolution step 968: f(x)= 314.29\n",
      "differential_evolution step 969: f(x)= 183.562\n",
      "differential_evolution step 970: f(x)= 183.562\n",
      "differential_evolution step 971: f(x)= 183.562\n",
      "differential_evolution step 972: f(x)= 183.562\n",
      "differential_evolution step 973: f(x)= 183.562\n",
      "differential_evolution step 974: f(x)= 183.562\n",
      "differential_evolution step 975: f(x)= 183.562\n",
      "differential_evolution step 976: f(x)= 183.562\n",
      "differential_evolution step 977: f(x)= 183.562\n",
      "differential_evolution step 978: f(x)= 183.562\n",
      "differential_evolution step 979: f(x)= 183.562\n",
      "differential_evolution step 980: f(x)= 183.562\n",
      "differential_evolution step 981: f(x)= 183.562\n",
      "differential_evolution step 982: f(x)= 183.562\n",
      "differential_evolution step 983: f(x)= 183.562\n",
      "differential_evolution step 984: f(x)= 168.035\n",
      "differential_evolution step 985: f(x)= 168.035\n",
      "differential_evolution step 986: f(x)= 168.035\n",
      "differential_evolution step 987: f(x)= 159.135\n",
      "differential_evolution step 988: f(x)= 159.135\n",
      "differential_evolution step 989: f(x)= 159.135\n",
      "differential_evolution step 990: f(x)= 159.135\n",
      "differential_evolution step 991: f(x)= 93.9673\n",
      "differential_evolution step 992: f(x)= 93.9673\n",
      "differential_evolution step 993: f(x)= 93.9673\n",
      "differential_evolution step 994: f(x)= 93.9673\n",
      "differential_evolution step 995: f(x)= 93.9673\n",
      "differential_evolution step 996: f(x)= 93.9673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "differential_evolution step 997: f(x)= 93.9673\n",
      "differential_evolution step 998: f(x)= 93.9673\n",
      "differential_evolution step 999: f(x)= 93.9673\n",
      "differential_evolution step 1000: f(x)= 93.9673\n",
      "differential_evolution step 1001: f(x)= 93.9673\n",
      "differential_evolution step 1002: f(x)= 93.9673\n",
      "differential_evolution step 1003: f(x)= 93.9673\n",
      "differential_evolution step 1004: f(x)= 93.9673\n",
      "differential_evolution step 1005: f(x)= 93.9673\n",
      "differential_evolution step 1006: f(x)= 93.9673\n",
      "differential_evolution step 1007: f(x)= 93.9673\n",
      "differential_evolution step 1008: f(x)= 93.9673\n",
      "differential_evolution step 1009: f(x)= 93.9673\n",
      "differential_evolution step 1010: f(x)= 93.9673\n",
      "differential_evolution step 1011: f(x)= 93.9673\n",
      "differential_evolution step 1012: f(x)= 79.7868\n",
      "differential_evolution step 1013: f(x)= 72.0785\n",
      "differential_evolution step 1014: f(x)= 72.0785\n",
      "differential_evolution step 1015: f(x)= 72.0785\n",
      "differential_evolution step 1016: f(x)= 72.0785\n",
      "differential_evolution step 1017: f(x)= 72.0785\n",
      "differential_evolution step 1018: f(x)= 72.0785\n",
      "differential_evolution step 1019: f(x)= 72.0785\n",
      "differential_evolution step 1020: f(x)= 72.0785\n",
      "differential_evolution step 1021: f(x)= 72.0785\n",
      "differential_evolution step 1022: f(x)= 72.0785\n",
      "differential_evolution step 1023: f(x)= 72.0785\n",
      "differential_evolution step 1024: f(x)= 72.0785\n",
      "differential_evolution step 1025: f(x)= 72.0785\n",
      "differential_evolution step 1026: f(x)= 68.9809\n",
      "differential_evolution step 1027: f(x)= 68.5482\n",
      "differential_evolution step 1028: f(x)= 68.5482\n",
      "differential_evolution step 1029: f(x)= 58.9168\n",
      "differential_evolution step 1030: f(x)= 58.9168\n",
      "differential_evolution step 1031: f(x)= 58.9168\n",
      "differential_evolution step 1032: f(x)= 58.9168\n",
      "differential_evolution step 1033: f(x)= 58.9168\n",
      "differential_evolution step 1034: f(x)= 58.9168\n",
      "differential_evolution step 1035: f(x)= 58.9168\n",
      "differential_evolution step 1036: f(x)= 58.9168\n",
      "differential_evolution step 1037: f(x)= 55.2283\n",
      "differential_evolution step 1038: f(x)= 55.2283\n",
      "differential_evolution step 1039: f(x)= 55.2283\n",
      "differential_evolution step 1040: f(x)= 55.2283\n",
      "differential_evolution step 1041: f(x)= 55.2283\n",
      "differential_evolution step 1042: f(x)= 55.2283\n",
      "differential_evolution step 1043: f(x)= 55.2283\n",
      "differential_evolution step 1044: f(x)= 55.2283\n",
      "differential_evolution step 1045: f(x)= 51.2555\n",
      "differential_evolution step 1046: f(x)= 51.2555\n",
      "differential_evolution step 1047: f(x)= 51.2555\n",
      "differential_evolution step 1048: f(x)= 51.2555\n",
      "differential_evolution step 1049: f(x)= 51.2555\n",
      "differential_evolution step 1050: f(x)= 51.2555\n",
      "differential_evolution step 1051: f(x)= 43.8536\n",
      "differential_evolution step 1052: f(x)= 43.8536\n",
      "differential_evolution step 1053: f(x)= 43.8536\n",
      "differential_evolution step 1054: f(x)= 43.8536\n",
      "differential_evolution step 1055: f(x)= 43.8536\n",
      "differential_evolution step 1056: f(x)= 43.8536\n",
      "differential_evolution step 1057: f(x)= 43.8536\n",
      "differential_evolution step 1058: f(x)= 43.8536\n",
      "differential_evolution step 1059: f(x)= 40.0474\n",
      "differential_evolution step 1060: f(x)= 40.0474\n",
      "differential_evolution step 1061: f(x)= 40.0474\n",
      "differential_evolution step 1062: f(x)= 40.0474\n",
      "differential_evolution step 1063: f(x)= 40.0474\n",
      "differential_evolution step 1064: f(x)= 40.0474\n",
      "differential_evolution step 1065: f(x)= 40.0474\n",
      "differential_evolution step 1066: f(x)= 40.0474\n",
      "differential_evolution step 1067: f(x)= 40.0474\n",
      "differential_evolution step 1068: f(x)= 40.0474\n",
      "differential_evolution step 1069: f(x)= 40.0474\n",
      "differential_evolution step 1070: f(x)= 40.0474\n",
      "differential_evolution step 1071: f(x)= 40.0474\n",
      "differential_evolution step 1072: f(x)= 40.0474\n",
      "differential_evolution step 1073: f(x)= 40.0474\n",
      "differential_evolution step 1074: f(x)= 40.0474\n",
      "differential_evolution step 1075: f(x)= 40.0474\n",
      "differential_evolution step 1076: f(x)= 40.0474\n",
      "differential_evolution step 1077: f(x)= 40.0474\n",
      "differential_evolution step 1078: f(x)= 40.0474\n",
      "differential_evolution step 1079: f(x)= 40.0474\n",
      "differential_evolution step 1080: f(x)= 40.0474\n",
      "differential_evolution step 1081: f(x)= 40.0474\n",
      "differential_evolution step 1082: f(x)= 40.0474\n",
      "differential_evolution step 1083: f(x)= 40.0474\n",
      "differential_evolution step 1084: f(x)= 40.0474\n",
      "differential_evolution step 1085: f(x)= 40.0474\n",
      "differential_evolution step 1086: f(x)= 38.7234\n",
      "differential_evolution step 1087: f(x)= 38.7234\n",
      "differential_evolution step 1088: f(x)= 38.7234\n",
      "differential_evolution step 1089: f(x)= 38.7234\n",
      "differential_evolution step 1090: f(x)= 38.7234\n",
      "differential_evolution step 1091: f(x)= 38.3649\n",
      "differential_evolution step 1092: f(x)= 38.3649\n",
      "differential_evolution step 1093: f(x)= 38.3649\n",
      "differential_evolution step 1094: f(x)= 38.3649\n",
      "differential_evolution step 1095: f(x)= 38.3649\n",
      "differential_evolution step 1096: f(x)= 38.3649\n",
      "differential_evolution step 1097: f(x)= 38.3649\n",
      "differential_evolution step 1098: f(x)= 38.3649\n",
      "differential_evolution step 1099: f(x)= 38.3649\n",
      "differential_evolution step 1100: f(x)= 38.3649\n",
      "differential_evolution step 1101: f(x)= 38.3649\n",
      "differential_evolution step 1102: f(x)= 38.3649\n",
      "differential_evolution step 1103: f(x)= 37.5206\n",
      "differential_evolution step 1104: f(x)= 37.5206\n",
      "differential_evolution step 1105: f(x)= 37.5206\n",
      "differential_evolution step 1106: f(x)= 37.5206\n",
      "differential_evolution step 1107: f(x)= 37.5206\n",
      "differential_evolution step 1108: f(x)= 37.5206\n",
      "differential_evolution step 1109: f(x)= 36.9274\n",
      "differential_evolution step 1110: f(x)= 36.9274\n",
      "differential_evolution step 1111: f(x)= 36.9274\n",
      "differential_evolution step 1112: f(x)= 35.1622\n",
      "differential_evolution step 1113: f(x)= 35.1622\n",
      "differential_evolution step 1114: f(x)= 35.1622\n",
      "differential_evolution step 1115: f(x)= 35.1622\n",
      "differential_evolution step 1116: f(x)= 35.1622\n",
      "differential_evolution step 1117: f(x)= 35.1622\n",
      "differential_evolution step 1118: f(x)= 35.1622\n",
      "differential_evolution step 1119: f(x)= 35.1622\n",
      "differential_evolution step 1120: f(x)= 35.1622\n",
      "differential_evolution step 1121: f(x)= 35.1622\n",
      "differential_evolution step 1122: f(x)= 35.1622\n",
      "differential_evolution step 1123: f(x)= 35.1622\n",
      "differential_evolution step 1124: f(x)= 35.1622\n",
      "differential_evolution step 1125: f(x)= 35.1622\n",
      "differential_evolution step 1126: f(x)= 35.0164\n",
      "differential_evolution step 1127: f(x)= 35.0164\n",
      "differential_evolution step 1128: f(x)= 33.7618\n",
      "differential_evolution step 1129: f(x)= 33.7618\n",
      "differential_evolution step 1130: f(x)= 33.7618\n",
      "differential_evolution step 1131: f(x)= 33.7618\n",
      "differential_evolution step 1132: f(x)= 33.7618\n",
      "differential_evolution step 1133: f(x)= 33.7618\n",
      "differential_evolution step 1134: f(x)= 33.7618\n",
      "differential_evolution step 1135: f(x)= 33.7618\n",
      "differential_evolution step 1136: f(x)= 33.7618\n",
      "differential_evolution step 1137: f(x)= 33.7618\n",
      "differential_evolution step 1138: f(x)= 33.7618\n",
      "differential_evolution step 1139: f(x)= 33.7618\n",
      "differential_evolution step 1140: f(x)= 33.7618\n",
      "differential_evolution step 1141: f(x)= 33.7618\n",
      "differential_evolution step 1142: f(x)= 33.7618\n",
      "differential_evolution step 1143: f(x)= 33.7618\n",
      "differential_evolution step 1144: f(x)= 33.7618\n",
      "differential_evolution step 1145: f(x)= 33.7618\n",
      "differential_evolution step 1146: f(x)= 33.7618\n",
      "differential_evolution step 1147: f(x)= 33.7618\n",
      "differential_evolution step 1148: f(x)= 33.7618\n",
      "differential_evolution step 1149: f(x)= 33.7618\n",
      "differential_evolution step 1150: f(x)= 33.7618\n",
      "differential_evolution step 1151: f(x)= 33.7618\n",
      "differential_evolution step 1152: f(x)= 33.7618\n",
      "differential_evolution step 1153: f(x)= 33.7618\n",
      "differential_evolution step 1154: f(x)= 33.7618\n",
      "differential_evolution step 1155: f(x)= 33.7618\n",
      "differential_evolution step 1156: f(x)= 33.7618\n",
      "differential_evolution step 1157: f(x)= 33.7618\n",
      "differential_evolution step 1158: f(x)= 33.7618\n",
      "differential_evolution step 1159: f(x)= 33.7618\n",
      "differential_evolution step 1160: f(x)= 33.7618\n",
      "differential_evolution step 1161: f(x)= 33.7618\n",
      "differential_evolution step 1162: f(x)= 33.7618\n",
      "differential_evolution step 1163: f(x)= 33.7618\n",
      "differential_evolution step 1164: f(x)= 33.7618\n",
      "differential_evolution step 1165: f(x)= 33.7618\n",
      "differential_evolution step 1166: f(x)= 33.7618\n",
      "differential_evolution step 1167: f(x)= 33.7618\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "differential_evolution step 1168: f(x)= 33.7618\n",
      "differential_evolution step 1169: f(x)= 33.4111\n",
      "differential_evolution step 1170: f(x)= 33.4111\n",
      "differential_evolution step 1171: f(x)= 33.4111\n",
      "differential_evolution step 1172: f(x)= 33.4111\n",
      "differential_evolution step 1173: f(x)= 33.4111\n",
      "differential_evolution step 1174: f(x)= 33.4111\n",
      "differential_evolution step 1175: f(x)= 33.4111\n",
      "differential_evolution step 1176: f(x)= 33.4111\n",
      "differential_evolution step 1177: f(x)= 33.4111\n",
      "differential_evolution step 1178: f(x)= 33.4111\n",
      "differential_evolution step 1179: f(x)= 33.4111\n",
      "differential_evolution step 1180: f(x)= 33.4111\n",
      "differential_evolution step 1181: f(x)= 33.4111\n",
      "differential_evolution step 1182: f(x)= 33.4111\n",
      "differential_evolution step 1183: f(x)= 33.4111\n",
      "differential_evolution step 1184: f(x)= 33.4111\n",
      "differential_evolution step 1185: f(x)= 33.4111\n",
      "differential_evolution step 1186: f(x)= 33.4111\n",
      "differential_evolution step 1187: f(x)= 33.4111\n",
      "differential_evolution step 1188: f(x)= 33.4111\n",
      "differential_evolution step 1189: f(x)= 33.3127\n",
      "differential_evolution step 1190: f(x)= 33.3127\n",
      "differential_evolution step 1191: f(x)= 33.3127\n",
      "differential_evolution step 1192: f(x)= 33.3127\n",
      "differential_evolution step 1193: f(x)= 33.3127\n",
      "differential_evolution step 1194: f(x)= 33.3127\n",
      "differential_evolution step 1195: f(x)= 33.3127\n",
      "differential_evolution step 1196: f(x)= 33.3127\n",
      "differential_evolution step 1197: f(x)= 33.3127\n",
      "differential_evolution step 1198: f(x)= 33.3127\n",
      "differential_evolution step 1199: f(x)= 33.3127\n",
      "differential_evolution step 1200: f(x)= 33.3127\n",
      "differential_evolution step 1201: f(x)= 33.3127\n",
      "differential_evolution step 1202: f(x)= 33.3127\n",
      "differential_evolution step 1203: f(x)= 33.3127\n",
      "differential_evolution step 1204: f(x)= 33.3127\n",
      "differential_evolution step 1205: f(x)= 33.3127\n",
      "differential_evolution step 1206: f(x)= 33.3127\n",
      "differential_evolution step 1207: f(x)= 33.3127\n",
      "differential_evolution step 1208: f(x)= 33.3127\n",
      "differential_evolution step 1209: f(x)= 33.3127\n",
      "differential_evolution step 1210: f(x)= 33.3127\n",
      "differential_evolution step 1211: f(x)= 33.3127\n",
      "differential_evolution step 1212: f(x)= 33.3127\n",
      "differential_evolution step 1213: f(x)= 33.3127\n",
      "differential_evolution step 1214: f(x)= 33.3127\n",
      "differential_evolution step 1215: f(x)= 33.3127\n",
      "differential_evolution step 1216: f(x)= 33.3127\n",
      "differential_evolution step 1217: f(x)= 33.3127\n",
      "differential_evolution step 1218: f(x)= 33.3127\n",
      "differential_evolution step 1219: f(x)= 33.3127\n",
      "differential_evolution step 1220: f(x)= 33.3127\n",
      "differential_evolution step 1221: f(x)= 33.2922\n",
      "differential_evolution step 1222: f(x)= 33.2922\n",
      "differential_evolution step 1223: f(x)= 33.1805\n",
      "differential_evolution step 1224: f(x)= 33.1805\n",
      "differential_evolution step 1225: f(x)= 33.1805\n",
      "differential_evolution step 1226: f(x)= 33.1805\n",
      "differential_evolution step 1227: f(x)= 33.1805\n",
      "differential_evolution step 1228: f(x)= 33.1805\n",
      "differential_evolution step 1229: f(x)= 33.1805\n",
      "differential_evolution step 1230: f(x)= 33.1805\n",
      "differential_evolution step 1231: f(x)= 33.1805\n",
      "differential_evolution step 1232: f(x)= 33.1805\n",
      "differential_evolution step 1233: f(x)= 33.1805\n",
      "differential_evolution step 1234: f(x)= 33.0664\n",
      "differential_evolution step 1235: f(x)= 33.0664\n",
      "differential_evolution step 1236: f(x)= 33.0664\n",
      "differential_evolution step 1237: f(x)= 33.0664\n",
      "differential_evolution step 1238: f(x)= 33.0664\n",
      "differential_evolution step 1239: f(x)= 33.0664\n",
      "differential_evolution step 1240: f(x)= 33.0664\n",
      "differential_evolution step 1241: f(x)= 33.0664\n",
      "differential_evolution step 1242: f(x)= 33.0664\n",
      "differential_evolution step 1243: f(x)= 33.0664\n",
      "differential_evolution step 1244: f(x)= 33.0664\n",
      "differential_evolution step 1245: f(x)= 33.0664\n",
      "differential_evolution step 1246: f(x)= 33.0664\n",
      "differential_evolution step 1247: f(x)= 33.0664\n",
      "differential_evolution step 1248: f(x)= 33.0664\n",
      "differential_evolution step 1249: f(x)= 33.0664\n",
      "differential_evolution step 1250: f(x)= 33.0664\n",
      "differential_evolution step 1251: f(x)= 33.0664\n",
      "differential_evolution step 1252: f(x)= 33.0664\n",
      "differential_evolution step 1253: f(x)= 33.0664\n",
      "differential_evolution step 1254: f(x)= 33.0664\n",
      "differential_evolution step 1255: f(x)= 32.999\n",
      "differential_evolution step 1256: f(x)= 32.999\n",
      "differential_evolution step 1257: f(x)= 32.999\n",
      "differential_evolution step 1258: f(x)= 32.999\n",
      "differential_evolution step 1259: f(x)= 32.999\n",
      "differential_evolution step 1260: f(x)= 32.999\n",
      "differential_evolution step 1261: f(x)= 32.999\n",
      "differential_evolution step 1262: f(x)= 32.999\n",
      "differential_evolution step 1263: f(x)= 32.9245\n",
      "differential_evolution step 1264: f(x)= 32.9245\n",
      "differential_evolution step 1265: f(x)= 32.9245\n",
      "differential_evolution step 1266: f(x)= 32.9245\n",
      "differential_evolution step 1267: f(x)= 32.9245\n",
      "differential_evolution step 1268: f(x)= 32.9245\n",
      "differential_evolution step 1269: f(x)= 32.9245\n",
      "differential_evolution step 1270: f(x)= 32.9245\n",
      "differential_evolution step 1271: f(x)= 32.9245\n",
      "differential_evolution step 1272: f(x)= 32.9245\n",
      "differential_evolution step 1273: f(x)= 32.9245\n",
      "differential_evolution step 1274: f(x)= 32.9245\n",
      "differential_evolution step 1275: f(x)= 32.9245\n",
      "differential_evolution step 1276: f(x)= 32.9245\n",
      "differential_evolution step 1277: f(x)= 32.9245\n",
      "differential_evolution step 1278: f(x)= 32.9245\n",
      "differential_evolution step 1279: f(x)= 32.9245\n",
      "differential_evolution step 1280: f(x)= 32.9245\n",
      "differential_evolution step 1281: f(x)= 32.9245\n",
      "differential_evolution step 1282: f(x)= 32.9245\n",
      "differential_evolution step 1283: f(x)= 32.9245\n",
      "differential_evolution step 1284: f(x)= 32.9245\n",
      "differential_evolution step 1285: f(x)= 32.9245\n",
      "differential_evolution step 1286: f(x)= 32.9245\n",
      "differential_evolution step 1287: f(x)= 32.9245\n",
      "differential_evolution step 1288: f(x)= 32.9245\n",
      "differential_evolution step 1289: f(x)= 32.9245\n",
      "differential_evolution step 1290: f(x)= 32.9245\n",
      "differential_evolution step 1291: f(x)= 32.9245\n",
      "differential_evolution step 1292: f(x)= 32.9245\n",
      "differential_evolution step 1293: f(x)= 32.887\n",
      "differential_evolution step 1294: f(x)= 32.887\n",
      "differential_evolution step 1295: f(x)= 32.887\n",
      "differential_evolution step 1296: f(x)= 32.887\n",
      "differential_evolution step 1297: f(x)= 32.887\n",
      "differential_evolution step 1298: f(x)= 32.887\n",
      "differential_evolution step 1299: f(x)= 32.887\n",
      "differential_evolution step 1300: f(x)= 32.887\n",
      "differential_evolution step 1301: f(x)= 32.887\n",
      "differential_evolution step 1302: f(x)= 32.887\n",
      "differential_evolution step 1303: f(x)= 32.887\n",
      "[  3.85499712e+08   3.76860793e+04   7.52108147e+08   8.68040493e+08\n",
      "   6.53948208e+03   3.77962959e+01   3.98883253e+08   5.65305004e+08\n",
      "   1.00000000e-12   5.79284540e+08   7.15983550e+08   4.17939402e+08\n",
      "   7.62227333e+08   2.82346989e+08   7.00094062e+08   3.29342720e+08\n",
      "   8.25118490e+08   1.50409428e+05   9.12419864e+08   4.23856301e+08\n",
      "   9.38589487e+08   5.91966829e+08   1.00000000e-12   6.57334680e+08\n",
      "   1.67771587e+01   5.90738529e+08   6.73929337e+08   7.87140113e+08\n",
      "   3.71643108e+08   3.35257725e+08   6.01625893e+08   5.43825501e+00\n",
      "   5.10805995e+08   2.82010503e+08   6.52100907e+08   1.79843426e-05\n",
      "   4.52479688e+08   2.61449380e-02   2.93621531e+01]\n"
     ]
    }
   ],
   "source": [
    "# If Experimental Limonene Results show that the Hand created \n",
    "# Kinetic Model Does not fit as well as the machine learning model\n",
    "\n",
    "def proteomicsData(t,k):\n",
    "    e = []\n",
    "    for i in range(int(len(k)/3)):\n",
    "        #Sorting the gains to ensure proteins only increase\n",
    "        gains = sorted(k[3*i:3*(i+1)],reverse=True)\n",
    "        e.append(leaky_hill_fcn(t,*gains)) \n",
    "    return e\n",
    "\n",
    "\n",
    "def kinetic_model(e1,e2,e3,e4,e5,e6,e7,e8,e9,\n",
    "                  s1,s2,s3,s4,s5,s6,s7,s8,s9,s10,\n",
    "                  k11,k12,k13,k21,k22,k23,k24,k31,k32,k33,k34,k35,\n",
    "                  k41,k42,k43,k44,k45,k51,k52,k61,k62,k63,k64,k65,\n",
    "                  k71,k72,k81,k82,k83,k84,k91,k92,Vin,ks3):\n",
    "    r1 = Vin - (k11*e1*s1)/(k12 + k13*s1) - k21*e2*s1*s2*ks3 / (k22*s2 + k23*s1 + k24*s1*s2)\n",
    "    r2 = (k11*e1*s1)/(k12 + k13*s1) - k21*e2*s1*s2*ks3 / (k22*s2 + k23*s1 + k24*s1*s2)\n",
    "    r3 = k21*e2*s1*s2*ks3 / (k22*s2 + k23*s1 + k24*s1*s2) - k31*e3*s3 / (k32*s1 + k33*s2 + k34*s3 + k35)\n",
    "    r4 = k31*e3*s3 / (k32*s1 + k33*s2 + k34*s3 + k35) - k41*e4*s4 / (k42*s9 + k43*s5 + k44*s4 + k45)\n",
    "    r5 = k41*e4*s4 / (k42*s9 + k43*s5 + k44*s4 + k45) - k51*e5*s5 / (k52 + s5)\n",
    "    r6 = k51*e5*s5 / (k52 + s5) - k61*e6*s6 / (k62*s5 + k63*s4 + k64*s6 + k65)\n",
    "    r7 = k61*e6*s6 / (k62*s5 + k63*s4 + k64*s6 + k65) - k71*e7*s7 / (k72 + s7) - k81*e8*s7*s8 / (k82 + k83*s7 + k84*s8 + s7*s8)\n",
    "    r8 = k71*e7*s7 / (k72 + s7) - k81*e8*s7*s8 / (k82 + k83*s7 + k84*s8 + s7*s8)\n",
    "    r9 = k81*e8*s7*s8 / (k82 + k83*s7 + k84*s8 + s7*s8) - k91*e9*s9 / (k92 + s9)\n",
    "    r10 = k91*e9*s9 / (k92 + s9)\n",
    "    return [r1,r2,r3,r4,r5,r6,r7,r8,r9,r10]\n",
    "    \n",
    "def solve_kinetic_ode(f,y0,times,k_fit):\n",
    "    sol = 1\n",
    "    return sol\n",
    "\n",
    "measured_substrates = ['Acetyl-CoA (uM)', 'HMG-CoA (uM)', 'Intracellular Mevalonate (uM)', 'Mev-P (uM)', 'IPP/DMAPP (uM)', 'Limonene g/L']\n",
    "measured_enzymes = ['AtoB', 'HMGS', 'HMGR', 'MK', 'PMK', 'PMD', 'Limonene Synthase']\n",
    "x_features = [('feature',val) for val in measured_enzymes+measured_substrates]\n",
    "y_targets = [('target',val) for val in measured_substrates]\n",
    "\n",
    "#print(x_features)\n",
    "#print(y_targets)\n",
    "\n",
    "if data_type == 'experimental' and  pathway == 'limonene':\n",
    "    [('feature', feature) for feature in specific_features[target]]\n",
    "    X = tsdf_max_strains[x_features].values.tolist()\n",
    "    y = tsdf_max_strains[y_targets].values.tolist()\n",
    "    \n",
    "    #Solve for Kinetic Coefficients from Training Set\n",
    "    def cost_fcn_gen(X,y):\n",
    "        def cost_fcn(free_params):\n",
    "            cost = 0\n",
    "            for x_val,y_vals in zip(X,y):\n",
    "                params = []\n",
    "                params.extend(x_val[0:6])       # AtoB to PMD Values\n",
    "                params.extend(free_params[0:2]) # Keep Constant GPPS and IDI levels as free parameters\n",
    "                params.extend(x_val[6:8])       # LS and Acetyl-CoA\n",
    "                params.append(free_params[2])   # AcetoAcetyl-CoA as a free Param\n",
    "                params.extend(x_val[8:11])      # HMG-CoA & Mev & MevP measured\n",
    "                params.append(free_params[3])   #MevPP \n",
    "                params.extend([x_val[11],x_val[11]]) #DMAPP & IDI Measured\n",
    "                params.extend([free_params[4],x_val[12]]) #GPP as a Free Parameter #Measured Limonene Synthase\n",
    "                params.extend(free_params[5:])  # Remaining Kinetic Free Parameters\n",
    "                \n",
    "                mp = kinetic_model(*params)\n",
    "                prediction = [mp[0],mp[2],mp[3],mp[4],mp[6]+mp[7],mp[9]]\n",
    "                cost += sum([(fx_val - y_val)**2  for fx_val,y_val in zip(prediction,y_vals)])\n",
    "            return cost\n",
    "        return cost_fcn\n",
    "\n",
    "    cost_fcn = cost_fcn_gen(X,y)\n",
    "    \n",
    "    #Call to check its working (num free params = 39)\n",
    "    print(cost_fcn([1,]*39))\n",
    "    \n",
    "    #Solve For Optimal Parameters\n",
    "    bounds = [(1*10**-12,10**9)]*39\n",
    "    sol = differential_evolution(cost_fcn,bounds,disp=True,maxiter=10000)\n",
    "    best_params = sol.x\n",
    "    print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import statistics\n",
    "\n",
    "def plot_species_curves(modelDict, title, df, targets, specific_features, ylim=None, cv=None,\n",
    "                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 3),training_sets=5):\n",
    "    \"\"\"\n",
    "    Generate a simple plot of the test and training learning curve. Returns Metrics for each predicted curve\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
    "        An object of that type which is cloned for each validation.\n",
    "\n",
    "    title : string\n",
    "        Title for the chart.\n",
    "\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Training vector, where n_samples is the number of samples and\n",
    "        n_features is the number of features.\n",
    "\n",
    "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
    "        Target relative to X for classification or regression;\n",
    "        None for unsupervised learning.\n",
    "\n",
    "    ylim : tuple, shape (ymin, ymax), optional\n",
    "        Defines minimum and maximum yvalues plotted.\n",
    "\n",
    "    cv : int, cross-validation generator or an iterable, optional\n",
    "        Determines the cross-validation splitting strategy.\n",
    "        Possible inputs for cv are:\n",
    "          - None, to use the default 3-fold cross-validation,\n",
    "          - integer, to specify the number of folds.\n",
    "          - An object to be used as a cross-validation generator.\n",
    "          - An iterable yielding train/test splits.\n",
    "\n",
    "        For integer/None inputs, if ``y`` is binary or multiclass,\n",
    "        :class:`StratifiedKFold` used. If the estimator is not a classifier\n",
    "        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n",
    "\n",
    "        Refer :ref:`User Guide <cross_validation>` for the various\n",
    "        cross-validators that can be used here.\n",
    "\n",
    "    n_jobs : integer, optional\n",
    "        Number of jobs to run in parallel (default 1).\n",
    "    \"\"\"\n",
    "    \n",
    "    #Set Random Seed For training\n",
    "    seed = 103\n",
    "    random.seed(seed)\n",
    "    \n",
    "    #Create figure / plots\n",
    "    fig = plt.figure(figsize=(12,16))\n",
    "    #fig.set_title(title)\n",
    "    #if ylim is not None:\n",
    "    #    plt.ylim(*ylim)\n",
    "    \n",
    "    #Create subplots for each target\n",
    "    ax = {}\n",
    "    for i,target in enumerate(targets):\n",
    "        ax[target] = plt.subplot(int(len(targets)/2), 2, i+1)\n",
    "    \n",
    "    #Get Randomized List of all Strains\n",
    "    strains = df.index.get_level_values(0).unique()\n",
    "    strains = list(strains.values)\n",
    "    #print(strains)\n",
    "    strains = random.sample(strains, len(strains))\n",
    "    \n",
    "    #Pick test strain\n",
    "    test_df = df.loc[(slice(strains[0],strains[0]),slice(None)),:]\n",
    "    strains = strains[1:]\n",
    "    \n",
    "    #Create Interpolation functions for each feature in the test strain\n",
    "    interpFun = {}\n",
    "    #display(test_df.reset_index())\n",
    "    for feature in df.columns:\n",
    "        X,y = remove_NaN(test_df.reset_index()['Time (h)'].tolist(),test_df[feature].tolist())\n",
    "        if isinstance(feature,tuple):\n",
    "            if feature[0] == 'feature':\n",
    "                feature = feature[1]\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        interpFun[feature] = interp1d(X,y)  \n",
    "\n",
    "    train_sizes = [int(len(strains)*size/training_sets)-1 for size in train_sizes]\n",
    "    for i,size in enumerate(train_sizes):\n",
    "        if size < 2:\n",
    "            train_sizes[i] = 2\n",
    "            \n",
    "    #Create Fits for each training set\n",
    "    fits = {}\n",
    "    for training_set in range(training_sets):        \n",
    "        fits[training_set] = {}\n",
    "\n",
    "        #Generate training strain data for this training set\n",
    "        training_strains = strains[0:(train_sizes[-1] + 1)]\n",
    "        #print(training_strains)\n",
    "        strains = strains[train_sizes[-1]:]\n",
    "        endSamples = train_sizes\n",
    "        #print('Strains:',strains)\n",
    "        #print('End Samples',endSamples)\n",
    "        sample_sets = [df.loc[(training_strains[0:endSample],slice(None)),:] for endSample in endSamples]\n",
    "\n",
    "        #For each set size in the training set fit the model and store it\n",
    "        for j,sample_set in enumerate(sample_sets):\n",
    "            \n",
    "            #print('Sample Set:',sample_set.index.get_level_values(0).unique().values)\n",
    "            \n",
    "            # Train Model\n",
    "            print('Training Models for Training Set',training_set,'In Sample set',j)\n",
    "            for target in targets:\n",
    "                feature_indecies = [('feature', feature) for feature in specific_features[target]]\n",
    "                X = sample_set[feature_indecies].values.tolist()\n",
    "\n",
    "                #print(feature_indecies)\n",
    "                #display(sample_set[feature_indecies])\n",
    "                target_index = ('target',target)\n",
    "                y = sample_set[target_index].values.tolist()\n",
    "                modelDict[target].fit(X,y)\n",
    "\n",
    "            print('Integrating ODEs!')\n",
    "            # Integrate Given Model Test Case\n",
    "            g = mlode(modelDict, test_df, targets, specific_features)\n",
    "            times = test_df.reset_index()['Time (h)'].tolist()\n",
    "\n",
    "            #Set Y0 initial condition\n",
    "            appended_targets = [('feature',target) for target in targets]\n",
    "            #display(test_df)\n",
    "            #display(test_df[appended_targets].iloc[0])\n",
    "            y0 = test_df[appended_targets].iloc[0].tolist()\n",
    "\n",
    "            #print('times:',times)\n",
    "            fit  = odeintz(g,y0,times)\n",
    "            fitT = list(map(list, zip(*fit)))\n",
    "            fits[training_set][train_sizes[j]] = fitT\n",
    "\n",
    "    \n",
    "    #Perform Statistics on Fits and generate plots\n",
    "    colors = ['b','g','k','y','m']\n",
    "    predictions = {}\n",
    "    lines =[]\n",
    "    labels = []\n",
    "    for k,target in enumerate(targets):\n",
    "        actual_data = [interpFun[target](t) for t in times]\n",
    "        predictions[target] = {'actual':actual_data}\n",
    "        predictions['Time'] = times\n",
    "        if k == 0:\n",
    "            lines.append(ax[target].plot(times,actual_data,'--', color='r')[0])\n",
    "            labels.append('Actual Dynamics')\n",
    "        else:\n",
    "            ax[target].plot(times,actual_data,'--', color='r')\n",
    "        ax[target].set_title(target)\n",
    "        \n",
    "        for j in range(len(sample_sets)):\n",
    "            upper = []\n",
    "            lower = []\n",
    "            aves = []\n",
    "            \n",
    "            predictions[target][train_sizes[j]] = []\n",
    "            for training_set in range(training_sets):\n",
    "                predictions[target][train_sizes[j]].append(fits[training_set][train_sizes[j]][k])\n",
    "            \n",
    "            for i,time in enumerate(times):\n",
    "                \n",
    "                values = []\n",
    "                for training_set in range(training_sets):\n",
    "                    #print(training_set,train_sizes[j],i)\n",
    "                    values.append(fits[training_set][train_sizes[j]][k][i])\n",
    "\n",
    "                #Compute Statistics of Values\n",
    "                #print(values)\n",
    "                ave = statistics.mean(values)\n",
    "                std = statistics.stdev(values)\n",
    "                aves += [ave,]\n",
    "                upper += [ave + std,]\n",
    "                lower += [ave - std,]\n",
    "                \n",
    "                #print(upper)\n",
    "                #print(times)\n",
    "                \n",
    "            #Compute upper and lower bounds for shading\n",
    "            ax[target].fill_between(times, lower,upper, alpha=0.1, color=colors[j])\n",
    "            if k == 0:\n",
    "                lines.append(ax[target].plot(times,aves,colors[j])[0])\n",
    "                labels.append(str(train_sizes[j]) + ' Strain Prediction')\n",
    "            else:\n",
    "                ax[target].plot(times,aves,colors[j])\n",
    "            print(colors[j],train_sizes[j])\n",
    "        plt.figlegend( lines, labels, loc = 'lower center', ncol=5, labelspacing=0. )       \n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y0 [0.286359015, 0.007628361, 0.892762772, 0.046703445, 0.173320815, 0.0]\n",
      "['Acetyl-CoA (uM)', 'HMG-CoA (uM)', 'Intracellular Mevalonate (uM)', 'Mev-P (uM)', 'IPP/DMAPP (uM)', 'Limonene g/L']\n",
      "72.0\r"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1QAAAI9CAYAAAA97Tl7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11339f5c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#3. Plot Comparison between Actual and Predicted Results\n",
    "if data_type == 'simulated':\n",
    "    real_vs_predicted = plot_species_curves(modelDict, 'title', tsdf, targets, specific_features,train_sizes=train_sizes,training_sets=training_sets)\n",
    "    plt.savefig('figures/MainFigure.pdf')\n",
    "    \n",
    "    \n",
    "elif data_type == 'experimental':\n",
    "    #Fit each test strain by solving the differential equation\n",
    "\n",
    "    #Select Random Time Series\n",
    "    #test_df = df\n",
    "    #display(test_df)\n",
    "    strains = test_df.index.get_level_values(0).unique()\n",
    "    #print(strains)\n",
    "\n",
    "    random_strain = random.sample(tuple(strains), 1)\n",
    "    #print(random_strain)\n",
    "\n",
    "    random_strain = list(random_strain)[0]\n",
    "    #random_strain = 180\n",
    "    random_df = test_df.loc[(slice(random_strain,random_strain),slice(None)),:]\n",
    "    #print('The Chosen Random Strain is:', random_strain)\n",
    "\n",
    "    #Set Y0 initial condition\n",
    "    y0 = random_df[targets].iloc[0].tolist()\n",
    "    print('y0',y0)\n",
    "    \n",
    "    #Create Differential Equation to Solve\n",
    "    #print('Creating Differential Equation!')\n",
    "    if featureReduction:\n",
    "        target_features = [header[1:]]*len(targets[0])\n",
    "        g = mlode(modelDict, random_df, targets, specific_features)\n",
    "    else:\n",
    "        g = mlode(modelDict, random_df, targets, specific_features,time_index='Time')\n",
    "\n",
    "    #Solve Differential Equation numerically using a runge-kutta 4,5 implementation\n",
    "    #print('Solving Differential Equation')\n",
    "    times = random_df.reset_index()['Time'].tolist()\n",
    "    fit = odeintz(g,y0,times)\n",
    "    fitT = list(map(list, zip(*fit)))\n",
    "    \n",
    "    \n",
    "    #Create Interpolation functions for each feature\n",
    "    interpFun = {}\n",
    "    for feature in random_df.columns:\n",
    "        X,y = remove_NaN(random_df.reset_index()['Time'].tolist(),random_df[feature].tolist())\n",
    "        interpFun[feature] = interp1d(X,y)\n",
    "\n",
    "        \n",
    "    if pathway == 'limonene':\n",
    "        #print(random_df.columns)\n",
    "        proteins = ['AtoB', 'HMGR', 'HMGS', 'MK', 'PMD', 'PMK','Limonene Synthase']\n",
    "        protein_fcns= []\n",
    "        for feature in random_df.columns:\n",
    "            X,y = remove_NaN(random_df.reset_index()['Time'].tolist(),random_df[feature].tolist())\n",
    "            protein_fcns.append(interp1d(X,y))\n",
    "        \n",
    "        #Generate Fit Kinetic Model ODE\n",
    "        def kinetic_ode(x,t,proteomic_fcns):\n",
    "            \n",
    "            #proteomics \n",
    "            proteomic_data = [proteomic_fcn(t) for proteomic_fcn in proteomic_fcns]\n",
    "\n",
    "            params = []\n",
    "            params.extend(proteomic_data[0:6]) # AtoB to PMD Values\n",
    "            params.extend(best_params[0:2])     # Keep Constant GPPS and IDI levels as free parameters\n",
    "            params.append(proteomic_data[6])   # \n",
    "            params.append(x[0])                #Acetyl-CoA\n",
    "            params.append(best_params[2])       # AcetoAcetyl-CoA as a free Param\n",
    "            params.extend(x[1:4])               # HMG-CoA & Mev & MevP measured\n",
    "            params.append(best_params[3])       # MevPP \n",
    "            params.extend([x[4],x[4]])          # DMAPP & IDI Measured\n",
    "            params.extend([best_params[4],x[5]]) # GPP as a Free Parameter #Measured Limonene Synthase\n",
    "            params.extend(best_params[5:])      # Remaining Kinetic Free Parameters\n",
    "            \n",
    "            \n",
    "            dxdt = kinetic_model(*params)\n",
    "            dxdt_combined = [dxdt[0],dxdt[2],dxdt[3],dxdt[4],dxdt[6]+dxdt[7],dxdt[9]]\n",
    "            return dxdt_combined\n",
    "        \n",
    "        for protein in proteins:\n",
    "            protein_fcns.append(interpFun[protein])\n",
    "        \n",
    "        #Solve Kinetic ODE at initial Conditions\n",
    "        kinetic_ode_p = lambda x,t : kinetic_ode(x,t,protein_fcns)\n",
    "        kinetic_fit = odeintz(kinetic_ode_p,y0,times)\n",
    "        kinetic_fit = list(map(list, zip(*kinetic_fit)))\n",
    "        \n",
    "    real_vs_predicted = {}\n",
    "    plt.figure(figsize=(12,8))\n",
    "    for i,target in enumerate(targets):\n",
    "        plt.subplot(2,3,i+1)\n",
    "        if data_type == 'experimental':\n",
    "            #Plot both High and Low Strain values\n",
    "            for strain in training_strains:\n",
    "                strainInterpFun = {}\n",
    "                strain_df = df.loc[(strain,slice(None))]\n",
    "                X,y = remove_NaN(strain_df.reset_index()['Time'].tolist(),strain_df[target].tolist())\n",
    "                strainInterpFun[target] = interp1d(X,y)\n",
    "                actual_data = [strainInterpFun[target](t) for t in times]\n",
    "                train_line, = plt.plot(times,actual_data,'r--')\n",
    "            \n",
    "            #Plot Kinetic Model Fit\n",
    "            if pathway =='limonene':\n",
    "                kinetic_pred = [max(kinetic_fit[i][j],0) for j,t in enumerate(times)]\n",
    "                kinetic_line, = plt.plot(times,kinetic_pred,color='k')\n",
    "\n",
    "        actual_data = [interpFun[target](t) for t in times]\n",
    "        pos_pred = [max(fitT[i][j],0) for j,t in enumerate(times)]\n",
    "        prediction_line, = plt.plot(times,pos_pred)\n",
    "        #print(actual_data)\n",
    "        test_line, = plt.plot(times,actual_data,'g--')\n",
    "        plt.ylabel(target)\n",
    "        plt.xlabel('Time [h]')\n",
    "        plt.xlim([0,72])\n",
    "        \n",
    "        if i == 5 and pathway == 'limonene':\n",
    "            plt.ylim([0,0.6])\n",
    "        \n",
    "        #Create variable for processing error residuals\n",
    "        if pathway == 'limonene':\n",
    "            real_vs_predicted[target] = [times,actual_data,pos_pred,kinetic_pred]\n",
    "        else:\n",
    "            real_vs_predicted[target] = [times,actual_data,pos_pred]\n",
    "    #Add Legend\n",
    "    if set_num==2:\n",
    "        product = 'Isopentenol'\n",
    "    elif set_num==3:\n",
    "        product = 'Limonene'\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.90)\n",
    "    plt.subplots_adjust(bottom=0.12)\n",
    "    plt.suptitle('Prediction of ' + product + ' Strain Dynamics', fontsize=18)\n",
    "    if pathway == 'limonene':\n",
    "        plt.figlegend( (train_line,test_line,prediction_line,kinetic_line), ('Training Set Data','Test Data','Machine Learning Model Prediction','Kinetic Model Prediction'), loc = 'lower center', ncol=5, labelspacing=0. )  \n",
    "    else:\n",
    "        plt.figlegend( (train_line,test_line,prediction_line), ('Training Set Data','Test Data','Machine Learning Model Prediction'), loc = 'lower center', ncol=5, labelspacing=0. ) \n",
    "    plt.savefig('data/' + product + '_prediction.eps', format='eps', dpi=600)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ML Fit: Acetyl-CoA (uM) 1.314951630566375 RMSE percent: 98.61732838805818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zakcostello/anaconda/lib/python3.6/site-packages/scipy/integrate/quadpack.py:364: IntegrationWarning: The maximum number of subdivisions (50) has been achieved.\n",
      "  If increasing the limit yields no improvement it is advised to analyze \n",
      "  the integrand in order to determine the difficulties.  If the position of a \n",
      "  local difficulty can be determined (singularity, discontinuity) one will \n",
      "  probably gain from splitting up the interval and calling the integrator \n",
      "  on the subranges.  Perhaps a special-purpose integrator should be used.\n",
      "  warnings.warn(msg, IntegrationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kinetic Fit: Acetyl-CoA (uM) 1.1638534445661914 RMSE percent: 90.80247967961212\n",
      "ML Fit: HMG-CoA (uM) 0.17762995146575597 RMSE percent: 65.49967689140145\n",
      "Kinetic Fit: HMG-CoA (uM) 0.19155872953101122 RMSE percent: 87.60900661689838\n",
      "ML Fit: Intracellular Mevalonate (uM) 285.6249757349928 RMSE percent: 230.28896859279251\n",
      "Kinetic Fit: Intracellular Mevalonate (uM) 35.42902131467681 RMSE percent: 94.70310289489558\n",
      "ML Fit: Mev-P (uM) 6.136026197119371 RMSE percent: 186.4893328478977\n",
      "Kinetic Fit: Mev-P (uM) 2.486155341372301 RMSE percent: 90.95595229978346\n",
      "ML Fit: IPP/DMAPP (uM) 28.007640268414228 RMSE percent: 315.2036614157078\n",
      "Kinetic Fit: IPP/DMAPP (uM) 12.486943186272855 RMSE percent: 208.46166938543038\n",
      "ML Fit: Limonene g/L 0.12029211367562681 RMSE percent: 76.62843994487179\n",
      "Kinetic Fit: Limonene g/L 0.17974218628087313 RMSE percent: 72.23336278749613\n",
      "\n",
      "Machine Learning Model Agrigate Error\n",
      "Average RMSE: 53.56358598270569\n",
      "Total Percent Error: 162.12123468012157\n",
      "\n",
      "Kinetic Model Agrigate Error\n",
      "Average RMSE: 8.656212367116675\n",
      "Total Percent Error: 107.46092894401936\n"
     ]
    }
   ],
   "source": [
    "#4. Error Residuals of Time Series Data + Key Moments Report\n",
    "\n",
    "#Not Quite Right... Need to Integrate over the \n",
    "if data_type == 'experimental':\n",
    "    #print(real_vs_predicted)\n",
    "\n",
    "    rmse_percent = []\n",
    "    rmse_average = []\n",
    "    rmse_percent_k = []\n",
    "    rmse_average_k = []\n",
    "    for metabolite in real_vs_predicted:\n",
    "        if pathway == 'limonene':\n",
    "            times,real,predicted,kinetic = real_vs_predicted[metabolite]\n",
    "        else:\n",
    "            times,real,predicted = real_vs_predicted[metabolite]\n",
    "        real_fcn = interp1d(times,real)\n",
    "        pred_fcn = interp1d(times,predicted)\n",
    "\n",
    "        integrand = lambda t: (real_fcn(t) - pred_fcn(t))**2\n",
    "        rmse = math.sqrt(quad(integrand,min(times),max(times))[0])\n",
    "        rmse_average.append(rmse)\n",
    "        percent_integrand = lambda t: abs(real_fcn(t) - pred_fcn(t))/(real_fcn(t)*max(times))\n",
    "        #print(metabolite,[percent_integrand(t) for t in times],[real_fcn(t) for t in times],[pred_fcn(t) for t in times])\n",
    "        \n",
    "        rmsep = math.sqrt(quad(percent_integrand,min(times),max(times))[0])\n",
    "        rmse_percent.append(rmsep)\n",
    "        print('ML Fit:',metabolite,rmse,'RMSE percent:',rmsep*100)\n",
    "        \n",
    "        if pathway == 'limonene':\n",
    "            kinetic_fcn = interp1d(times,kinetic)\n",
    "            integrand = lambda t: (real_fcn(t) - kinetic_fcn(t))**2\n",
    "            rmsek = math.sqrt(quad(integrand,min(times),max(times))[0])\n",
    "            percent_integrand = lambda t: abs(real_fcn(t) - kinetic_fcn(t))/(real_fcn(t)*max(times))\n",
    "            rmsepk = math.sqrt(quad(percent_integrand,min(times),max(times))[0])\n",
    "            rmse_percent_k.append(rmsepk)\n",
    "            rmse_average_k.append(rmsek)\n",
    "            print('Kinetic Fit:',metabolite,rmsek,'RMSE percent:',rmsepk*100)\n",
    "    \n",
    "    print('')\n",
    "    print('Machine Learning Model Agrigate Error')\n",
    "    print('Average RMSE:',sum(rmse_average)/len(rmse_average))\n",
    "    print('Total Percent Error:',sum(rmse_percent)/len(rmse_percent)*100)\n",
    "\n",
    "    if pathway == 'limonene':\n",
    "        print('')\n",
    "        print('Kinetic Model Agrigate Error')\n",
    "        print('Average RMSE:',sum(rmse_average_k)/len(rmse_average_k))\n",
    "        print('Total Percent Error:',sum(rmse_percent_k)/len(rmse_percent_k)*100)\n",
    "        \n",
    "elif data_type == 'simulated':\n",
    "    times = real_vs_predicted['Time']\n",
    "    rmse_dict = {}\n",
    "    for metabolite in real_vs_predicted:\n",
    "        if metabolite not in ['Time',]:\n",
    "            actual = real_vs_predicted[metabolite]['actual']\n",
    "            real_fcn = interp1d(times,actual)\n",
    "            rmse_dict[metabolite] = {}\n",
    "            for test_size in real_vs_predicted[metabolite]:\n",
    "                print(metabolite,'Test Size:',test_size)\n",
    "                total_rmse = []\n",
    "                if test_size not in ['Time','actual']:\n",
    "                    #For Each Test Size Produce Values + Moments Reporting...\n",
    "                    rmse = []\n",
    "                    for test_strain in enumerate(real_vs_predicted[metabolite][test_size]):\n",
    "                        #print(test_strain)\n",
    "                        pred_fcn = interp1d(times,test_strain[1])\n",
    "                        integrand = lambda t: (real_fcn(t) - pred_fcn(t))**2\n",
    "                        rmse.append(math.sqrt(quad(integrand,min(times),max(times))[0]))\n",
    "                        \n",
    "                    #Report Moments for each Test Size\n",
    "                    rmse_dict[metabolite][test_size] = rmse\n",
    "                    print('Mean RMSE:',np.mean(rmse),'Standard Deviation RMSE:',np.std(rmse))\n",
    "                    print('RMSEs:',rmse)\n",
    "                    print('')\n",
    "                    \n",
    "\n",
    "    #Calculate Mean Total Error + Mean Standard Deviation for Composite Error\n",
    "    size_dict = {}\n",
    "    for metabolite in rmse_dict:\n",
    "        for test_size in rmse_dict[metabolite]:\n",
    "            size_dict[test_size] = [0]*len(rmse_dict[metabolite][test_size])\n",
    "            size_dict[test_size] = [s + r for s,r in zip(rmse_dict[metabolite][test_size],size_dict[test_size])]\n",
    "\n",
    "    #Print out Agrigate Statistics\n",
    "    for test_size in size_dict:\n",
    "        rmse = size_dict[test_size]\n",
    "        print('Test Size for Agrigate Statistics:',test_size)\n",
    "        print('Mean RMSE:',np.mean(rmse),'Standard Deviation RMSE:',np.std(rmse))\n",
    "        print('RMSEs:',rmse)\n",
    "        print('')\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if data_type == 'simulated':\n",
    "    \n",
    "    \n",
    "    #Plot Agrigate Statistics in the form of a bar graph...\n",
    "    sizes = list(size_dict.keys())\n",
    "    means = [np.mean(size_dict[size]) for size in sizes]\n",
    "    stdevs = [np.std(size_dict[size]) for size in sizes]\n",
    "    y_pos = np.arange(len(sizes))\n",
    "\n",
    "    #Calculate Percent Errors              \n",
    "    #for size in sizes:\n",
    "    #    for training_set in training_sets:\n",
    "    #        percent_integrand = lambda t: abs(real_fcn(t) - kinetic_fcn(t))/(real_fcn(t)*max(times))\n",
    "    \n",
    "    \n",
    "    plt.bar(y_pos, means, align='center',yerr=stdevs)\n",
    "    plt.xticks(y_pos, sizes)\n",
    "    plt.ylabel('RMSE')\n",
    "    plt.xlabel('Training Set Size')\n",
    "    plt.title('RMSE vs Training Set Size')\n",
    "    plt.savefig('figures/RMSEvsSize.pdf')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
